# 라이센스 표기

이용자는 아래의 조건을 따르는 경우에 한하여 자유롭게:
- 이 저작물을 복제, 배포, 전송, 전시, 공연 및 방송할 수 있습니다.

**다음과 같은 조건을 따라야 합니다.**
- **저작자표시:** 귀하는 원저작자를 표시하여야 합니다.
- **비영리:** 귀하는 이 저작물을 영리 목적으로 이용할 수 없습니다.
- **변경금지:** 귀하는 이 저작물을 개작, 변형 또는 가공할 수 없습니다.

---

# 논문 정보

| 항목 | 내용 |
|---|---|
| **논문 제목** | Q-learning-based Strategic Artificial Potential Field for Path Planning in Battlefield Environments <br> = Q-learning기반의 SAPF를 이용한 지상전장환경에서의 경로계획 |
| **저자** | Jisun Lee |
| **발행사항** | 서울 : 고려대학교 대학원, 2024 |
| **학위논문사항** | 학위논문(박사) -- 고려대학교 대학원, 산업경영공학과 경영공학전공, 2024. 8 |
| **발행연도** | 2024 |
| **작성언어** | 영어 |
| **주제어** | Path planning ; Artificial potential field ; Q-learning ; Concealment and cover ; Ground battlefield environments |
| **발행국(도시)** | 서울 |
| **형태사항** | xii, 81p ; 26 cm |
| **일반주기명** | 지도교수: Yoonho Seo |
| **UCI식별코드** | I804:11009-000000289146 |
| **DOI식별코드** | 10.23186/korea.000000289146.11009.0001570 |

---
<1>
Doctoral Dissertation
Q-learning-based Strategic Artificial 
Potential Field for Path Planning in 
Battlefield Environments
Jisun Lee
Department of Industrial and Management Engineering
Graduate School
Korea University
August 2024Q-learning-based Strategic Artificial 
Potential Field for Path Planning in 
Battlefield Environments
by 
Jisun Lee

under the supervision of Professor Yoonho Seo
A dissertation submitted in partial fulfillment of 
the requirements for the degree of 
Doctor of Philosophy.
Department of Industrial and Management Engineering
Graduate School


Korea University
April 2024

<2>
The dissertation of Jisun Lee has been approved by 
the dissertation committee in partial fulfillment of 
the requirements for the degree of 
Doctor of Philosophy.
June 2024
__________________________
Committee Chair: Yoonho Seo
__________________________
Committee Member: Jun-Geol Baek
__________________________
Committee Member: Taesu Cheong
__________________________
Committee Member: Chang Ouk Kim
__________________________
Committee Member: Hyun Woo Jeon

<3>
Q-learning-based Strategic Artificial Potential Field for 
Path Planning in Battlefield Environments
by Jisun Lee
Department of Industrial and Management Engineering
under the supervision of Professor Yoonho Seo
ABSTRACT
Path planning in battlefield environments differs from typical path planning 
because it may not always involve obstacle avoidance. In fact, obstacles can be 
utilized to hide from enemies or facilitate troop advancement through destruction 
and relocation. Therefore, path planning in such environments requires specificity. 
One widely used method for obstacle avoidance is the artificial potential field (APF) 
method. This study proposes a strategic artificial potential field (SAPF) algorithm 
that utilizes obstacles for path planning. The Q-learning algorithm is also being 
researched for its application in learning methods involving path planning and 
obstacle avoidance. This study proposes a path planning algorithm utilizing SAPFbased
Q-learning(Q-SAPF) that takes into account concealment and cover in various 
terrains.
The proposed Q-SAPF algorithm was compared to Q-learning based on the 

<4>
results of repeated experiments and analysis of variance (ANOVA) and the post hoc 
Tukey technique to examine the statistical significance and differences between the 
algorithms, respectively. Q-SAPF outperformed Q-learning by generating shorter 
path plans with differences in path lengths of 1.96–8.77. It further achieved faster 
learning times of approximately 14.57–69.29 seconds and path scores that were 
higher by approximately 2.73-47.96. Specifically, its success rate was 13.73–54.89% 
higher than that of Q-learning. The achieved outcome demonstrated the superior 
learning ability of the Q-SAPF.
The study results confirm the feasibility of path planning that incorporates 
concealment and cover by utilizing obstacles. The findings of this study can 
contribute to path planning in various situations beyond wartime environments.
Keywords: Path planning, Artificial potential field, Q-learning, Concealment 
and cover, Ground battlefield environments

<5>
Q-learning 기반의 SAPF 를 이용한
지상전장환경에서의 경로계획
이 지 선
산 업 경 영 공 학 과
지도교수: 서 윤 호
국문 초록
전장환경에서의 경로계획은과 달리 특수성을 지닌다. 기존의 경로계획이 장애물은 꼭
피해야 한다면, 전장환경에서는 이를 이용해야한다. 장애물을 이용하여 적으로부터
숨기도하고, 때로는 빠른 이동을 위해 파괴하기도 한다.
특히, 지상에서의 지형 활용은 군사작전에서 중요한 요소중 하나이며, 지형의 특성을
효과적으로 활용하는 것은 전시에서 승리를 가져올 수 있는 확률을 높인다. 
지상전장에서는 공중이나 해상의 비해 적의 위치를 파악하는 것이 힘들며, 적 역시 아군의
위치를 파악하는 것이 힘들다. 이런 상황에서 지형의 특성을 잘 활용항 은폐와 엄폐를 하며
이동하는 것은 필수적이다.
본 연구는 지상 전장환경에서의 지형을 활용한 경로계획을 다룬다

<6>
인공전위장법(Artificial Potential Field, APF)는 장애물 회피를 위해 널리 사용되었다. 
본 논문에서는 장애물을 활용하여 경로계획을 형성하는 전략적 인공전위장법(Strategic 
Artificial Potential Field, SAPF)를 정의하고 실험에 활용하였다. Q 러닝 역시 경로계획 및
장애물 회피에서 환경을 통한 학습의 계산 방법을 활용하여 지속적으로 연구에 활용되고
있다. 최종적으로 본 논문에서는 다양한 지형을 고려한 지상 전장환경에서 은폐와 엄폐를
고려한 전략적 인공전위장법기반의 Q 러닝(Q-SAPF)을 활용한 경로계획 알고리즘을
제안하였다.
제안된 Q-SAPF 알고리즘은 Q 러닝과 비교한다. 실험 결과의 유의성을 확인하기
위하여 분산분석을 실시하였으며, 각 알고리즘 간의 차이를 확인하기 위하여, 사후분석을
실시하였다. 연구에서 제안된 Q-SAPF 는 Q 러닝에 비해, 경로 길이는 1.96-8.77 차이의
상대적으로 짧은 경로계획을 약 14.57 초-69.29 초 가량 빠른 시간에 도출하였다. 이 때, 
경로 점수는 약 2.73-47.96 포인트 높게 나타났다. 특히, 성공율은 13.73%-54.89%까지
차이가 났다. 달성된 결과는 Q-SAPF 가 우수한 학습능력을 보여주었다.
본 연구 결과를 통해 장애물을 활용하여 은폐와 엄폐가 가능한 경로 계획이 가능함을
확인하였다. 향후, 일반적인 경로계획이 아닌, 전시상황과 같은 다양한 상황에서의
특수성을 지닌 경로계획에 기여할 것으로 기대된다.

중심어: 경로계획, 전장 환경, 은폐와 엄폐, Q 러닝, 인공전위장법

<7>
ACKNOWLEDMENTS
박사라는 꿈을 가지고 시작한 열정가득했던 기나긴 여정을 이제 마무리하고자 합니다.
이런 여정을 응원해주신 분들께 감사의 인사를 전하고자 합니다.
저를 지도해주신 서윤호 교수님께 감사드립니다. 학위를 취득할 수 있도록 끝까지
든든하게 지켜주셔서 감사합니다. 학위를 받는 지금에서야 연구실에서 교수님이 해주신
많은 이야기들이 느껴집니다. 앞으로의 미래에서 큰 꿈을 꿀수 있도록 하겠습니다. 다시 한
번 감사의 인사 전합니다.
바쁘신 일정 속에서도 제 학위논문 심사를 해주신 백준걸 교수님, 정태수 교수님,
김창욱 교수님, 전현우 교수님께 감사드립니다.
저의 학위를 많이 기다렸을 우리 ISD 연구실 식구들에게 고맙다는 말 전합니다.
함께였기에 힘들었지만 즐겁게 연구하며 생활할 수 있었습니다. 논문게재 소식을
누구보다 기뻐해준 동표오빠와 동화오빠, 뒤늦게 박사학위를 시작해서 세심히 학위심사를
챙겨준 용희 오빠, 연구실 생활에서 빼놓을수 없는 하나뿐인 내 동기인 도현오빠와 츤데레
동수오빠, 내 땡깡을 다 받아준 지희언니, 다들 너무 고맙습니다. 그 외에도 함께 했던
연구실 식구들! 정말 고맙습니다!
학위 논문 쓸 시간을 배려해준 우리 사무실 식구들에게 고맙다는 말 전합니다.
국방분야에 함께 할 수 있게 길을 만들어준 박영욱 교수님, 부족한 나를 믿고 따라 준
미선팀장, 우림간사, 정원과장에게 감사드립니다. 배려해준 시간 덕에 잘 마무리할 수
있었습니다.

<8>
이 결실을 정말이고 기다렸을 사랑하는 부모님께 이 논문을 바칩니다. 말로 표현할 수
없을 정도로 감사합니다. 기약없는 먼 여정에 그간 노심초사했을 마음의 기쁨과 위안이
되었으면 좋겠습니다. 정말 정말 감사합니다. 사랑합니다.
묵묵히 지켜봐준 오빠와 새언니에게도 감사의 말 전합니다. 그리고, 아직은 이게 어떤
의미인지 모르겠지만, 정연아! 가은아! 고모가 박사란다. 
저를 믿고 응원해준 많은 분들이 계십니다. 그대들이 있어 제가 더 빛나는 사람이 될
수 있었습니다. 이제 그런 응원과 감사의 마음을 담아, 제가 옆에 있어 빛이 날 수 있도록
응원하겠습니다. 이 논문을 기다렸을 많은 분들에게 직접 찾아가 마음을 전하겠습니다.
끝까지 포기하지 않았던 제 자신에게 고생했고 수고했다는 말하고 싶습니다. 
지선아! 수고했다!
2024 년 7 월,
감사의 마음을 담아
이 지 선

<9>
TABLE OF CONTENTS
ABSTRACT............................................................................................................... i
국문 초록.................................................................................................................. iii
ACKNOWLEDMENTS ............................................................................................v
TABLE OF CONTENTS........................................................................................ vii
LIST OF TABLES................................................................................................... ix
LIST OF FIGURES ................................................................................................. xi
CHAPTER 1. Introduction.........................................................................................1
CHAPTER 2. Related Work ......................................................................................5
2.1 General path planning .................................................................................5
2.2 Path planning of UAV, USV.......................................................................6
2.3 Artificial Potential Field(APF) Method ......................................................7
2.4 Q-learning .................................................................................................13
CHAPTER 3. Environment representation..............................................................19
3.1 Experiment configuration .........................................................................19
3.2 Result value of experiment .......................................................................22
3.2.1 Definition of a path length .....................................................................22
3.2.2 Definition of a path score.......................................................................23
3.2.3 Definition of experiment time................................................................26
3.2.4 Definition of success rate.......................................................................27

<10>
CHAPTER 4. Q-SAPF Algorithm for Path Planning..............................................28
4.1 Strategic artificial potential field (SAPF) Method....................................28
4.2 Q-learning based on SAFP (Q-SAFP) Method.........................................33
CHAPTER 5. Experimental results .........................................................................37
5.1 Analysis of Map 1 Results........................................................................39
5.2 Analysis of Map 2 Results........................................................................43
5.3 Analysis of Map 3 Results........................................................................47
5.4 Analysis of Map 4 Results........................................................................49
5.5 Analysis of Map 5 Results........................................................................52
5.6 Analysis of Map 6 Results........................................................................56
5.7 Analysis of Map 7 Results........................................................................59
5.8 Analysis of Map 8 Results........................................................................62
5.9 Overall Review of the Analysis Results ...................................................66
CHAPTER 6. Conclusion and Future Directions ....................................................75
REFERENCES.........................................................................................................76

<11>
LIST OF TABLES
Table 1. Path planning pseudocode for APF........................................................... 12
Table 2. Path planning pseudocode for Q-learning................................................. 17
Table 3. Terrain and code shown in the maps......................................................... 19
Table 4. Terrain distribution of the experimental maps.......................................... 21
Table 5. Score of terrains........................................................................................ 24
Table 6. Path planning pseudocode for SAPF ........................................................ 31
Table 7. Path planning pseudocode Q-SAFP.......................................................... 35
Table 8. Experimental parameters .......................................................................... 37
Table 9. Experimental results.................................................................................. 38
Table 10. ANOVA of Map 1 .................................................................................. 40
Table 11. Post hoc analysis results for Map 1......................................................... 41
Table 12. ANOVA of Map 2 .................................................................................. 44
Table 13. Post hoc analysis results for Map 2......................................................... 45
Table 14. ANOVA of Map 3 .................................................................................. 47
Table 15. Post hoc analysis results for Map 3......................................................... 48

<12>
Table 16. ANOVA of Map 4 .................................................................................. 50
Table 17. Post hoc analysis results for Map 4......................................................... 51
Table 18. ANOVA of Map 5 .................................................................................. 53
Table 19. Post hoc analysis results for Map 5......................................................... 54
Table 20. ANOVA of Map 6 .................................................................................. 56
Table 21. Post hoc analysis results for Map 6......................................................... 57
Table 22. ANOVA of Map 7 .................................................................................. 59
Table 23. Post hoc analysis results for Map 7......................................................... 60
Table 24. ANOVA of Map 8 .................................................................................. 63
Table 25. Post hoc analysis results for Map 8......................................................... 64
Table 26. Path length results per map..................................................................... 67
Table 27. Path score(pt) results per map................................................................. 69
Table 28. Experiment time(s) per map.................................................................... 71
Table 29. Success rate(%) per map......................................................................... 72
Table 30. Rank result of Friedman test................................................................... 73

<12>
LIST OF FIGURES
Figure 1. Concept of the APF method .................................................................... 11
Figure 2. Organization of Data set.......................................................................... 20
Figure 3. Movement direction from the current location........................................ 22
Figure 4. Concept of a path length.......................................................................... 23
Figure 5. Method for assigning additional scores................................................... 25
Figure 6. Concept of the SAPF method .................................................................. 29
Figure 7. Comparison of SAPF and APF................................................................ 32
Figure 8. Example of Oscillations........................................................................... 34
Figure 9. Experimental results for Map 1 ............................................................... 42
Figure 10. Experimental results for Map 2 ............................................................. 46
Figure 11. Experimental results for Map 3 ............................................................. 49
Figure 12. Experimental results for Map 4 ............................................................. 52
Figure 13. Experimental results for Map 5 ............................................................. 55

<13>
Figure 14. Experimental results for Map 6 ............................................................. 58
Figure 15. Experimental results for Map 7 ............................................................. 61
Figure 16. Experimental results for Map 8 ............................................................. 65
Figure 17. Experiment time(s) per map .................................................................. 70
Figure 18. Success rate(%) per map........................................................................ 71

<14>
CHAPTER 1. Introduction
A battlespace is the area in which military operations and strategies are devised; 
these include air (air warfare), land (land warfare), sea (naval warfare), and 
information (information warfare). It also encompasses nonphysical battlespaces, 
comprising all the environments and factors needed to achieve mission objectives, 
wage battles, and safeguard troops. A battlefield is a specific location where physical 
battles are fought. Defense tactics such as concealment and cover are utilized to 
evade enemies. Concealment involves utilizing geographical features to avoid enemy 
detection, while cover entails obstructing enemy attacks or reducing the area under 
attack.
The utilization of terrain features is one of the key factors in planning military 
operations, and effectively using the characteristics of the terrain increases the 
number of factors that can lead to victory[1]. On a battlefield, identifying the location 
of an enemy is difficult; hence, it is necessary to traverse the terrain while taking 
precautions and utilizing concealment and cover. While prompt arrival at the 
destination is crucial, military conditions such as ammunition and fuel availability, 
soldier strength, and food supply must also be factored in while moving.
Recently, there has been a surge of research in the field of defense on MannedUnmanned
Teaming (MUM-T) worldwide. MUM-T refers to the operation of

<15>
missions where Manned Systems and Unmanned Systems work together[2]. Due to 
its diverse applicability and potential, MUM-T is expected to be a game-changer in 
future warfare. This is because the autonomous mission execution capabilities are 
continuously improving, and MUM-T can be utilized for difficult missions that are 
directly related to the survival of combat personnel[3]. Effective mission execution 
in MUM-T necessitates path planning for movement. MUM-T can be broadly 
categorized into three types: Ground MUM-T, which involves the use of Unmanned 
Ground Vehicles (UGVs) or unmanned armored vehicles; Aerial MUM-T, which 
employs fighter jets, helicopters, or Unmanned Aerial Vehicles (UAVs); and 
Maritime MUM-T, which utilizes Unmanned Underwater vehicles (UUVs).
Unmanned aerial vehicles (UAVs) are designed to perform advanced tasks such 
as complex terrain exploration, dangerous target reconnaissance, and aerial 
surveillance of enemy-occupied areas in place of humans. In this context, the most 
important research task is path planning [4], as it enables UAVs to navigate through 
enemy territories and successfully execute missions in air defense regions [5]. UAVs 
have been used effectively in a wide range of missions and have significantly reduced 
casualties during warfare[6]. Consequently, research is currently underway to 
investigate the feasibility of using UAVs for military supply transportation in 
mountainous terrain.
Path planning is crucial to navigation mission management systems, and a

<16>
sophisticated and secure approach to path planning is necessary in complex 
operational environments[7]. Autonomous underwater vehicles (AUVs) are 
particularly useful in various areas of battlefields, such as maritime reconnaissance, 
underwater search and investigation, navigation support, and tracking submarines[8]. 
To accomplish missions in a marine environment that is easily detectable by enemies, 
autonomous paths must be carefully planned to avoid collisions in a complex and 
unpredictable underwater environment and conserve energy [9].
Unlike in air or marine environments, planning executable paths for mobile 
vehicles such as robots on uneven terrain with slopes, steps, and rough surfaces is a 
challenging task[10]. Moreover, executing tasks safely and stably in a battlefield 
environment with atypical paths, uncertain conditions, and dense forests without a 
map for navigation poses a unique challenge. For instance, in a battlefield 
environment, obstacles may be utilized as concealment and cover instead of being 
avoided, as in traditional path planning. In this way, path planning in the battlefield 
environment has specificity. Assuming that a tank moves to the enemy base, if it 
moves by road, it can move quickly, but it can also be exposed to the enemy. 
Considering the battlefield environment, a tank can pass by, destroying the building 
if necessary, when moving.
This study proposes a path planning approach that enables concealment and 
cover in ground battlefield environments, taking into account various terrains such

<17>
as forests, rivers, and puddles. The approach suggested in this study utilizes obstacles 
as tools for concealment and cover rather than avoiding them to explore paths.
The remainder of this paper is organized as follows. Section 2 introduces related 
work of the path planning and methodology. Section 3 presents the problem 
description. Section 4 presents explains the concepts of strategic APF, and strategic 
APF-based Q-learning. Section 5 presents the experiment and analysis, and finally, 
Section 6 presents the conclusion and future directions of this study.

<18>
CHAPTER 2. Related Work
2.1 General path planning
In general, path finding involves avoiding obstacles while traveling from a 
starting point to a target point with the aim of minimizing distance and time. Path 
planning, on the other hand, aims to solve problems in various situations where the 
environment, including obstacles, is completely or partially unknown. Path planning 
can be particularly challenging when uncertainty is heightened due to a lack of 
information about the environment.
Path planning has been studied extensively, but the complexity of the problem 
increases in dynamic and complex environments, making it difficult to find optimal 
routes. Consequently, many researchers continuously explore methods to facilitate 
smooth movement of agents in unknown environments. Han, J. (2019) proposed a 
mechanism for path planning in unknown environments based on the importance of 
obstacles and surrounding points[11]. Chen, X., Zhao, M., & Yin, L. (2020) 
suggested a dynamic path planning method for unmanned aerial vehicles using the 
A* algorithm to prepare for sudden threats[12]. Chen, P., Pei, J., Lu, W., & Li, M. 
(2022) introduced a path planning method using the Soft Actor-Critic (SAC), a deep 
reinforcement learning algorithm, for dynamic obstacle avoidance with 
manipulators[13]. Huang, R., Qin, C., Li, J. L., & Lan, X. (2023) conducted research

<19>
using a modified reward approach with Deep Q-Networks (DQN) to achieve 
obstacle-free paths in dynamic unknown environments[14].
2.2 Path planning of UAV, USV
In recent years, UAVs have demonstrated excellent mobility and adaptability 
and are being widely used not only in the military sector for surveillance, search, and 
rescue operations but also in the civilian sector in urban environments[15]. The main 
objective of UAV path planning is to ensure that UAV performance requirements 
are met when designing a safe flight path to a target location [16]. UAV path 
planning takes into account various factors, including mission role, collision 
avoidance, energy limitations, and task-related constraints [17].
Several studies have been conducted on UAV path planning. Bai et al. (2021) 
proposed a path planning algorithm that combines the A* algorithm and the dynamic 
window algorithm (DWA) to satisfy both the security and speed requirements of 
UAVs[18]. Phung & Ha (2021) proposed a spherical vector-based particle swarm 
optimization (SPSO) for UAV path planning, focusing on the safety and validity of 
the path [19]. Han et al. (2022) researched the UAV path planning problem in 
complex indoor environments where a poor flight path may be formed[20].
Unmanned surface vehicles (USVs) are utilized for various military missions, 
such as intelligent surveillance and reconnaissance (ISR), mine countermeasures

<20>
(MCMs), and anti-submarine warfare (ASW)[21]. In the marine environment, 
obstacles and environmental factors such as weather pose significant challenges in 
path planning for USVs. Accordingly, the necessity of effective path planning for 
USVs has been emphasized [22]. To address the path planning problems of USVs in 
complex marine environments, Chen et al. (2021) suggested an improved ACO-APF 
algorithm with an adaptive early warning function. Liu et al. (2019) proposed the ant 
colony algorithm (ACA) and the clustering algorithm (CA) for obstacle avoidance
[23], [24]. Singh et al. (2018) proposed USV path planning that considers variable 
ocean currents and analyzed the effects of headwind and tailwind surface ocean 
currents on path planning [7]. Additionally, Guo et al. (2019) suggested an improved 
genetic algorithm (GA) to deduce an optimal anti-submarine search path model that 
considers the direction and speed of a USV [25]. Bai et al. (2023) investigated USV 
path planning in complex marine environments using a plant growth-based path 
planning algorithm (PGR) [26].
2.3 Artificial Potential Field(APF) Method
The APF method is a popular collision avoidance technique used in the 
development of unmanned vehicles and mobile robots. This effective methodology 
utilizes attractive and repulsive forces to control an artificial potential field. The 
attractive force is generated in the direction of the target point, while the repulsive 
force is generated in the opposite direction of any obstacle. Using this mechanism,

<21>
an agent can avoid obstacles and follow the intended operational direction [27], [28].
The APF algorithm plans a path by creating a virtual potential field within a 
given environment, which includes both attractive and repulsive potential fields. The 
attractive potential field is generated by the target point and acts as a gravitational 
force on the agent, while the obstacle generates the repulsive potential field. By 
calculating the resultant force of these fields, the agent can navigate a path that avoids 
collisions with obstacles.
The potential fields are typically defined mathematically as functions of distance. 
The attractive potential increases as the distance between the agent and its goal 
decreases, implying a relationship where the force becomes stronger as the agent 
approaches its goal. This reflects a direct relation to the closeness rather than an 
inverse relationship with distance itself. Conversely, the repulsive potential is indeed 
inversely proportional to the distance between the agent and obstacles, up to a certain 
threshold distance. Beyond this threshold, the repulsive potential becomes either 
constant or null, indicating that obstacles no longer exert an influence on the agent's 
movement[29].
In the path planning problem, the space is considered to be two-dimensional. 
The gravitational potential field function as defined by the target point is given by 
the following equation [30], [31]:

<22> 수식 다수 구간 글자 깨짐 많음
𝑈𝑈𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) = 1
2 𝑘𝑘𝑎𝑎𝑎𝑎𝑎𝑎 ∙ 𝛿𝛿2�𝑋𝑋, 𝑋𝑋𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡� (1)
where 𝑘𝑘𝑎𝑎𝑎𝑎𝑎𝑎 is the attraction coefficient, 𝑋𝑋 is the 2D spatial coordinate of the 
controlled agent, 𝑋𝑋𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 is the target point, and 𝛿𝛿�𝑋𝑋, 𝑋𝑋𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔� is the distance between 
the controlled agent and the target point. 𝑈𝑈𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) represents the targeted 
gravitational attraction between the controlled agent and the goal, which increases 
with the coefficient of gravitational attraction.
The derivative of the gravitational potential field 𝑈𝑈𝑎𝑎𝑎𝑎𝑎𝑎 with respect to the relative 
distance between the controlled agent and the target point is given by
𝐹𝐹𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) = −∇𝑈𝑈𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) = − 1
2
𝑘𝑘𝑎𝑎𝑎𝑎𝑎𝑎∇δ2�𝑋𝑋, 𝑋𝑋𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡� = −𝐾𝐾𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋, 𝑋𝑋𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡) (2)
where ∇ is the partial derivative operator. The repulsion force potential field 
generated between the controlled agent and the obstacle is given by
𝑈𝑈𝑟𝑟𝑟𝑟𝑟𝑟(𝑋𝑋) =
(3)
{
1
2𝐾𝐾𝑟𝑟𝑟𝑟𝑟𝑟 � 1
𝛿𝛿(𝑋𝑋, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜) − 1
𝛿𝛿0
�
2
𝛿𝛿(𝑋𝑋, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜) ≤ 𝛿𝛿0
0 𝛿𝛿(𝑋𝑋, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜) > 𝛿𝛿010

<23> 수식 다수 구간 글자 깨짐 많음
where 𝐾𝐾𝑟𝑟𝑟𝑟𝑟𝑟 is the repulsion coefficient, 𝑋𝑋 is the 2D spatial coordinate of the 
controlling agent, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜 is the coordinate of the obstacle, 𝛿𝛿(𝑋𝑋, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜) is the 
distance between the controlling agent and the obstacle, and 𝛿𝛿0 is the maximum 
repulsive force acting distance of the obstacle. If the distance between the agent and 
the obstacle, 𝛿𝛿(𝑋𝑋, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜), is larger than the maximum influence distance 𝛿𝛿0, the 
repulsion force potential field for the obstacle of the controlled agent becomes zero 
(𝑈𝑈𝑟𝑟𝑟𝑟𝑟𝑟 = 0). Conversely, if 𝛿𝛿0 is small, the repulsion force potential field increases 
as the controlled agent approaches the obstacle [32].
Based on Equation (3), the repulsion force can be calculated using Equation (4) 
as follows:
𝐹𝐹𝑟𝑟𝑟𝑟𝑟𝑟(𝑋𝑋) = −∇𝑈𝑈𝑟𝑟𝑟𝑟𝑟𝑟(𝑋𝑋) (4)
Using Equation (4), the total force 𝐹𝐹(𝑋𝑋𝑞𝑞) for position 𝑞𝑞 is calculated using the 
potential force of the obstacle and target point:
𝐹𝐹�𝑋𝑋𝑞𝑞� = �𝐹𝐹𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋𝑖𝑖)
𝑛𝑛
𝑖𝑖=1
+ �𝐹𝐹𝑟𝑟𝑟𝑟𝑟𝑟�𝑋𝑋𝑗𝑗�
𝑛𝑛
𝑗𝑗=1
(5)

<24>
Figure 1. Concept of the APF method
Using the above principle, Guan et al. (2021) designed an APF-based reward 
function suitable for reinforcement learning. The reward increases as the agent 
approaches the target point and decreases as the agent approaches the obstacle[33]. 
Instant or stage rewards are assigned based on the current state, where the distance 
to the target point is divided into specific ranges and a higher positive reward is 
assigned for a closer distance to the target point. A gravity reward function using an 
attractive force assigns a higher positive reward if the agent is within a certain range 
of the target point. On the other hand, a repulsion penalty function using repulsive 
force assigns a higher negative reward if the agent is within a specific range of an 
obstacle.
Table 1 presents the pseudocode for the APF used in this study.

<25> 수식 다수 구간 글자 깨짐 많음
Table 1. Path planning pseudocode for APF
Input: start point, goal point, map environment
Output: Final Path, Path Length, Path Score
Set the start point 𝑆𝑆, the goal point 𝐺𝐺
Set the attraction coefficient 𝐾𝐾𝑎𝑎𝑎𝑎𝑎𝑎 , the repulsive coefficient 𝐾𝐾𝑟𝑟𝑟𝑟𝑟𝑟
Set the current position 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝
Set the current state 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 to the start point 𝑆𝑆
While(𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 is not the Goal point 𝐺𝐺)
Calculate the attractive force 𝐹𝐹𝑎𝑎𝑎𝑎𝑎𝑎 as a vector point from 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 to 𝐺𝐺 using 
Eq .(2)
Calculate the repulsive force 𝐹𝐹𝑟𝑟𝑟𝑟𝑟𝑟 as a vector point away from each obstacle 
using Eq. (4)
Calculate the total force 𝐹𝐹𝑞𝑞 using Equation (5)
Update 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 by taking a step in the direction of𝐹𝐹𝑞𝑞
End While
End
The APF method is utilized in path planning because of its simplicity and 
efficiency in providing smooth and safe plans [34]. By using the resultant forces 
from the target points and obstacles, safe paths can be efficiently calculated. 
However, the APF method can be sensitive to obstacles, which can limit its

<26>
effectiveness. Despite this, it is still widely used due to its easy maneuverability and 
ability to generate smoother trajectories with minimal computing resources [35]. 
Researchers have continued to explore the potential of the APF in path planning. Fan 
et al. (2020) proposed a modified APF algorithm to address path planning for 
avoiding movable, unknown obstacles[36]. Wang et al. (2019) proposed an APF 
algorithm that incorporated driving behavior characteristics and demonstrated its 
effectiveness and feasibility in obstacle avoidance path planning [37]. Shin & Kim 
(2021) proposed and validated a local hybrid PR-APF positioning network path 
planning approach to provide safe navigation for UVs [38].
2.4 Q-learning 
Reinforcement learning is a method for learning the optimal decision-making 
process through repeated interactions between an agent and the environment.
Q-learning is a type of reinforcement learning and is a model-free algorithm. The 
model-free algorithm attempts to find a policy function that maximizes the expected 
sum of future rewards through the action of the agent. The environment is not one of 
the input variables for this algorithm. Therefore, the next state and the next reward 
are passively obtained. In other words, this algorithm has to explore because it does 
not know the environment. Policy functions should be gradually learned through trial 
and error based on such exploration.

<27>
Additionally, Q-learning is based on the Markov decision process (MDP). The 
goal of Q-learning is to determine the optimal policy for an agent to take a specific 
action in a specific situation in a finite MDP. The predicted value of the final reward 
should be maximized, starting from the current state and proceeding through 
successive stages.
Q-learning learns the optimal policy by estimating the Q-function, which 
predicts the expected utility of taking a particular action in a specific state. The 
optimal policy can be derived from the learned Q-function by selecting the action 
with the highest Q-value in each state. One of the advantages of Q-learning is its 
ability to compare the expected values of actions in a model-free manner without any 
prior knowledge of the environment. It can also be used in environments where 
transitions and rewards are probabilistic. In Q-learning, "Q" represents the expected 
reward of the action taken in the current state. [39], [40], [41].
In Q-learning, the goal of the agent is to maximize the reward until the end of 
the episode. Reinforcement learning uses state-value functions and action-value 
functions to maximize rewards. The state-value function is expressed using Equation 
(6).

<28> 수식 다수 구간 글자 깨짐 많음
𝑉𝑉𝜋𝜋(𝑠𝑠) = 𝐸𝐸𝜋𝜋[𝑅𝑅𝑡𝑡+1 + 𝛾𝛾𝑅𝑅𝑡𝑡+2 + 𝛾𝛾2𝑅𝑅𝑡𝑡+3 + ⋯ |𝑆𝑆𝑡𝑡 = 𝑠𝑠]
= 𝐸𝐸𝜋𝜋[𝑅𝑅𝑡𝑡+1 + 𝛾𝛾𝐸𝐸𝜋𝜋(𝑅𝑅𝑡𝑡+2 + ⋯ )|𝑆𝑆𝑡𝑡 = 𝑠𝑠]
= 𝐸𝐸𝜋𝜋[𝑅𝑅𝑡𝑡+1 + 𝛾𝛾𝑉𝑉𝜋𝜋(𝑆𝑆𝑡𝑡+1)|𝑆𝑆𝑡𝑡 = 𝑠𝑠]
(6)
In equation (6), 𝑅𝑅𝑡𝑡 refers to the reward value at time 𝑡𝑡. When a policy 𝜋𝜋 is 
applied at time 𝑡𝑡, the value of the expected state 𝑠𝑠 can be expressed as the sum of 
future rewards, where 𝛾𝛾 in Equation (6) represents the discount factor. The discount 
factor, which determines the importance of present rewards relative to future rewards, 
has a value of [0,1].
When a certain action 𝑎𝑎 is taken in a specific state 𝑠𝑠, the Q-value is used to 
calculate the value of that action, which is known as the action-value function and is 
expressed using Equation (7). The action-value function predicts the value of the 
sum of rewards until the end of the episode, given that a specific action is taken using 
the discount factor.
𝑄𝑄𝜋𝜋(𝑠𝑠, 𝑎𝑎) = 𝐸𝐸𝜋𝜋[𝑅𝑅𝑡𝑡+1 + 𝛾𝛾𝑅𝑅𝑡𝑡+2 + 𝛾𝛾2𝑅𝑅𝑡𝑡+3 + ⋯ |𝑆𝑆𝑡𝑡 = 𝑠𝑠, 𝐴𝐴𝑡𝑡 = 𝑎𝑎]
= 𝐸𝐸𝜋𝜋[𝑅𝑅𝑡𝑡+1 + 𝛾𝛾𝑉𝑉𝜋𝜋(𝑆𝑆𝑡𝑡+1, 𝐴𝐴𝑡𝑡+1)|𝑆𝑆𝑡𝑡 = 𝑠𝑠, 𝐴𝐴𝑡𝑡 = 𝑎𝑎] (7)
The Q-value can be expressed according to the Bellman Equation (8): 𝑎𝑎′ is an 
action that maximizes the Q-value in the next state 𝑠𝑠′
.

<29> 수식 다수 구간 글자 깨짐 많음
𝑄𝑄(𝑠𝑠, 𝑎𝑎) = 𝑟𝑟 + 𝛾𝛾max𝑎𝑎′𝑄𝑄(𝑠𝑠′
, 𝑎𝑎′
) (8)
Thus, at each time 𝑡𝑡, the agent takes action 𝑎𝑎𝑡𝑡 in state 𝑠𝑠𝑡𝑡 and transitions to a new 
state 𝑠𝑠𝑡𝑡+1. At this time, the reward 𝑟𝑟𝑡𝑡 is obtained, and the Q-value is updated. The 
key aspect of the algorithm is its use of the Bellman equation, which enables iterative 
updates of the value function. The algorithm achieves this by taking a weighted 
average of the current value and new information at each iteration.
In summary, the Q-learning formula is given as follows Equation (9):
𝑄𝑄(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) ← (1 − 𝛼𝛼)𝑄𝑄(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) + 𝛼𝛼[𝑟𝑟𝑡𝑡 + 𝛾𝛾max𝑎𝑎′𝑄𝑄(𝑠𝑠𝑡𝑡+1, 𝑎𝑎) − 𝑄𝑄(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡)] (9)
𝛼𝛼: learning rate,0 < 𝛼𝛼 ≤ 1 
𝑠𝑠𝑡𝑡: the current state at time 𝑡𝑡
𝑎𝑎𝑡𝑡: the action at time 𝑡𝑡 
𝑄𝑄(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡): the current value
max𝑎𝑎′𝑄𝑄(𝑠𝑠𝑡𝑡+1, 𝑎𝑎): estimate of optimal future value

<30> 수식 다수 구간 글자 깨짐 많음
Table 2. Path planning pseudocode for Q-learning
Input: start point, goal point, map environment
Output: Final Path, Path Length, Path Score
Initialize Q-table with all zeros
Initialize the state 𝑠𝑠, the action 𝑎𝑎
Set the start point 𝑆𝑆, the goal point 𝐺𝐺
Set the learning rate 𝛼𝛼, the discount rate 𝛾𝛾, the exploration rate 𝜀𝜀
Set the max iteration
While(iteration < Max iteration)
Set the current state 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 to the initial state 𝑠𝑠
While(𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 is not the Goal point 𝐺𝐺)
With probability epsilon, Select a random action, 𝑎𝑎
 Otherwise, select the action with the highest Q-value in the next 
state for 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠
 Perform the action 𝑎𝑎, and Observe the new state, 𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁_𝑠𝑠 , and 
reward, 𝑟𝑟
 Update Q-value for 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 and a using Equation (9).
 Set 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 to 𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁_𝑠𝑠,
End While
Decrease epsilon to gradually reduce exploration rate 𝜀𝜀
End While
End

<31>
With advancements in computer technology, path planning problems can be 
solved using a variety of learning algorithms. These algorithms have the advantage 
of including conditions that have not been solved previously. Reinforcement learning, 
in particular, can be used to explore unknown environments based on rewards and 
penalties. It is suitable for path planning problems in which an agent receives a 
reward if it avoids colliding with obstacles and a penalty if it collides with them. 
Low et al. (2019) proposed an improved Q-learning algorithm for path planning in 
environments with static obstacles of various shapes, sizes, and layouts. The use of 
Q-learning reduces computation time and generates efficient paths [42]. Jiang & Xin 
(2019) proposed the Q-learning algorithm for path planning in a free-space 
environment [43]. Wei et al. (2016) presented a novel discrete-time deterministic Qlearning
algorithm that updates the Q-function for all states and control spaces to 
induce the Q-function to converge to an optimal value, thereby considerably 
accelerating the convergence speed[44]. In addition, Low et al. (2022) addressed 
path planning via distance measurement, Q-function modification to overcome dead 
ends, and the introduction of virtual targets to bypass dead ends via an approach 
known as improved Q-learning (IQL) [45].

<32>
CHAPTER 3. Environment representation
3.1 Experiment configuration
This study proposes a path planning approach that takes into account terrain 
conditions to find an optimal route from a starting point to a target point in a 
battlefield environment while ensuring concealment and cover using terrain features. 
The starting point, target point, and terrain features are represented in a twodimensional
grid on a 50 × 50 map. The terrain and corresponding codes used in the 
maps are shown in Table 3. This terrain includes what can be considered a battlefield 
environment.
Table 3. Terrain and code shown in the maps
Code Terrain Code Terrain
0 Target point 6 Forest
1 Flatland 7 Puddle
2 Hill 8 Sand
3 Destructible building 9 River
4 Destroyed building A Road
5 Indestructible building B Bridge

<31>
In this study, the map created custom for the experiments. In the event of ground 
warfare in Korea, locations near the Korean Demilitarized Zone (DMZ), which are 
of strategic military importance and typically host villages and military bases, were 
randomly selected. The terrain of the randomly designated maps was examined and 
formed into a 50*50 grid.
Figure 2. Organization of Data set
A total of eight maps were used in the experiment, and the terrain distribution of 
each map is presented in Table 4.

<32>
Table 4. Terrain distribution of the experimental maps
|      | 1      | 2      | 3            | 4         | 5              | 6      |
|------|--------|--------|--------------|-----------|----------------|--------|
|      | Flatland | Hill | Destructible | Destroyed | Indestructible | Forest |
| Map 1| 31.24% | 12.80% | 14.60%       | 1.60%     | 14.36%         | 2.16%  |
| Map 2| 37.96% | 7.20%  | 14.20%       | 3.08%     | 12.44%         | 8.04%  |
| Map 3| 46.32% | 10.24% | 24.36%       | 0.00%     | 14.04%         | 0.00%  |
| Map 4| 43.64% | 3.80%  | 15.16%       | 0.00%     | 15.52%         | 2.52%  |
| Map 5| 41.39% | 8.45%  | 15.88%       | 0.94%     | 12.08%         | 8.00%  |
| Map 6| 38.82% | 4.41%  | 24.69%       | 0.00%     | 12.41%         | 5.67%  |
| Map 7| 29.04% | 6.40%  | 16.32%       | 2.48%     | 14.44%         | 17.52% |
| Map 8| 36.00% | 0.88%  | 16.12%       | 1.28%     | 7.88%          | 14.20% |

|      | 7     | 8     | 9     | A      | B      |
|------|-------|-------|-------|--------|--------|
|      | Puddle| Sand  | River | Road   | Bridge |
| Map 1| 0.64% | 0.60% | 5.92% | 14.08% | 2.00%  |
| Map 2| 0.76% | 0.32% | 3.44% | 12.00% | 0.56%  |
| Map 3| 0.00% | 0.00% | 0.00% | 5.04%  | 0.00%  |
| Map 4| 3.32% | 1.56% | 6.20% | 7.36%  | 0.92%  |
| Map 5| 0.00% | 0.00% | 1.59% | 11.27% | 0.41%  |
| Map 6| 7.71% | 0.00% | 0.00% | 6.29%  | 0.00%  |
| Map 7| 0.52% | 1.56% | 0.00% | 11.72% | 0.00%  |
| Map 8| 0.00% | 0.16% | 3.24% | 19.48% | 0.76%  |

The movement of an agent is determined by choosing one of the available actions, 
which include moving left, right, up, down, or diagonally. If there is an obstacle 
blocking a certain direction, the agent can choose to move in any of the other 
available directions.

<33> 수식 다수 구간 글자 깨짐 많음
Figure 3. Movement direction from the current location
3.2 Result value of experiment
The results of the experiment aim to compare path length, path score, success rate, and 
experiment time. The methods for deriving each of these values are described below.
3.2.1 Definition of a path length
In this study, the path length is defined as the total distance between the start and 
target points, as shown in Equation (10):
𝐿𝐿𝑝𝑝𝑝𝑝𝑝𝑝ℎ = ��(𝑥𝑥𝑖𝑖+1 − 𝑥𝑥𝑖𝑖)2 + (𝑦𝑦𝑖𝑖+1 − 𝑦𝑦𝑖𝑖)2
𝑛𝑛
𝑖𝑖=0
(10)
Here, in 𝑖𝑖 = 0, 1, ⋯ , 𝑛𝑛, when 𝑖𝑖 = 0, the starting point is 𝑃𝑃𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = (𝑥𝑥0 , 𝑦𝑦0), and 

<34> 수식 다수 구간 글자 깨짐 많음
when 𝑖𝑖 = 𝑛𝑛, the target point is 𝑃𝑃𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 = (𝑥𝑥𝑛𝑛 , 𝑦𝑦𝑛𝑛). Furthermore, the current state 
point is 𝑃𝑃𝑐𝑐𝑐𝑐𝑐𝑐 = (𝑥𝑥𝑖𝑖 , 𝑦𝑦𝑖𝑖), and the next state point is expressed as 𝑃𝑃𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛 = (𝑥𝑥𝑖𝑖+1, 𝑦𝑦𝑖𝑖+1). 
Figure 4. Concept of a path length
When an agent is assumed to move once, the path length is 1 when it moves left, 
right, up, or down, and the path length is √2 when it moves in diagonal directions. 
3.2.2 Definition of a path score 
In this study, two types of path scores are defined for comparison: a common 
path score based on terrain characteristics and an additional path score based on 
concealment and cover. 
For the common path score, each terrain is assigned a score based on its 
characteristics, taking into account the possibility of applying concealment and cover. 
Terrains that allowed for concealment and cover were assigned a positive score, 
while those that hindered movement were assigned a negative score. Terrains such

<35> 수식 다수 구간 글자 깨짐 많음
as roads and flatlands, which are relatively easy to traverse but do not offer much 
opportunity for concealment and cover, are not assigned a score. Obstacles are also 
not assigned a score, as a path cannot be established through them.
Table 5. Score of terrains
| Code | Terrain                | Score     |
|------|-----------------------|-----------|
| 0    | Target point          | 0         |
| 1    | Flatland              | 0         |
| 2    | Hill                  | 3         |
| 3    | Destructible building | 3         |
| 4    | Destroyed building    | 0         |
| 5    | Indestructible building | Obstacle |
| 6    | Forest                | 3         |
| 7    | Puddle                | -1        |
| 8    | Sand                  | -1        |
| 9    | River                 | Obstacle  |
| A    | Road                  | 0         |
| B    | Bridge                | 0         |
𝑆𝑆𝑐𝑐𝑐𝑐𝑐𝑐 = �𝑠𝑠𝑥𝑥𝑖𝑖,𝑦𝑦𝑖𝑖
𝑛𝑛
𝑖𝑖=0
(11)
where 𝑆𝑆𝑐𝑐𝑐𝑐𝑐𝑐 is the common path score in which 𝑠𝑠𝑥𝑥𝑖𝑖,𝑦𝑦𝑖𝑖 is the score of the current 
state point (𝑥𝑥𝑖𝑖, 𝑦𝑦𝑖𝑖).

<36> 수식 다수 구간 글자 깨짐 많음
For the additional path score, a positive score is assigned if concealment and 
cover are possible on the path. Starting from the locations where concealment and 
cover are possible, a score of 𝜔𝜔 is assigned to up to two spaces in the left, right, up, 
and down directions and to one space in the diagonal direction.
Figure 5. Method for assigning additional scores
𝑆𝑆𝑎𝑎𝑎𝑎𝑎𝑎 = �𝑎𝑎𝑎𝑎𝑥𝑥𝑖𝑖,𝑦𝑦𝑖𝑖
𝑛𝑛
𝑖𝑖=0
(12)
where 𝑆𝑆𝑎𝑎𝑎𝑎𝑎𝑎 is the additional path score and 𝑎𝑎𝑎𝑎𝑥𝑥𝑖𝑖,𝑦𝑦𝑖𝑖 is the additional score of the 
current state point (𝑥𝑥𝑖𝑖, 𝑦𝑦𝑖𝑖).

<37> 수식 다수 구간 글자 깨짐 많음
The total path score, represented as 𝑆𝑆, is the sum of the common path score and 
additional path score, as shown in Equation (13).
𝑆𝑆 = 𝑆𝑆𝑐𝑐𝑐𝑐𝑐𝑐 + 𝑆𝑆𝑎𝑎𝑎𝑎𝑎𝑎 (13)
A high path score indicates that concealment and cover have been effectively 
applied.
By contrast, the score of the target point is set to zero: the value of the path score 
is derived when path exploration is successful, and a score of zero is assigned to the 
target point to ensure that only the concealment and cover scores of the final searched 
path are evaluated.
The unit of measurement for the results is expressed in points. Furthermore, 
these scores are calculated after each algorithm is completed and are used solely for 
the purpose of comparing results. The reward and penalty values for Q-learning can 
be found in Chapter 5.
3.2.3 Definition of experiment time
The experiment time is defined as the duration from the start to the end of the 
experiment. The termination conditions for APF and SAPF are defined as when a path has 
been planned from the start point to the target point. For Q-learning, the termination 
condition is defined as when the applied iterations have been completed.

<38> 수식 다수 구간 글자 깨짐 많음
The Experiment time is represented as 𝐸𝐸𝐸𝐸, as show Equation(14).
𝐸𝐸𝐸𝐸 = 𝑇𝑇𝑒𝑒𝑒𝑒𝑒𝑒 − 𝑇𝑇𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 (14)
𝑇𝑇𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠: Start time 
𝑇𝑇𝑒𝑒𝑒𝑒𝑒𝑒: End time 
3.2.4 Definition of success rate 
The success rate is defined as a metric to evaluate how well each algorithm 
locates a path plan during the experiment. It is calculated using the number of 
successful path plans from the start point to the target point out of the total number 
of trials.
The Experiment time is represented as 𝑆𝑆𝑆𝑆, as show Equation(15).
𝑆𝑆𝑆𝑆 = 𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁 𝑜𝑜𝑜𝑜 𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆
𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛 𝑜𝑜𝑜𝑜 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 (15)

<39>
CHAPTER 4. Q-SAPF Algorithm for Path 
Planning
In this paper, we define a Strategic Artificial Potential Field for solving path 
planning in battlefield environments, and employ a methodology based on Qlearning.
4.1 Strategic artificial potential field (SAPF) Method
This study proposes a SAPF method for path planning for concealment and cover, 
which uses only a gravity potential field. In conventional APF, obstacles are avoided 
through the repulsive force that acts on them. However, on a battlefield, it may be 
necessary to use obstacles for concealment and coverage while moving toward a 
target point.
The SAPF method uses an attractive force for concealment and cover instead of 
the repulsive force that acts on obstacles, as shown in Equation (5) for a conventional 
APF. In the SAPF, the attractive force is defined separately for the target point and 
the obstacle.

<40> 수식 다수 구간 글자 깨짐 많음
Figure 6. Concept of the SAPF method
Using Equations (1) and (2), the attractive force of a target point is defined as 
follows:
𝐹𝐹𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) = −∇𝑈𝑈𝑡𝑡𝑡𝑡𝑟𝑟𝑎𝑎𝑎𝑎𝑎𝑎 (𝑋𝑋) = −𝑘𝑘𝑡𝑡𝑡𝑡𝑟𝑟𝑎𝑎𝑎𝑎𝑎𝑎 �𝑋𝑋, 𝑋𝑋𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡� (16)
where 𝑘𝑘𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎 is the attraction coefficient for a target point, 𝑋𝑋 is the 2D spatial 
coordinate of the controlled agent, 𝑋𝑋𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 is the target point, and 𝑈𝑈𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) is the 
targeted gravitational attraction between the controlled agent and goal.
The attractive potential field for an obstacle is defined as follows:
𝐹𝐹𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) = −∇𝑈𝑈𝑜𝑜𝑜𝑜𝑠𝑠𝑎𝑎𝑎𝑎𝑎𝑎 (𝑋𝑋) = −𝑘𝑘𝑜𝑜𝑜𝑜𝑠𝑠𝑎𝑎𝑎𝑎𝑎𝑎 (𝑋𝑋, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜) (17)

<41> 수식 다수 구간 글자 깨짐 많음
where 𝑘𝑘𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎 is the attraction coefficient for an obstacle, 𝑋𝑋𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜 is the 2D 
spatial coordinate of the obstacle, and 𝑈𝑈𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎(𝑋𝑋) is the targeted gravitational 
attraction between the controlled agent and goal. However, to ensure that the force 
toward a target point has a greater impact, 𝑘𝑘𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎 < 𝑘𝑘𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎.
Using the above equation, the total force 𝐹𝐹(𝑋𝑋𝑞𝑞) at position 𝑞𝑞 is calculated using 
the potential forces of the obstacle and target point:
�𝑋𝑋𝑞𝑞� = �𝐹𝐹𝑡𝑡𝑡𝑡𝑟𝑟𝑎𝑎𝑎𝑎𝑎𝑎 (𝑋𝑋𝑖𝑖)
𝑛𝑛
𝑖𝑖=1
+ �𝐹𝐹𝑜𝑜𝑜𝑜𝑠𝑠𝑎𝑎𝑎𝑎𝑎𝑎 (𝑋𝑋𝑗𝑗)
𝑛𝑛
𝑗𝑗=1
(18)

<42> 수식 다수 구간 글자 깨짐 많음
Table 6. Path planning pseudocode for SAPF
Input: Start point, goal point, map environment
Output: Final Path, Path Length, Path Score
---
    Set the start point 𝑆𝑆, the goal point 𝐺𝐺
    Set the attraction coefficient for target point 𝐾𝐾𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎
Set the attraction coefficient for obstacle point𝐾𝐾𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎
    Set the current position 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝
    Set the current state 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 to the start point 𝑆𝑆
While(𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 is not the Goal point 𝐺𝐺)
    Calculate the attractive force for Target point 𝐹𝐹𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎 as a vector 
point from 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 to 𝐺𝐺 using Eq. (13)
    Calculate the attraction force for obstacle point 𝐹𝐹𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎 as a vector 
point away from each obstacle using Eq.(14)
    Calculate the total force 𝐹𝐹𝑞𝑞 using Eq. (15)
    Update 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 by taking a step in the direction of 𝐹𝐹𝑞𝑞
End While
End
---

<43>
SAPF formed a force different from APF owing to the attractive force generated 
from the obstacle. As a result, it heads to the target point along a different path from 
the APF. Figure 6 shows the movement lines of APF and SAPF according to 
obstacles. The arrow in the picture represents the total force received when the agent 
reached that location.
Figure 7. Comparison of SAPF and APF

<44>
4.2 Q-learning based on SAFP (Q-SAFP) Method
This study proposes Q-learning using SAPF.
Oscillations can occur in path planning when using APF due to the force of 
obstacles, resulting in unstable movements [28]. The SASF proposed in this study 
also has oscillations. An example of oscillations generated in APF and SAPF is 
shown in Figure 7.
To overcome the issues of oscillations in path planning, this study proposed a 
hybrid algorithm that incorporates Q-learning. The algorithm first explores a path 
using the SAPF algorithm, then configures a Q-table based on the explored path, and 
finally performs Q-learning to learn the optimal policy.

<45>
(a) Oscillation
(b) Oscillations of APF
(c) Oscillations of SAPF

Figure 8. Example of Oscillations

<46> 수식 다수 구간 글자 깨짐 많음
Table 7. Path planning pseudocode Q-SAFP
Input: start point, goal point, map environment
Output: Final Path, Path Length, Path Score
---
    Initialize Q-table with all zeros
    Initialize the state 𝑠𝑠, the action 𝑎𝑎
    Set the start point 𝑆𝑆, the goal point 𝐺𝐺
    Set the learning rate 𝛼𝛼, the discount rate 𝛾𝛾, the exploration rate 𝜀𝜀
    Set the max iteration
    Set the attraction coefficient for target point 𝐾𝐾𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎
    Set the attraction coefficient for obstacle point 𝐾𝐾𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎
    Set the current position 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝
    Set the current state 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 to the start point 𝑆𝑆
While(𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 is not the Goal point 𝐺𝐺)
    Calculate the attractive force for Target point 𝐹𝐹𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎 as a vector point from 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 to 𝐺𝐺 using Eq. (13)
    Calculate the attraction force for obstacle point 𝐹𝐹𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎 as a vector point away from each obstacle using Eq.(14)
    Calculate the total force 𝐹𝐹𝑞𝑞 using Eq. (15)
    Update 𝐶𝐶𝐶𝐶𝐶𝐶_𝑝𝑝 by taking a step in the direction of 𝐹𝐹𝑞𝑞
End While
End

<47> 수식 다수 구간 글자 깨짐 많음
    While(iteration < Max iteration)
        Set the current state 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 to the initial state 𝑠𝑠
    While(𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 is not the Goal point 𝐺𝐺)
            With probability epsilon, Select a random action, 𝑎𝑎
        Otherwise, select the action with the highest Q-value in the next state for 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠
        Perform the action 𝑎𝑎, and Observe the new state, 𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁_𝑠𝑠 , and reward,𝑟𝑟
        Update Q-value for 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 and a using Eq. (9).
        Set 𝐶𝐶𝐶𝐶𝐶𝐶_𝑠𝑠 to 𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁_𝑠𝑠,
            End While
        Decrease epsilon to gradually reduce exploration rate 𝜀𝜀
        End While
        End

<48> 수식 다수 구간 글자 깨짐 많음
CHAPTER 5. Experimental results
In this section, the proposed Q-SAPF is experimentally compared against APF, 
Sapf, and Q-learning. The experiment was conducted on a 50 × 50 grid map with 
various terrain types. The algorithms for path length, path score, experiment time, 
and success rate were derived and compared.
The parameters used for the experiment are listed in Table 8.
Table 8. Experimental parameters
Parameter
APF 𝐾𝐾𝑎𝑎𝑎𝑎𝑎𝑎 = 10,𝐾𝐾𝑟𝑟𝑟𝑟𝑟𝑟 = 5
SAPF 𝐾𝐾𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎 = 10,𝐾𝐾𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎 = 2
Q 𝛼𝛼 = 0.3, 𝛾𝛾 = 0.9, 𝜀𝜀 = 0.9, max iteration = 4000
Q-SAPF 𝐾𝐾𝑡𝑡𝑡𝑡𝑡𝑡_𝑎𝑎𝑎𝑎𝑎𝑎 = 10, 𝐾𝐾𝑜𝑜𝑜𝑜𝑜𝑜_𝑎𝑎𝑎𝑎𝑎𝑎 = 2, 𝛼𝛼 = 0.3, 𝛾𝛾 = 0.9, 𝜀𝜀 = 0.9, max iteration = 4000
In Q-learning and Q-SAPF, the reward is +1 when the movement is successful 
and -1 when it fails, and a reward value of +10 is given when the goal is reached. 
The path score was set to 𝜔𝜔 = 5 when deducing the experimental results. Through 
repeated experiments using four different experimental methods, the averages and 
standard deviations for eight different maps were derived and are listed in Table 9.

<49>
Table 9. Experimental results
### Map 1

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 113.16(6.79) | 141.30(13.96)  | 80.70(5.82)    | 78.31(4.55)     |
| Path Score(pt)    | 280.20(73.21)| 369.00(83.14)  | 215.33(39.83)  | 233.03(23.29)   |
| Exp. Time (s)     | 29.50(8.39)  | 95.39(41.96)   | 40.86(13.33)   | 16.56(9.56)     |
| Success rate (%)  | 30.71(9.84)  | 7.00(3.25)     | 72.14(6.29)    | 99.65(0.06)     |

### Map 2

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 112.15(11.25)| 147.37(12.53)  | 81.52(4.41)    | 72.75(3.81)     |
| Path Score(pt)    | 338.13(64.90)| 340.30(48.36)  | 190.67(43.43)  | 228.50(22.96)   |
| Exp. Time (s)     | 27.91(9.44)  | 266.99(85.63)  | 86.19(20.91)   | 16.50(6.43)     |
| Success rate (%)  | 66.16(14.55) | 2.23(0.78)     | 44.76(12.97)   | 99.65(0.06)     |

### Map 3

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 117.22(13.58)| 154.04(10.89)  | 80.43(3.05)    | 76.25(2.79)     |
| Path Score(pt)    | 407.17(54.16)| 468.87(73.65)  | 225.07(35.96)  | 239.77(26.70)   |
| Exp. Time (s)     | 26.17(8.85)  | 401.66(162.16) | 68.21(20.16)   | 15.35(2.46)     |
| Success rate (%)  | 58.23(13.31) | 2.60(1.37)     | 68.09(8.31)    | 99.61(0.06)     |

### Map 4

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 106.29(14.61)| 139.34(17.08)  | 65.60(3.71)    | 63.64(1.41)     |
| Path Score(pt)    | 286.53(40.61)| 347.00(75.50)  | 205.90(25.31)  | 219.20(26.31)   |
| Exp. Time (s)     | 11.35(3.31)  | 74.12(48.53)   | 46.07(12.49)   | 18.05(9.69)     |
| Success rate (%)  | 30.76(8.76)  | 5.16(6.26)     | 78.65(5.81)    | 99.74(0.05)     |

### Map 5

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 103.53(7.68) | 152.10(13.20)  | 78.36(2.93)    | 71.95(2.90)     |
| Path Score(pt)    | 284.47(33.47)| 447.87(75.86)  | 246.70(54.53)  | 249.43(23.10)   |
| Exp. Time (s)     | 70.66(23.82) | 145.10(71.14)  | 41.77(8.59)    | 10.34(1.57)     |
| Success rate (%)  | 19.90(7.54)  | 3.92(1.65)     | 72.88(5.34)    | 99.64(0.08)     |

### Map 6

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 127.90(15.10)| 117.19(13.14)  | 79.07(4.98)    | 71.70(3.60)     |
| Path Score(pt)    | 349.17(65.73)| 354.07(64.47)  | 217.63(27.67)  | 209.00(19.58)   |
| Exp. Time (s)     | 16.43(4.50)  | 12.78(1.73)    | 32.88(10.80)   | 7.21(1.24)      |
| Success rate (%)  | 74.26(9.58)  | 79.79(7.15)    | 75.26(7.48)    | 99.57(0.09)     |

### Map 7

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 132.44(17.37)| 91.95(14.34)   | 72.98(3.14)    | 64.72(2.66)     |
| Path Score(pt)    | 443.97(58.13)| 312.03(58.13)  | 198.87(22.22)  | 246.83(12.15)   |
| Exp. Time (s)     | 24.22(7.97)  | 2.90(0.71)     | 23.30(5.88)    | 8.73(2.32)      |
| Success rate (%)  | 27.94(13.42) | 75.78(8.46)    | 86.62(3.79)    | 99.65(0.06)     |

<50>
### Map 8

| 항목              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 142.85(17.32)| 122.63(12.71)  | 69.49(11.00)   | 80.95(1.37)     |
| Path Score(pt)    | 321.63(89.33)| 252.53(36.39)  | 154.30(24.04)  | 240.70(26.70)   |
| Exp. Time (s)     | 173.98(90.86)| 24.10(8.71)    | 47.18(8.81)    | 17.44(8.13)     |
| Success rate (%)  | 1.80(1.34)   | 32.88(10.79)   | 64.94(5.84)    | 99.67(0.07)     |

Analysis of variance (ANOVA) was conducted to compare the averages of the 
experimental methods on each map. In this case, the null hypothesis was that the 
average path length, path score, experiment time, and success rate for the four 
experimental methods are all equal. To verify the differences among the respective 
experimental methods, post hoc analysis was conducted using the Tukey method. 
Appendix 1 show result of the experimental methods on each map. 
5.1 Analysis of Map 1 Results 
Map 1 consists of hills, destructible buildings, and roads at 12–15% of the map. 
There are numerous indestructible buildings designated as obstacles located around 
the starting point. The ANOVA results for Map 1 are summarized in Table 10. 
Analysis of the results reveals statistically significant differences among the methods 
in terms of path length, path score, experiment time, and success rate, with all 
significance levels lower than 0.05. The Tukey HSD results for Map 1 are 
summarized in Table 11.

<51>
Table 10. ANOVA of Map 1
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 107,359.610    | 3   | 35,786.537   | 244.862  | <.001  |
|                       | Within Groups  | 16,953.407     | 116 | 146.150      |          |        |
|                       | Total          | 124,313.018    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 424,351.758    | 3   | 141,450.586  | 53.397   | <.001  |
|                       | Within Groups  | 307,287.033    | 116 | 2,649.026    |          |        |
|                       | Total          | 731,638.792    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 483,825.597    | 3   | 161,275.199  | 76.124   | <.001  |
|                       | Within Groups  | 245,756.015    | 116 | 2,118.586    |          |        |
|                       | Total          | 729,581.612    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 159,214.142    | 3   | 53,071.381   | 1393.065 | <.001  |
|                       | Within Groups  | 4,419.233      | 116 | 38.097       |          |        |
|                       | Total          | 163,633.375    | 119 |              |          |        |

<52>
Table 11. Post hoc analysis results for Map 1
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | 20.22067*             | <.001  | 12.0841                             | 28.3572     |
|                     | APF       | Q         | 73.36233*             | <.001  | 65.2258                             | 81.4989     |
|                     | APF       | Q-SAPF    | 61.89733*             | <.001  | 53.7608                             | 70.0339     |
|                     | SAPF      | Q         | 53.14167*             | <.001  | 45.0051                             | 61.2782     |
|                     | SAPF      | Q-SAPF    | 41.67667*             | <.001  | 33.5401                             | 49.8132     |
|                     | Q         | Q-SAPF    | -11.46500*            | .002   | -19.6015                            | -3.3285     |
| Path Score (pt)     | APF       | SAPF      | 69.10000*             | <.001  | 34.4596                             | 103.7404    |
|                     | APF       | Q         | 167.33333*            | <.001  | 132.6929                            | 201.9737    |
|                     | APF       | Q-SAPF    | 80.93333*             | <.001  | 46.2929                             | 115.5737    |
|                     | SAPF      | Q         | 98.23333*             | <.001  | 63.5929                             | 132.8737    |
|                     | SAPF      | Q-SAPF    | 11.83333              | .810   | -22.8071                            | 46.4737     |
|                     | Q         | Q-SAPF    | -86.40000*            | <.001  | -121.0404                           | -51.7596    |
| Experiment Time (s) | APF       | SAPF      | 149.87867*            | <.001  | 118.9000                            | 180.8573    |
|                     | APF       | Q         | 126.80300*            | <.001  | 95.8243                             | 157.7817    |
|                     | APF       | Q-SAPF    | 156.54400*            | <.001  | 125.5653                            | 187.5227    |
|                     | SAPF      | Q         | -23.07567             | .217   | -54.0543                            | 7.9030      |
|                     | SAPF      | Q-SAPF    | 6.66533               | .943   | -24.3133                            | 37.6440     |
|                     | Q         | Q-SAPF    | 29.74100              | .065   | -1.2377                             | 60.7197     |
| Success Rate (%)    | APF       | SAPF      | -31.07733*            | <.001  | -35.2315                            | -26.9232    |
|                     | APF       | Q         | -63.14533*            | <.001  | -67.2995                            | -58.9912    |
|                     | APF       | Q-SAPF    | -97.87367*            | <.001  | -102.0278                           | -93.7195    |
|                     | SAPF      | Q         | -32.06800*            | <.001  | -36.2222                            | -27.9138    |
|                     | SAPF      | Q-SAPF    | -66.79633*            | <.001  | -70.9505                            | -62.6422    |
|                     | Q         | Q-SAPF    | -34.72833*            | <.001  | -38.8825                            | -30.5742    |
The path lengths of Q and Q-SAPF on Map 1 are equivalent; in contrast, QSAPF
is shorter than APF by approximately 61.90, and Q-SAPF and SAPF differ in 
length by approximately 41.68. Although there was no significant difference 
between the SAPF and Q-SAPF in terms of path score, the path score of the Q-SAPF

<53>
was 80 points lower than that of the APF and 86.4 points higher than that of the QPF. 
The Tukey HSD analysis revealed that APF and SAPF were derived approximately 
149.88 seconds after Q, whereas Q-SAPF was derived approximately 156.54 
seconds after Q. The post hoc analysis of the success rate revealed significant 
average differences among all the methodologies. The success rates of Q-SAPF were 
97.87, 66.80, and 34.73% greater than those of APF, SAPF, and Q, respectively. 
Figure 8 shows the experimental results for Map 1, which represent the shortest path 
length obtained by each experiment
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 9. Experimental results for Map 1

<54>
5.2 Analysis of Map 2 Results
Like Map 1, Map 2 also consists of hills, destructible buildings, and roads on 7–
12% of the map. A river flows through the center of Map 2, and the path should be 
planned to cross the river to reach the target point.
The ANOVA results for Map 2 are summarized in Table 12. Analysis of these 
data revealed that there were statistically significant differences among the methods 
in terms of path length, path score, experiment time, and success rate, with all 
significance levels lower than 0.05. The Tukey HSD results for Map 2 are 
summarized in Table 13. The Tukey HSD results for path length indicate that there 
are differences between the path lengths obtained by APF, SAPF, Q-learning, and 
Q-SAPF. The path length of Q-SAPF is shorter than the lengths of APF, SAPF, and 
Q by 39.39, 74.62, and 8.77, respectively. The post hoc analysis of the path scores 
indicated that there was no significant difference between APF and SAPF; however, 
the path score of Q-SAPF exceeded that of Q by 37.83, and Q-SAPF had a shorter 
path length and a higher path score than did Q-SAPF.

<55>
Table 12. ANOVA of Map 2
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 102,841.463    | 3   | 34,280.488   | 431.799  | <.001  |
|                       | Within Groups  | 9,209.224      | 116 | 79.390       |          |        |
|                       | Total          | 112,050.687    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 525,684.867    | 3   | 175,228.289  | 78.199   | <.001  |
|                       | Within Groups  | 259,931.933    | 116 | 2,240.793    |          |        |
|                       | Total          | 785,616.800    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 1,207,344.102  | 3   | 402,448.034  | 203.764  | <.001  |
|                       | Within Groups  | 229,108.573    | 116 | 1,975.074    |          |        |
|                       | Total          | 1,436,452.675  | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 149,826.107    | 3   | 49,942.036   | 525.042  | <.001  |
|                       | Within Groups  | 11,033.936     | 116 | 95.120       |          |        |
|                       | Total          | 160,860.043    | 119 |              |          |        |

<55>
Table 13. Post hoc analysis results for Map 2
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -35.22633*            | <.001  | -41.2232                            | -29.2295    |
|                     | APF       | Q         | 30.62333*             | <.001  | 24.6265                             | 36.6202     |
|                     | APF       | Q-SAPF    | 39.39533*             | <.001  | 33.3985                             | 45.3922     |
|                     | SAPF      | Q         | 65.84967*             | <.001  | 59.8528                             | 71.8465     |
|                     | SAPF      | Q-SAPF    | 74.62167*             | <.001  | 68.6248                             | 80.6185     |
|                     | Q         | Q-SAPF    | 8.77200*              | .001   | 2.7752                              | 14.7688     |
| Path Score (pt)     | APF       | SAPF      | -2.16667              | .998   | -34.0263                            | 29.6929     |
|                     | APF       | Q         | 147.46667*            | <.001  | 115.6071                            | 179.3263    |
|                     | APF       | Q-SAPF    | 109.63333*            | <.001  | 77.7737                             | 141.4929    |
|                     | SAPF      | Q         | 149.63333*            | <.001  | 117.7737                            | 181.4929    |
|                     | SAPF      | Q-SAPF    | 111.80000*            | <.001  | 79.9404                             | 143.6596    |
|                     | Q         | Q-SAPF    | -37.83333*            | .013   | -69.6929                            | -5.9737     |
| Experiment Time (s) | APF       | SAPF      | -239.07900*           | <.001  | -268.9900                           | -209.1680   |
|                     | APF       | Q         | -58.27367*            | <.001  | -88.1847                            | -28.3626    |
|                     | APF       | Q-SAPF    | 11.41500              | .753   | -18.4960                            | 41.3260     |
|                     | SAPF      | Q         | 180.80533*            | <.001  | 150.8943                            | 210.7164    |
|                     | SAPF      | Q-SAPF    | 250.49400*            | <.001  | 220.5830                            | 280.4050    |
|                     | Q         | Q-SAPF    | 69.68867*             | <.001  | 39.7776                             | 99.5997     |
| Success Rate (%)    | APF       | SAPF      | 63.92500*             | <.001  | 57.3609                             | 70.4891     |
|                     | APF       | Q         | 21.40400*             | <.001  | 14.8399                             | 27.9681     |
|                     | APF       | Q-SAPF    | -33.48900*            | <.001  | -40.0531                            | -26.9249    |
|                     | SAPF      | Q         | -42.52100*            | <.001  | -49.0851                            | -35.9569    |
|                     | SAPF      | Q-SAPF    | -97.41400*            | <.001  | -103.9781                           | -90.8499    |
|                     | Q         | Q-SAPF    | -54.89300*            | <.001  | -61.4571                            | -48.3289    |
The experimental results indicate that Q-SAPF is approximately 11.4, 250.49, 
and 69.68 seconds faster than APF, SAPF, and Q, respectively. Furthermore, the 
success rates of Q-SAPF are 33.48, 97.41, and 54.89% greater than those of APF, 
SAPF, and Q, respectively. Based on the experimental results of Map 2, Q-SAPF

<56>
exhibited a high success rate and a short path length. Figure 9 shows the experimental 
results for Map 2, which represent the shortest path length obtained by each 
experiment.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 10. Experimental results for Map 2

<57>
5.3 Analysis of Map 3 Results
Map 3 comprises hills, destructible buildings, indestructible buildings, and roads, 
with flatlands in the center. The only obstacle in Map 3 is the indestructible building, 
which accounts for approximately 14% of the map. The ANOVA results for Map 3 
are summarized in Table 14. Analysis of the results revealed that there were 
statistically significant differences among the methods in terms of path length, path 
score, experiment time, and success rate, with all significance levels lower than 0.05.
Table 14. ANOVA of Map 3
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 119,061.235    | 3   | 39,687.078   | 495.710  | <.001  |
|                       | Within Groups  | 9,287.094      | 116 | 80.061       |          |        |
|                       | Total          | 128,348.329    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 1,328,485.500  | 3   | 442,828.500  | 170.917  | <.001  |
|                       | Within Groups  | 300,544.867    | 116 | 2,590.904    |          |        |
|                       | Total          | 1,629,030.367  | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 3,045,759.296  | 3   | 1,015,253.099| 151.614  | <.001  |
|                       | Within Groups  | 776,770.673    | 116 | 6,696.299    |          |        |
|                       | Total          | 3,822,529.969  | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 146,987.043    | 3   | 48,995.681   | 790.490  | <.001  |
|                       | Within Groups  | 7,189.842      | 116 | 61.981       |          |        |
|                       | Total          | 154,176.885    | 119 |              |          |        |

<58>
The post hoc analysis results for Map 3 are summarized in Table 15. On this 
map, there were no differences between Q-SAPF and Q in terms of path length, path 
score, or experiment time. However, the success rate of Q-SAPF is 31.52% greater 
than that of Q-SAPF. Furthermore, the path length of Q-SAPF is 40.97 shorter than 
that of APF and 77.7 shorter than that of SAPF. 
Table 15. Post hoc analysis results for Map 3
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -36.81367*            | <.001  | -42.8358                            | -30.7915    |
|                     | APF       | Q         | 36.79567*             | <.001  | 30.7735                             | 42.8178     |
|                     | APF       | Q-SAPF    | 40.97433*             | <.001  | 34.9522                             | 46.9965     |
|                     | SAPF      | Q         | 73.60933*             | <.001  | 67.5872                             | 79.6315     |
|                     | SAPF      | Q-SAPF    | 77.78800*             | <.001  | 71.7659                             | 83.8101     |
|                     | Q         | Q-SAPF    | 4.17867               | .274   | -1.8435                             | 10.2008     |
| Path Score (pt)     | APF       | SAPF      | -61.70000*            | <.001  | -95.9583                            | -27.4417    |
|                     | APF       | Q         | 182.10000*            | <.001  | 147.8417                            | 216.3583    |
|                     | APF       | Q-SAPF    | 167.40000*            | <.001  | 133.1417                            | 201.6583    |
|                     | SAPF      | Q         | 243.80000*            | <.001  | 209.5417                            | 278.0583    |
|                     | SAPF      | Q-SAPF    | 229.10000*            | <.001  | 194.8417                            | 263.3583    |
|                     | Q         | Q-SAPF    | -14.70000             | .679   | -48.9583                            | 19.5583     |
| Experiment Time (s) | APF       | SAPF      | -375.49500*           | <.001  | -430.5703                           | -320.4197   |
|                     | APF       | Q         | -42.04500             | .198   | -97.1203                            | 13.0303     |
|                     | APF       | Q-SAPF    | 10.81667              | .956   | -44.2586                            | 65.8920     |
|                     | SAPF      | Q         | 333.45000*            | <.001  | 278.3747                            | 388.5253    |
|                     | SAPF      | Q-SAPF    | 386.31167*            | <.001  | 331.2364                            | 441.3870    |
|                     | Q         | Q-SAPF    | 52.86167              | .065   | -2.2136                             | 107.9370    |
| Success Rate (%)    | APF       | SAPF      | 55.63200*             | <.001  | 50.3333                             | 60.9307     |
|                     | APF       | Q         | -9.85500*             | <.001  | -15.1537                            | -4.5563     |
|                     | APF       | Q-SAPF    | -41.38067*            | <.001  | -46.6794                            | -36.0820    |
|                     | SAPF      | Q         | -65.48700*            | <.001  | -70.7857                            | -60.1883    |
|                     | SAPF      | Q-SAPF    | -97.01267*            | <.001  | -102.3114                           | -91.7140    |
|                     | Q         | Q-SAPF    | -31.52567*            | <.001  | -36.8244                            | -26.2270    |

<59>
Figure 10 represents the shortest path length of each experiment on Map 3.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 11. Experimental results for Map 3
5.4 Analysis of Map 4 Results 
Map 4 does not contain destructible buildings, but other terrain types are evenly 
distributed. In Map 4, terrains that can provide concealment and cover are mostly 
located on the left-hand side with respect to the center. The ANOVA results for Map

<60>
4 are summarized in Table 16, from which it can be concluded that there is a 
statistically significant difference between the methods in terms of path length, path 
score, experiment time, and success rate, with all significance levels lower than 0.05.
Table 16. ANOVA of Map 4
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 118,046.159    | 3   | 39,348.720   | 302.251  | <.001  |
|                       | Within Groups  | 15,101.510     | 116 | 130.185      |          |        |
|                       | Total          | 133,147.669    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 383,330.025    | 3   | 127,776.675  | 58.864   | <.001  |
|                       | Within Groups  | 251,800.967    | 116 | 2,170.698    |          |        |
|                       | Total          | 635,130.992    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 74,294.900     | 3   | 24,764.967   | 37.859   | <.001  |
|                       | Within Groups  | 75,880.050     | 116 | 654.138      |          |        |
|                       | Total          | 150,174.950    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 168,749.963    | 3   | 56,249.988   | 1,503.845| <.001  |
|                       | Within Groups  | 4,338.878      | 116 | 37.404       |          |        |
|                       | Total          | 173,088.841    | 119 |              |          |        |
The post hoc analysis results for Map 4 are presented in Table 17. On this map, 
there were no differences between Q-SAPF and Q in terms of path length, path score, 
or experiment time. However, the success rate of Q-SAPF is 21.1% greater than that

<61>
of Q, and the path length of Q-SAPF is 42.64 shorter than that of APF and 75.70 
shorter than that of SAPF.
Table 17. Post hoc analysis results for Map 4
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -33.05367*            | <.001  | -40.7330                            | -25.3744    |
|                     | APF       | Q         | 40.68800*             | <.001  | 33.0087                             | 48.3673     |
|                     | APF       | Q-SAPF    | 42.64867*             | <.001  | 34.9694                             | 50.3280     |
|                     | SAPF      | Q         | 73.74167*             | <.001  | 66.0624                             | 81.4210     |
|                     | SAPF      | Q-SAPF    | 75.70233*             | <.001  | 68.0230                             | 83.3816     |
|                     | Q         | Q-SAPF    | 1.96067               | .910   | -5.7186                             | 9.6400      |
| Path Score (pt)     | APF       | SAPF      | -60.46667*            | <.001  | -91.8240                            | -29.1093    |
|                     | APF       | Q         | 80.63333*             | <.001  | 49.2760                             | 111.9907    |
|                     | APF       | Q-SAPF    | 67.33333*             | <.001  | 35.9760                             | 98.6907     |
|                     | SAPF      | Q         | 141.10000*            | <.001  | 109.7427                            | 172.4573    |
|                     | SAPF      | Q-SAPF    | 127.80000*            | <.001  | 96.4427                             | 159.1573    |
|                     | Q         | Q-SAPF    | -13.30000             | .687   | -44.6573                            | 18.0573     |
| Experiment Time (s) | APF       | SAPF      | -62.76933*            | <.001  | -79.9830                            | -45.5556    |
|                     | APF       | Q         | -34.71367*            | <.001  | -51.9274                            | -17.5000    |
|                     | APF       | Q-SAPF    | -6.69733              | .741   | -23.9110                            | 10.5164     |
|                     | SAPF      | Q         | 28.05567*             | <.001  | 10.8420                             | 45.2694     |
|                     | SAPF      | Q-SAPF    | 56.07200*             | <.001  | 38.8583                             | 73.2857     |
|                     | Q         | Q-SAPF    | 28.01633*             | <.001  | 10.8026                             | 45.2300     |
| Success Rate (%)    | APF       | SAPF      | 25.60333*             | <.001  | 21.4871                             | 29.7196     |
|                     | APF       | Q         | -47.88800*            | <.001  | -52.0042                            | -43.7718    |
|                     | APF       | Q-SAPF    | -68.98300*            | <.001  | -73.0992                            | -64.8668    |
|                     | SAPF      | Q         | -73.49133*            | <.001  | -77.6076                            | -69.3751    |
|                     | SAPF      | Q-SAPF    | -94.58633*            | <.001  | -98.7026                            | -90.4701    |
|                     | Q         | Q-SAPF    | -21.09500*            | <.001  | -25.2112                            | -16.9788    |

<62>
Figure 11 represents the shortest path length of each experiment on Map 4.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 12. Experimental results for Map 4
5.5 Analysis of Map 5 Results
Map 5 contains destructible buildings, indestructible buildings, and roads and 
accounts for 11–16% of the map. A forest where concealment and cover are possible 
is located on the right-hand side of the map. There were statistically significant

<63>
differences in the success rate among the methods, with all significance levels lower 
than 0.05. The results of the ANOVA for Map 5 are summarized in Table 18. Table 
19 summarizes the Tukey HSD results for Map 5.
Table 18. ANOVA of Map 5
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 119,199.162    | 3   | 39,733.054   | 635.663  | <.001  |
|                       | Within Groups  | 7,250.746      | 116 | 62.506       |          |        |
|                       | Total          | 126,449.908    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 819,033.767    | 3   | 273,011.256  | 105.188  | <.001  |
|                       | Within Groups  | 301,074.600    | 116 | 2,595.471    |          |        |
|                       | Total          | 1,120,108.367  | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 298,814.549    | 3   | 99,604.850   | 69.836   | <.001  |
|                       | Within Groups  | 165,447.010    | 116 | 1,426.267    |          |        |
|                       | Total          | 464,261.560    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 180,396.980    | 3   | 60,132.327   | 2,731.984| <.001  |
|                       | Within Groups  | 2,553.218      | 116 | 22.010       |          |        |
|                       | Total          | 182,950.197    | 119 |              |          |        |

<64>
Table 19. Post hoc analysis results for Map 5
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -48.56900*            | <.001  | -53.8901                            | -43.2479    |
|                     | APF       | Q         | 25.16933*             | <.001  | 19.8482                             | 30.4904     |
|                     | APF       | Q-SAPF    | 31.58500*             | <.001  | 26.2639                             | 36.9061     |
|                     | SAPF      | Q         | 73.73833*             | <.001  | 68.4172                             | 79.0594     |
|                     | SAPF      | Q-SAPF    | 80.15400*             | <.001  | 74.8329                             | 85.4751     |
|                     | Q         | Q-SAPF    | 6.41567*              | .011   | 1.0946                              | 11.7368     |
| Path Score (pt)     | APF       | SAPF      | -163.40000*           | <.001  | -197.6884                           | -129.1116   |
|                     | APF       | Q         | 37.76667*             | .025   | 3.4782                              | 72.0551     |
|                     | APF       | Q-SAPF    | 35.03333*             | .043   | .7449                               | 69.3218     |
|                     | SAPF      | Q         | 201.16667*            | <.001  | 166.8782                            | 235.4551    |
|                     | SAPF      | Q-SAPF    | 198.43333*            | <.001  | 164.1449                            | 232.7218    |
|                     | Q         | Q-SAPF    | -2.73333              | .997   | -37.0218                            | 31.5551     |
| Experiment Time (s) | APF       | Sapf      | -74.44167*            | <.001  | -99.8596                            | -49.0237    |
|                     | APF       | Q         | 28.89100*             | .019   | 3.4731                              | 54.3089     |
|                     | APF       | Q-SAPF    | 60.32233*             | <.001  | 34.9044                             | 85.7403     |
|                     | SAPF      | Q         | 103.33267*            | <.001  | 77.9147                             | 128.7506    |
|                     | SAPF      | Q-SAPF    | 134.76400*            | <.001  | 109.3461                            | 160.1819    |
|                     | Q         | Q-SAPF    | 31.43133*             | .009   | 6.0134                              | 56.8493     |
| Success Rate (%)    | APF       | SAPF      | 15.98667*             | <.001  | 12.8291                             | 19.1442     |
|                     | APF       | Q         | -52.97200*            | <.001  | -56.1296                            | -49.8144    |
|                     | APF       | Q-SAPF    | -79.73367*            | <.001  | -82.8912                            | -76.5761    |
|                     | SAPF      | Q         | -68.95867*            | <.001  | -72.1162                            | -65.8011    |
|                     | SAPF      | Q-SAPF    | -95.72033*            | <.001  | -98.8779                            | -92.5628    |
|                     | Q         | Q-SAPF    | -26.76167*            | <.001  | -29.9192                            | -23.6041    |
The results indicate that there are differences in path length among APF, SAPF, Qlearning,
and Q-SAPF. The path lengths of Q-SAPF are shorter than those of APF, 
SAPF, and Q by 31.58, 80.15, and 6.41, respectively. The post hoc analysis results 
of the path score indicate that Q-SAPF does not significantly differ from Q-SAPF.

<65>
Q-SAPF achieved an experiment time of approximately 60.32, 134.76, and 31.43 
seconds faster than APF, Sapf, and Q, respectively. Furthermore, the success rates 
of Q-SAPF are 79.73, 95.72, and 26.76% greater than those of APF, SAPF, and Q, 
respectively. Figure 12 shows the experimental results for Map 5. The results 
represent the shortest path length of each experiment.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 13. Experimental results for Map 5

<66>
5.6 Analysis of Map 6 Results 
Map 6 has a significant proportion of destructible buildings, occupying 
approximately 25% of the map, and large roads are located on the left and top-left 
regions. The ANOVA results for Map 6 are summarized in Table 20. Statistically 
significant differences in path length, path score, experiment time, and success rate 
were observed among the methods, with all significance levels lower than 0.05. The 
Tukey HSD results for Map 6 are summarized in Table 21.
Table 20. ANOVA of Map 6
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 69,266.605     | 3   | 23,088.868   | 210.619  | <.001  |
|                       | Within Groups  | 12,716.383     | 116 | 109.624      |          |        |
|                       | Total          | 81,982.988     | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 575,284.867    | 3   | 191,761.622  | 79.682   | <.001  |
|                       | Within Groups  | 279,163.000    | 116 | 2,406.578    |          |        |
|                       | Total          | 854,447.867    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 10,977.154     | 3   | 3,659.051    | 103.463  | <.001  |
|                       | Within Groups  | 4,102.427      | 116 | 35.366       |          |        |
|                       | Total          | 15,079.581     | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 12,561.673     | 3   | 4,187.224    | 84.273   | <.001  |
|                       | Within Groups  | 5,763.633      | 116 | 49.686       |          |        |
|                       | Total          | 18,325.306     | 119 |              |          |        |

<67>
Table 21. Post hoc analysis results for Map 6
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | 10.71433*             | <.001  | 3.6675                              | 17.7611     |
|                     | APF       | Q         | 48.83767*             | <.001  | 41.7909                             | 55.8845     |
|                     | APF       | Q-SAPF    | 56.20300*             | <.001  | 49.1562                             | 63.2498     |
|                     | SAPF      | Q         | 38.12333*             | <.001  | 31.0765                             | 45.1701     |
|                     | SAPF      | Q-SAPF    | 45.48867*             | <.001  | 38.4419                             | 52.5355     |
|                     | Q         | Q-SAPF    | 7.36533*              | .037   | .3185                               | 14.4121     |
| Path Score (pt)     | APF       | SAPF      | -4.90000              | .980   | -37.9171                            | 28.1171     |
|                     | APF       | Q         | 131.53333*            | <.001  | 98.5162                             | 164.5505    |
|                     | APF       | Q-SAPF    | 140.16667*            | <.001  | 107.1495                            | 173.1838    |
|                     | SAPF      | Q         | 136.43333*            | <.001  | 103.4162                            | 169.4505    |
|                     | SAPF      | Q-SAPF    | 145.06667*            | <.001  | 112.0495                            | 178.0838    |
|                     | Q         | Q-SAPF    | 8.63333               | .904   | -24.3838                            | 41.6505     |
| Experiment Time (s) | APF       | SAPF      | 3.64800               | .088   | -.3545                              | 7.6505      |
|                     | APF       | Q         | -16.45233*            | <.001  | -20.4548                            | -12.4498    |
|                     | APF       | Q-SAPF    | 9.22600*              | <.001  | 5.2235                              | 13.2285     |
|                     | SAPF      | Q         | -20.10033*            | <.001  | -24.1028                            | -16.0978    |
|                     | SAPF      | Q-SAPF    | 5.57800*              | .002   | 1.5755                              | 9.5805      |
|                     | Q         | Q-SAPF    | 25.67833*             | <.001  | 21.6758                             | 29.6808     |
| Success Rate (%)    | APF       | SAPF      | -5.52833*             | .015   | -10.2725                            | -.7842      |
|                     | APF       | Q         | -1.00200              | .946   | -5.7462                             | 3.7422      |
|                     | APF       | Q-SAPF    | -25.31033*            | <.001  | -30.0545                            | -20.5662    |
|                     | SAPF      | Q         | 4.52633               | .067   | -.2178                              | 9.2705      |
|                     | SAPF      | Q-SAPF    | -19.78200*            | <.001  | -24.5262                            | -15.0378    |
|                     | Q         | Q-SAPF    | -24.30833*            | <.001  | -29.0525                            | -19.5642    |
The results indicate that there are differences in path length among APF, SAPF, 
Q-learning, and Q-SAPF. The path lengths of Q-SAPF are 56.20, 45.48, and 7.36 
shorter than those of APF, SAPF, and Q, respectively. Although there is no 
difference between Q and Q-SAPF in terms of path score, the duration of the Q-

<68>
SAPF experiment is approximately 25.67 seconds faster, and its success rate is 24.50% 
greater than that of the Q-SAPF. Figure 13 represents the shortest path length result 
for each experiment on Map 6.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 14. Experimental results for Map 6

<69>
5.7 Analysis of Map 7 Results 
Map 7 contains a forest that covers 17.5% of the area, and the target point can be 
reached only by passing through it. The ANOVA results for Map 7 indicate that 
there are statistically significant differences among the methods in terms of path 
length, path score, experiment time, and success rate, with all significance levels 
lower than 0.05. The ANOVA results for Map 7 are summarized in Table 22.
Table 22. ANOVA of Map 7
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 81,979.484     | 3   | 27,326.495   | 208.565  | <.001  |
|                       | Within Groups  | 15,198.485     | 116 | 131.021      |          |        |
|                       | Total          | 97,177.969     | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 1,017,753.758  | 3   | 339,251.253  | 183.398  | <.001  |
|                       | Within Groups  | 214,577.567    | 116 | 1,849.807    |          |        |
|                       | Total          | 1,232,331.325  | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 10,181.692     | 3   | 3,393.897    | 130.609  | <.001  |
|                       | Within Groups  | 3,014.273      | 116 | 25.985       |          |        |
|                       | Total          | 13,195.965     | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 87,987.877     | 3   | 29,329.292   | 441.011  | <.001  |
|                       | Within Groups  | 7,714.549      | 116 | 66.505       |          |        |
|                       | Total          | 95,702.427     | 119 |              |          |        |

<70>
The post hoc analysis results in Table 23 indicate that there are significant 
differences in path length among APF, SAPF, Q-learning, and Q-SAPF. 
Table 23. Post hoc analysis results for Map 7
| 항목                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | 40.48600*             | <.001  | 32.7821                             | 48.1899     |
|                     | APF       | Q         | 59.45700*             | <.001  | 51.7531                             | 67.1609     |
|                     | APF       | Q-SAPF    | 67.72233*             | <.001  | 60.0184                             | 75.4262     |
|                     | SAPF      | Q         | 18.97100*             | <.001  | 11.2671                             | 26.6749     |
|                     | SAPF      | Q-SAPF    | 27.23633*             | <.001  | 19.5324                             | 34.9402     |
|                     | Q         | Q-SAPF    | 8.26533*              | .030   | .5614                               | 15.9692     |
| Path Score (pt)     | APF       | SAPF      | 131.93333*            | <.001  | 102.9864                            | 160.8803    |
|                     | APF       | Q         | 245.10000*            | <.001  | 216.1531                            | 274.0469    |
|                     | APF       | Q-SAPF    | 197.13333*            | <.001  | 168.1864                            | 226.0803    |
|                     | SAPF      | Q         | 113.16667*            | <.001  | 84.2197                             | 142.1136    |
|                     | SAPF      | Q-SAPF    | 65.20000*             | <.001  | 36.2531                             | 94.1469     |
|                     | Q         | Q-SAPF    | -47.96667*            | <.001  | -76.9136                            | -19.0197    |
| Experiment Time (s) | APF       | SAPF      | 21.32100*             | <.001  | 17.8901                             | 24.7519     |
|                     | APF       | Q         | .92700                | .895   | -2.5039                             | 4.3579      |
|                     | APF       | Q-SAPF    | 15.49367*             | <.001  | 12.0628                             | 18.9245     |
|                     | SAPF      | Q         | -20.39400*            | <.001  | -23.8249                            | -16.9631    |
|                     | SAPF      | Q-SAPF    | -5.82733*             | <.001  | -9.2582                             | -2.3965     |
|                     | Q         | Q-SAPF    | 14.56667*             | <.001  | 11.1358                             | 17.9975     |
| Success Rate (%)    | APF       | SAPF      | -47.83967*            | <.001  | -53.3283                            | -42.3510    |
|                     | APF       | Q         | -58.68100*            | <.001  | -64.1697                            | -53.1923    |
|                     | APF       | Q-SAPF    | -71.71100*            | <.001  | -77.1997                            | -66.2223    |
|                     | SAPF      | Q         | -10.84133*            | <.001  | -16.3300                            | -5.3527     |
|                     | SAPF      | Q-SAPF    | -23.87133*            | <.001  | -29.3600                            | -18.3827    |
|                     | Q         | Q-SAPF    | -13.03000*            | <.001  | -18.5187                            | -7.5413     |

<71>
The path lengths of Q-SAPF are 67.72, 27.23, and 8.26 shorter than those of 
APF, SAPF, and Q, respectively. The path score of Q-SAPF is 47.97 higher than that 
of Q; thus, Q-SAPF achieved a shorter path length and a higher path score than did 
Q. Furthermore, the success rates of Q-SAPF were 71.71, 23.87, and 13.03% greater 
than those of APF, SAPF, and Q, respectively. 
Figure 14 represents the shortest path length for each experiment on Map 7.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 15. Experimental results for Map 7

<72>
5.8 Analysis of Map 8 Results
Map 8 consists of a forest that accounts for 14.2% of the area and a road that 
accounts for 19.5% of the area. The road extends through the forest. The ANOVA 
results for Map 8 are summarized in Table 24. There are statistically significant 
differences among the methods in terms of path length, path score, experiment time, 
and success rate, with all significance levels lower than 0.05. The Tukey HSD results 
indicate that there are differences in terms of path length among APF, SAPF, Qlearning,
and Q-SAPF. The path lengths of Q-SAPF are 61.89 and 41.67 shorter than 
those of APF and SAPF, respectively. Although it is 11.46 times longer than that of 
Q, its path score exceeds that of Q by 86.4 points. The success rates of Q-SAPF are 
97.87, 66.79, and 34.72% greater than those of APF, SAPF, and Q, respectively. 
Figure 15 represents the shortest path length for each experiment on Map 8.

<73>
Table 24. ANOVA of Map 8
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 107,359.610    | 3   | 35,786.537   | 244.862  | <.001  |
|                       | Within Groups  | 16,953.407     | 116 | 146.150      |          |        |
|                       | Total          | 124,313.018    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 424,351.758    | 3   | 141,450.586  | 53.397   | <.001  |
|                       | Within Groups  | 307,287.033    | 116 | 2,649.026    |          |        |
|                       | Total          | 731,638.792    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 483,825.597    | 3   | 161,275.199  | 76.124   | <.001  |
|                       | Within Groups  | 245,756.015    | 116 | 2,118.586    |          |        |
|                       | Total          | 729,581.612    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 159,214.142    | 3   | 53,071.381   | 1,393.065| <.001  |
|                       | Within Groups  | 4,419.233      | 116 | 38.097       |          |        |
|                       | Total          | 163,633.375    | 119 |              |          |        |

<74>
Table 25. Post hoc analysis results for Map 8
| 항목 | (I) group | (J) group | Mean Difference (I-J) | Sig. | 95% Confidence Interval Lower Bound | Upper Bound |
|-------|-----------|-----------|----------------------|------|-------------------------------------|-------------|
| Path Length | APF | SAPF | 20.22067* | <.001 | 12.0841 | 28.3572 |
|             | APF | Q    | 73.36233* | <.001 | 65.2258 | 81.4989 |
|             | APF | Q-SAPF | 61.89733* | <.001 | 53.7608 | 70.0339 |
|             | SAPF | Q    | 53.14167* | <.001 | 45.0051 | 61.2782 |
|             | SAPF | Q-SAPF | 41.67667* | <.001 | 33.5401 | 49.8132 |
|             | Q    | Q-SAPF | -11.46500* | .002 | -19.6015 | -3.3285 |
| Path Score (pt) | APF | SAPF | 69.10000* | <.001 | 34.4596 | 103.7404 |
|             | APF | Q    | 167.33333* | <.001 | 132.6929 | 201.9737 |
|             | APF | Q-SAPF | 80.93333* | <.001 | 46.2929 | 115.5737 |
|             | SAPF | Q    | 98.23333* | <.001 | 63.5929 | 132.8737 |
|             | SAPF | Q-SAPF | 11.83333 | .810 | -22.8071 | 46.4737 |
|             | Q    | Q-SAPF | -86.40000* | <.001 | -121.0404 | -51.7596 |
| Experiment Time (s) | APF | SAPF | 149.87867* | <.001 | 118.9000 | 180.8573 |
|             | APF | Q    | 126.80300* | <.001 | 95.8243 | 157.7817 |
|             | APF | Q-SAPF | 156.54400* | <.001 | 125.5653 | 187.5227 |
|             | SAPF | Q    | -23.07567 | .217 | -54.0543 | 7.9030 |
|             | SAPF | Q-SAPF | 6.66533 | .943 | -24.3133 | 37.6440 |
|             | Q    | Q-SAPF | 29.74100 | .065 | -1.2377 | 60.7197 |
| Success Rate (%) | APF | SAPF | -31.07733* | <.001 | -35.2315 | -26.9232 |
|             | APF | Q    | -63.14533* | <.001 | -67.2995 | -58.9912 |
|             | APF | Q-SAPF | -97.87367* | <.001 | -102.0278 | -93.7195 |
|             | SAPF | Q    | -32.06800* | <.001 | -36.2222 | -27.9138 |
|             | SAPF | Q-SAPF | -66.79633* | <.001 | -70.9505 | -62.6422 |
|             | Q    | Q-SAPF | -34.72833* | <.001 | -38.8825 | -30.5742 |

<75>
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 16. Experimental results for Map 8

<76>
5.9 Overall Review of the Analysis Results
When comparing Q-learning with APF and SAPF, Q-learning was shown to 
deduce shorter path lengths. This is due to the occurrence of oscillations in APF and 
SAPF, resulting in longer path lengths than those of Q-learning and Q-SAPF. In 
comparison, between Q-learning and Q-SAPF, the path length of Q-SAPF was found 
to be relatively lower overall. Figure 16 displays these results.
Figure 16. Path length results per map

<77>
Table 26. Path length results per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 113.16(6.79)  | 141.30(13.96) | 80.70(5.82)  | 78.31(4.55)   |
| Map 2 | 112.15(11.25) | 147.37(12.53) | 81.52(4.41)  | 72.75(3.81)   |
| Map 3 | 117.22(13.58) | 154.04(10.89) | 80.43(3.05)  | 76.25(2.79)   |
| Map 4 | 106.29(14.61) | 139.34(17.08) | 65.60(3.71)  | 63.64(1.41)   |
| Map 5 | 103.53(7.68)  | 152.10(13.20) | 78.36(2.93)  | 71.95(2.90)   |
| Map 6 | 127.90(15.10) | 117.19(13.14) | 79.07(4.98)  | 71.70(3.60)   |
| Map 7 | 132.44(17.37) | 91.95(14.34)  | 72.98(3.14)  | 64.72(2.66)   |
| Map 8 | 142.85(17.32) | 122.63(12.71) | 69.49(11.00) | 80.95(1.37)   |
Differences were confirmed through ANOVA and post hoc tests for each map. 
Q-SAPF showed a difference ranging from 31.58 to 67.72 compared to APF, 
depending on the map. In the case of APF, it appeared that differences arose due to 
the force pushing obstacles away while searching for a path. Compared to SAPF, 
differences ranged from 27.23 to 80.15 depending on the map. For SAPF, strong 
oscillations between obstacles resulted in a longer path length. However, when 
comparing Q and path length, the difference in results for maps 3 and 4 (refer to 
tables 15 and 17) was not significant. This finding suggests that there might be no 
difference in path lengths between Q and Q-SAPF, depending on the type of map 
and the locations of the start point and target points.

<78>
Compared to the APF and SAPF, the use of Q-learning resulted in lower path 
scores. Like in the previous results, oscillations occurring in the APF and SAPF 
resulted in higher path scores than did those in the Q-learning and Q-SAPF methods. 
When comparing Q-learning and Q-SAPF, the overall results of Q-SAPF were 
greater than those of Q-learning. These results are shown in Figure 17.
Figure 17. Path score(pt) results per map

<79>
Table 27. Path score(pt) results per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 280.20(73.21) | 369.00(83.14) | 215.33(39.83)| 233.03(23.29) |
| Map 2 | 338.13(64.90) | 340.30(48.36) | 190.67(43.43)| 228.50(22.96) |
| Map 3 | 407.17(54.16) | 468.87(73.65) | 225.07(35.96)| 239.77(26.70) |
| Map 4 | 286.53(40.61) | 347.00(75.50) | 205.90(25.31)| 219.20(26.31) |
| Map 5 | 284.47(33.47) | 447.87(75.86) | 246.70(54.53)| 249.43(23.10) |
| Map 6 | 349.17(65.73) | 354.07(64.47) | 217.63(27.67)| 209.00(19.58) |
| Map 7 | 443.97(58.13) | 312.03(58.13) | 198.87(22.22)| 246.83(12.15) |
| Map 8 | 321.63(89.33) | 252.53(36.39) | 154.30(24.04)| 240.70(26.70) |

Path scores for Q-SAPF were found to be lower, ranging from 35.04 points to 
197.14 points compared to APF, and 65.02 points to 229.1 points lower than SAPF. 
These differences were confirmed through ANOVA and post hoc tests. APF and 
SAPF exhibited higher points due to oscillations around obstacles that hindered 
quick escape. This suggests that using Q-SAPF could enable quicker escapes without 
lingering around obstacles. However, careful examination is needed for the path 
scores between Q and Q-SAPF. In the cases of maps 3, 4, 5, and 6, the differences 
in points were not significant (refer to figures 15, 17, 19, and 21). This observation 
is attributed to the dispersed distribution of obstacles across the maps. If the target 
point changes, differences might be observed.

<80>
According to Figure 16, the experiment time, with the exception of SAPF, 
appeared to be similar across each map for the different methodologies. Q-SAPF 
seems to derive paths faster than APF, SAPF, and Q. In particular, it shows about a 
20-second difference compared to Q.
Figure 17. Experiment time(s) per map
APF and SAPF showed significant differences in experiment time depending on 
the map. This appears to be due to the varying forces arising from the distribution of 
obstacles on the map, which also resulted in longer experiment times. Q and Q-SAPF, 
on the other hand, incurred penalties due to obstacles, facilitating faster pathfinding.

<81>
Table 28. Experiment time(s) per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 29.50(8.39)   | 95.39(41.96)  | 40.86(13.33) | 16.56(9.56)   |
| Map 2 | 27.91(9.44)   | 266.99(85.63) | 86.19(20.91) | 16.50(6.43)   |
| Map 3 | 26.17(8.85)   | 401.66(162.16)| 68.21(20.16) | 15.35(2.46)   |
| Map 4 | 11.35(3.31)   | 74.12(48.53)  | 46.07(12.49) | 18.05(9.69)   |
| Map 5 | 70.66(23.82)  | 145.10(71.14) | 41.77(8.59)  | 10.34(1.57)   |
| Map 6 | 16.43(4.50)   | 12.78(1.73)   | 32.88(10.80) | 7.21(1.24)    |
| Map 7 | 24.22(7.97)   | 2.90(0.71)    | 23.30(5.88)  | 8.73(2.32)    |
| Map 8 | 173.98(90.86) | 24.10(8.71)   | 47.18(8.81)  | 17.44(8.13)   |
It is worth noting that the success rate of Q-SAFP was consistently high, ranging 
from 99.5% to 99.7%. On the other hand, the success rate of Q varied significantly 
depending on the terrain, ranging from 44.76 to 86.62%, and did not exceed 90% in 
any of the experiments. 
Figure 18. Success rate(%) per map

<82>
Table 29. Success rate(%) per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 30.71(9.84)   | 7.00(3.25)    | 72.14(6.29)  | 99.65(0.06)   |
| Map 2 | 66.16(14.55)  | 2.23(0.78)    | 44.76(12.97) | 99.65(0.06)   |
| Map 3 | 58.23(13.31)  | 2.60(1.37)    | 68.09(8.31)  | 99.61(0.06)   |
| Map 4 | 30.76(8.76)   | 5.16(6.26)    | 78.65(5.81)  | 99.74(0.05)   |
| Map 5 | 19.90(7.54)   | 3.92(1.65)    | 72.88(5.34)  | 99.64(0.08)   |
| Map 6 | 74.26(9.58)   | 79.79(7.15)   | 75.26(7.48)  | 99.57(0.09)   |
| Map 7 | 27.94(13.42)  | 75.78(8.46)   | 86.62(3.79)  | 99.65(0.06)   |
| Map 8 | 1.80(1.34)    | 32.88(10.79)  | 64.94(5.84)  | 99.67(0.07)   |
Q-SAPF has shown superior performance compared to other methodologies. 
APF and SAPF exhibited varying differences in success rates depending on the map. 
For instance, APF struggled on map 8 due to the presence of large horizontal 
obstacles that impeded movement. In contrast, SAPF, unlike APF, was affected by 
various forces due to scattered obstacles, influencing its pathfinding capabilities. 
When compared to Q, differences in success rates were observed across all maps. Q 
likely encountered difficulties in pathfinding when encountering obstacles, incurring 
penalties. However, Q-SAPF facilitated movement close to obstacles, thereby 
reducing penalties and achieving higher success rates.

<84>
The experimental results indicate that the proposed Q-SAPF method 
outperforms other methods in terms of path length, path score, experiment time, and 
success rate during learning. Therefore, the proposed method is highly effective for 
path planning in various terrain environments where concealment and cover are 
possible. 
In this study, the Friedman test was performed. As a result, the rank was 
derived. The rankings are all statistically significant. Table 26 shows the results. 
Q-SAPF is ranked at the top overall. 
Table 30. Rank result of Friedman test
| Map   | Metric            | APF  | SAPF | Q    | Q-SAPF | Chi-Square | Asymp. Sig. |
|-------|-------------------|------|------|------|--------|------------|-------------|
| Map 1 | Path length       | 3.03 | 3.97 | 1.67 | 1.33   | 80.840     | <.001       |
|       | Path Score        | 2.63 | 3.67 | 1.67 | 2.03   | 41.240     | <.001       |
|       | Exp. Time (s)     | 2.17 | 3.90 | 2.80 | 1.13   | 72.520     | <.001       |
|       | Success rate (%)  | 3.00 | 4.00 | 2.00 | 1.00   | 90.000     | <.001       |
| Map 2 | Path length       | 3.03 | 3.97 | 1.93 | 1.07   | 86.600     | <.001       |
|       | Path Score        | 3.47 | 3.50 | 1.23 | 1.80   | 72.520     | <.001       |
|       | Exp. Time (s)     | 1.93 | 4.00 | 2.97 | 1.10   | 85.480     | <.001       |
|       | Success rate (%)  | 2.17 | 4.00 | 2.83 | 1.00   | 85.000     | <.001       |
| Map 3 | Path length       | 3.03 | 3.97 | 1.83 | 1.17   | 83.840     | <.001       |
|       | Path Score        | 3.27 | 3.73 | 1.42 | 1.58   | 74.458     | <.001       |
|       | Exp. Time (s)     | 1.90 | 4.00 | 3.00 | 1.10   | 86.760     | <.001       |
|       | Success rate (%)  | 2.73 | 4.00 | 2.27 | 1.00   | 82.960     | <.001       |

<85>
| Map   | Metric            | APF  | SAPF | Q    | Q-SAPF | Chi-Square | Asymp. Sig. |
|-------|-------------------|------|------|------|--------|------------|-------------|
| Map 4 | Path length       | 3.10 | 3.90 | 1.70 | 1.30   | 79.200     | <.001       |
|       | Path Score        | 3.07 | 3.73 | 1.43 | 1.77   | 63.320     | <.001       |
|       | Exp. Time (s)     | 1.33 | 3.53 | 3.37 | 1.77   | 66.920     | <.001       |
|       | Success rate (%)  | 3.03 | 3.97 | 2.00 | 1.00   | 88.840     | <.001       |
| Map 5 | Path length       | 3.00 | 4.00 | 1.93 | 1.07   | 87.760     | <.001       |
|       | Path Score        | 2.47 | 4.00 | 1.80 | 1.73   | 60.322     | <.001       |
|       | Exp. Time (s)     | 2.93 | 3.93 | 2.13 | 1.00   | 83.280     | <.001       |
|       | Success rate (%)  | 3.00 | 4.00 | 2.00 | 1.00   | 90.000     | <.001       |
| Map 6 | Path length       | 3.70 | 3.30 | 1.93 | 1.07   | 80.200     | <.001       |
|       | Path Score        | 3.43 | 3.57 | 1.77 | 1.23   | 75.221     | <.001       |
|       | Exp. Time (s)     | 2.80 | 2.20 | 4.00 | 1.00   | 84.240     | <.001       |
|       | Success rate (%)  | 3.22 | 2.65 | 3.13 | 1.00   | 57.562     | <.001       |
| Map 7 | Path length       | 3.93 | 3.03 | 2.00 | 1.03   | 85.320     | <.001       |
|       | Path Score        | 3.97 | 3.00 | 1.07 | 1.97   | 85.320     | <.001       |
|       | Exp. Time (s)     | 3.50 | 1.00 | 3.47 | 2.03   | 79.240     | <.001       |
|       | Success rate (%)  | 4.00 | 2.87 | 2.13 | 1.00   | 85.840     | <.001       |
| Map 8 | Path length       | 3.87 | 3.13 | 1.23 | 1.77   | 79.400     | <.001       |
|       | Path Score        | 3.60 | 2.80 | 1.07 | 2.53   | 60.400     | <.001       |
|       | Exp. Time (s)     | 4.00 | 1.80 | 2.93 | 1.27   | 80.080     | <.001       |
|       | Success rate (%)  | 4.00 | 3.00 | 2.00 | 1.00   | 90.000     | <.001       |

<86>
CHAPTER 6. Conclusion and Future 
Directions
This study proposed a path planning approach for effective concealment and 
cover in wartime situations using Q-learning and SAPF. The experimental results 
confirmed that the proposed method minimized the final path length and algorithm 
execution time more than did the general Q-learning method. The path score for 
verifying concealment and cover was also relatively higher. Furthermore, the 
proposed method demonstrated outstanding results on maps with varying 
characteristics. Therefore, the proposed method can be effectively used for military 
supply transportation or troop advancement in wartime situations.
The algorithm proposed in this study analyzed different paths in a grid 
environment. In future studies, the proposed algorithm can be more effectively 
applied to path planning in a linear environment. Additionally, path planning in this 
study considered all mobile vehicles that can be operated on the ground. For more 
realistic path planning, additional studies must be conducted in which the 
characteristics of mobile vehicles operable on the ground, such as tanks, armored 
vehicles, and reconnaissance vehicles, are taken into account.

<87>
REFERENCES
[1] W. Dawid and K. Pokonieczny, “Methodology of using terrain passability maps for 
planning the movement of troops and navigation of unmanned ground vehicles,” 
Sensors, vol. 21, no. 14, 2021, doi: 10.3390/s21144682.
[2] F. J. Guadalupe, “Manned-unmanned teaming,” Aviation Digest, vol. 2, no. 3, 2014.
[3] Son In-geun, Lee Tae-gyun, and Lim Jae-seong, “Technological trends of manned 
and unmanned combined combat system (MUM-T) for implementation of Mosaic 
warfare,,” The Journal of The Korean Institute of Communication Sciences, vol. 40, 
no. 4, pp. 22–30, 2023, [Online]. Available: 
http://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11342843
[4] R. Zhao, Y. Wang, G. Xiao, C. Liu, P. Hu, and H. Li, “A method of path planning 
for unmanned aerial vehicle based on the hybrid of selfish herd optimizer and particle 
swarm optimizer,” Applied Intelligence, vol. 52, no. 14, 2022, doi: 10.1007/s10489-
021-02353-y.
[5] D. Zhuoning, Z. Rulin, C. Zongji, and Z. Rui, “Study on UAV Path Planning 
Approach Based on Fuzzy Virtual Force,” Chinese Journal of Aeronautics, vol. 23, 
no. 3, 2010, doi: 10.1016/S1000-9361(09)60225-9.
[6] N. Tuśnio and W. Wróblewski, “The efficiency of drones usage for safety and rescue 
operations in an open area: A case from Poland,” Sustainability (Switzerland), vol. 
14, no. 1, 2022, doi: 10.3390/su14010327.
[7] Y. Singh, S. Sharma, R. Sutton, D. Hatton, and A. Khan, “A constrained A* approach 
towards optimal path planning for an unmanned surface vehicle in a maritime 
environment containing dynamic obstacles and ocean currents,” Ocean Engineering, 
vol. 169, 2018, doi: 10.1016/j.oceaneng.2018.09.016.
[8] R. L. Wernli, “Recent US Navy underwater vehicle projects,” Space and Naval 77
Warfare Systems Center Report, 2002.
[9] Y. Zheng, “Multimachine Collaborative Path Planning Method Based on A∗
Mechanism Connection Depth Neural Network Model,” IEEE Access, vol. 10, 2022, 
doi: 10.1109/ACCESS.2022.3168719.
[10] B. Zhang, G. Li, Q. Zheng, X. Bai, Y. Ding, and A. Khan, “Path Planning for 
Wheeled Mobile Robot in Partially Known Uneven Terrain,” Sensors, vol. 22, no. 
14, 2022, doi: 10.3390/s22145217.
[11] J. Han, “A surrounding point set approach for path planning in unknown 
environments,” Comput Ind Eng, vol. 133, pp. 121–130, 2019.
[12] X. Chen, M. Zhao, and L. Yin, “Dynamic path planning of the UAV avoiding static 
and moving obstacles,” J Intell Robot Syst, vol. 99, pp. 909–931, 2020.
[13] P. Chen, J. Pei, W. Lu, and M. Li, “A deep reinforcement learning based method for 
real-time path planning and dynamic obstacle avoidance,” Neurocomputing, vol. 497, 
pp. 64–75, 2022.
[14] R. Huang, C. Qin, J. L. Li, and X. Lan, “Path planning of mobile robot in unknown 
dynamic continuous environment using reward‐modified deep Q‐network,” Optim 
Control Appl Methods, vol. 44, no. 3, pp. 1570–1587, 2023.
[15] J. Chen, Y. Zhang, L. Wu, T. You, and X. Ning, “An Adaptive Clustering-Based 
Algorithm for Automatic Path Planning of Heterogeneous UAVs,” IEEE 
Transactions on Intelligent Transportation Systems, vol. 23, no. 9, 2022, doi: 
10.1109/TITS.2021.3131473.
[16] Y. Zhao, Z. Zheng, and Y. Liu, “Survey on computational-intelligence-based UAV 
path planning,” Knowl Based Syst, vol. 158, 2018, doi: 
10.1016/j.knosys.2018.05.033.78
[17] J. Chen, X. Zhang, B. Xin, and H. Fang, “Coordination Between Unmanned Aerial 
and Ground Vehicles: A Taxonomy and Optimization Perspective,” IEEE Trans 
Cybern, vol. 46, no. 4, 2016, doi: 10.1109/TCYB.2015.2418337.
[18] X. Bai, H. Jiang, J. Cui, K. Lu, P. Chen, and M. Zhang, “UAV Path Planning Based 
on Improved A ∗ and DWA Algorithms,” International Journal of Aerospace 
Engineering, vol. 2021, 2021, doi: 10.1155/2021/4511252.
[19] M. D. Phung and Q. P. Ha, “Safety-enhanced UAV path planning with spherical 
vector-based particle swarm optimization,” Appl Soft Comput, vol. 107, 2021, doi: 
10.1016/j.asoc.2021.107376.
[20] B. Han et al., “Grid-optimized UAV indoor path planning algorithms in a complex 
environment,” International Journal of Applied Earth Observation and 
Geoinformation, vol. 111. 2022. doi: 10.1016/j.jag.2022.102857.
[21] I. Bae and J. Hong, “Survey on the Developments of Unmanned Marine Vehicles: 
Intelligence and Cooperation,” Sensors, vol. 23, no. 10. 2023. doi: 
10.3390/s23104643.
[22] F. Wang, Y. Bai, and L. Zhao, “Physical Consistent Path Planning for Unmanned 
Surface Vehicles under Complex Marine Environment,” J Mar Sci Eng, vol. 11, no. 
6, 2023, doi: 10.3390/jmse11061164.
[23] Y. Chen, G. Bai, Y. Zhan, X. Hu, and J. Liu, “Path Planning and Obstacle Avoiding 
of the USV Based on Improved ACO-APF Hybrid Algorithm with Adaptive EarlyWarning,”
IEEE Access, vol. 9, 2021, doi: 10.1109/ACCESS.2021.3062375.
[24] X. Liu, Y. Li, J. Zhang, J. Zheng, and C. Yang, “Self-Adaptive Dynamic Obstacle 
Avoidance and Path Planning for USV under Complex Maritime Environment,” 
IEEE Access, vol. 7, 2019, doi: 10.1109/ACCESS.2019.2935964.
[25] H. Guo, Z. Mao, W. Ding, and P. Liu, “Optimal search path planning for unmanned 79
surface vehicle based on an improved genetic algorithm,” Computers and Electrical 
Engineering, vol. 79, 2019, doi: 10.1016/j.compeleceng.2019.106467.
[26] X. Bai, B. Li, X. Xu, and Y. Xiao, “USV path planning algorithm based on plant 
growth,” Ocean Engineering, vol. 273, 2023, doi: 10.1016/j.oceaneng.2023.113965.
[27] J. Sun, G. Liu, G. Tian, and J. Zhang, “Smart obstacle avoidance using a danger index 
for a dynamic environment,” Applied Sciences (Switzerland), vol. 9, no. 8, 2019, doi: 
10.3390/app9081589.
[28] Y. Koren and J. Borenstein, “Potential field methods and their inherent limitations 
for mobile robot navigation,” in Proceedings - IEEE International Conference on 
Robotics and Automation, 1991. doi: 10.1109/robot.1991.131810.
[29] O. Khatib, “Real-time obstacle avoidance for manipulators and mobile robots,” in 
Proceedings - IEEE International Conference on Robotics and Automation, 1985. 
doi: 10.1109/ROBOT.1985.1087247.
[30] Q. Yao et al., “Path Planning Method with Improved Artificial Potential Field - A 
Reinforcement Learning Perspective,” IEEE Access, vol. 8, 2020, doi: 
10.1109/ACCESS.2020.3011211.
[31] Z. Wu, J. Dai, B. Jiang, and H. R. Karimi, “Robot path planning based on artificial 
potential field with deterministic annealing,” ISA Trans, vol. 138, 2023, doi: 
10.1016/j.isatra.2023.02.018.
[32] T. Luan, Z. Tan, B. You, M. Sun, and H. Yao, “Path planning of unmanned surface 
vehicle based on artificial potential field approach considering virtual target points,” 
Transactions of the Institute of Measurement and Control, vol. 46, no. 6, 2024, doi: 
10.1177/01423312231190208.
[33] M. Guan, F. X. Yang, J. C. Jiao, and X. P. Chen, “Research on path planning of 
mobile robot based on improved Deep Q Network,” in Journal of Physics: 80
Conference Series, 2021. doi: 10.1088/1742-6596/1820/1/012024.
[34] U. Orozco-Rosas, O. Montiel, and R. Sepúlveda, “Mobile robot path planning using 
membrane evolutionary artificial potential field,” Applied Soft Computing Journal, 
vol. 77, 2019, doi: 10.1016/j.asoc.2019.01.036.
[35] H. Y. Zhang, W. M. Lin, and A. X. Chen, “Path planning for the mobile robot: A 
review,” Symmetry (Basel), vol. 10, no. 10, 2018, doi: 10.3390/sym10100450.
[36] X. Fan, Y. Guo, H. Liu, B. Wei, and W. Lyu, “Improved Artificial Potential Field 
Method Applied for AUV Path Planning,” Math Probl Eng, vol. 2020, 2020, doi: 
10.1155/2020/6523158.
[37] P. Wang, S. Gao, L. Li, B. Sun, and S. Cheng, “Obstacle avoidance path planning 
design for autonomous driving vehicles based on an improved artificial potential field 
algorithm,” Energies (Basel), vol. 12, no. 12, 2019, doi: 10.3390/en12122342.
[38] Y. Shin and E. Kim, “Hybrid path planning using positioning risk and artificial 
potential fields,” Aerosp Sci Technol, vol. 112, 2021, doi: 10.1016/j.ast.2021.106640.
[39] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Mach Learn, vol. 8, no. 3, pp. 279–
292, 1992, doi: 10.1007/BF00992698.
[40] J. Clifton and E. Laber, “Q-Learning: Theory and applications,” Annual Review of 
Statistics and Its Application, vol. 7. 2020. doi: 10.1146/annurev-statistics-031219-
041220.
[41] B. Jang, M. Kim, G. Harerimana, and J. W. Kim, “Q-Learning Algorithms: A 
Comprehensive Classification and Applications,” IEEE Access, vol. 7, 2019, doi: 
10.1109/ACCESS.2019.2941229.
[42] E. S. Low, P. Ong, and K. C. Cheah, “Solving the optimal path planning of a mobile 
robot using improved Q-learning,” Rob Auton Syst, vol. 115, 2019, doi: 
10.1016/j.robot.2019.02.013.81
[43] J. Jiang and J. Xin, “Path planning of a mobile robot in a free-space environment 
using Q-learning,” Progress in Artificial Intelligence, vol. 8, no. 1, 2019, doi: 
10.1007/s13748-018-00168-6.
[44] Q. Wei, F. L. Lewis, Q. Sun, P. Yan, and R. Song, “Discrete-time deterministic Qlearning:
A novel convergence analysis,” IEEE Trans Cybern, vol. 47, no. 5, 2017, 
doi: 10.1109/TCYB.2016.2542923.
[45] E. S. Low, P. Ong, C. Y. Low, and R. Omar, “Modified Q-learning with distance 
metric and virtual target on path planning of mobile robot,” Expert Syst Appl, vol. 
199, 2022, doi: 10.1016/j.eswa.2022.117191.