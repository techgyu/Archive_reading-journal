# ë¼ì´ì„¼ìŠ¤ í‘œê¸°

ì´ìš©ìëŠ” ì•„ë˜ì˜ ì¡°ê±´ì„ ë”°ë¥´ëŠ” ê²½ìš°ì— í•œí•˜ì—¬ ììœ ë¡­ê²Œ:
- ì´ ì €ì‘ë¬¼ì„ ë³µì œ, ë°°í¬, ì „ì†¡, ì „ì‹œ, ê³µì—° ë° ë°©ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.**
- **ì €ì‘ìí‘œì‹œ:** ê·€í•˜ëŠ” ì›ì €ì‘ìë¥¼ í‘œì‹œí•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
- **ë¹„ì˜ë¦¬:** ê·€í•˜ëŠ” ì´ ì €ì‘ë¬¼ì„ ì˜ë¦¬ ëª©ì ìœ¼ë¡œ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
- **ë³€ê²½ê¸ˆì§€:** ê·€í•˜ëŠ” ì´ ì €ì‘ë¬¼ì„ ê°œì‘, ë³€í˜• ë˜ëŠ” ê°€ê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

---

# ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|---|---|
| **ë…¼ë¬¸ ì œëª©** | Q-learning-based Strategic Artificial Potential Field for Path Planning in Battlefield Environments <br> = Q-learningê¸°ë°˜ì˜ SAPFë¥¼ ì´ìš©í•œ ì§€ìƒì „ì¥í™˜ê²½ì—ì„œì˜ ê²½ë¡œê³„íš |
| **ì €ì** | Jisun Lee |
| **ë°œí–‰ì‚¬í•­** | ì„œìš¸ : ê³ ë ¤ëŒ€í•™êµ ëŒ€í•™ì›, 2024 |
| **í•™ìœ„ë…¼ë¬¸ì‚¬í•­** | í•™ìœ„ë…¼ë¬¸(ë°•ì‚¬) -- ê³ ë ¤ëŒ€í•™êµ ëŒ€í•™ì›, ì‚°ì—…ê²½ì˜ê³µí•™ê³¼ ê²½ì˜ê³µí•™ì „ê³µ, 2024. 8 |
| **ë°œí–‰ì—°ë„** | 2024 |
| **ì‘ì„±ì–¸ì–´** | ì˜ì–´ |
| **ì£¼ì œì–´** | Path planning ; Artificial potential field ; Q-learning ; Concealment and cover ; Ground battlefield environments |
| **ë°œí–‰êµ­(ë„ì‹œ)** | ì„œìš¸ |
| **í˜•íƒœì‚¬í•­** | xii, 81p ; 26 cm |
| **ì¼ë°˜ì£¼ê¸°ëª…** | ì§€ë„êµìˆ˜: Yoonho Seo |
| **UCIì‹ë³„ì½”ë“œ** | I804:11009-000000289146 |
| **DOIì‹ë³„ì½”ë“œ** | 10.23186/korea.000000289146.11009.0001570 |

---
<1>
Doctoral Dissertation
Q-learning-based Strategic Artificial 
Potential Field for Path Planning in 
Battlefield Environments
Jisun Lee
Department of Industrial and Management Engineering
Graduate School
Korea University
August 2024Q-learning-based Strategic Artificial 
Potential Field for Path Planning in 
Battlefield Environments
by 
Jisun Lee

under the supervision of Professor Yoonho Seo
A dissertation submitted in partial fulfillment of 
the requirements for the degree of 
Doctor of Philosophy.
Department of Industrial and Management Engineering
Graduate School


Korea University
April 2024

<2>
The dissertation of Jisun Lee has been approved by 
the dissertation committee in partial fulfillment of 
the requirements for the degree of 
Doctor of Philosophy.
June 2024
__________________________
Committee Chair: Yoonho Seo
__________________________
Committee Member: Jun-Geol Baek
__________________________
Committee Member: Taesu Cheong
__________________________
Committee Member: Chang Ouk Kim
__________________________
Committee Member: Hyun Woo Jeon

<3>
Q-learning-based Strategic Artificial Potential Field for 
Path Planning in Battlefield Environments
by Jisun Lee
Department of Industrial and Management Engineering
under the supervision of Professor Yoonho Seo
ABSTRACT
Path planning in battlefield environments differs from typical path planning 
because it may not always involve obstacle avoidance. In fact, obstacles can be 
utilized to hide from enemies or facilitate troop advancement through destruction 
and relocation. Therefore, path planning in such environments requires specificity. 
One widely used method for obstacle avoidance is the artificial potential field (APF) 
method. This study proposes a strategic artificial potential field (SAPF) algorithm 
that utilizes obstacles for path planning. The Q-learning algorithm is also being 
researched for its application in learning methods involving path planning and 
obstacle avoidance. This study proposes a path planning algorithm utilizing SAPFbased
Q-learning(Q-SAPF) that takes into account concealment and cover in various 
terrains.
The proposed Q-SAPF algorithm was compared to Q-learning based on the 

<4>
results of repeated experiments and analysis of variance (ANOVA) and the post hoc 
Tukey technique to examine the statistical significance and differences between the 
algorithms, respectively. Q-SAPF outperformed Q-learning by generating shorter 
path plans with differences in path lengths of 1.96â€“8.77. It further achieved faster 
learning times of approximately 14.57â€“69.29 seconds and path scores that were 
higher by approximately 2.73-47.96. Specifically, its success rate was 13.73â€“54.89% 
higher than that of Q-learning. The achieved outcome demonstrated the superior 
learning ability of the Q-SAPF.
The study results confirm the feasibility of path planning that incorporates 
concealment and cover by utilizing obstacles. The findings of this study can 
contribute to path planning in various situations beyond wartime environments.
Keywords: Path planning, Artificial potential field, Q-learning, Concealment 
and cover, Ground battlefield environments

<5>
Q-learning ê¸°ë°˜ì˜ SAPF ë¥¼ ì´ìš©í•œ
ì§€ìƒì „ì¥í™˜ê²½ì—ì„œì˜ ê²½ë¡œê³„íš
ì´ ì§€ ì„ 
ì‚° ì—… ê²½ ì˜ ê³µ í•™ ê³¼
ì§€ë„êµìˆ˜: ì„œ ìœ¤ í˜¸
êµ­ë¬¸ ì´ˆë¡
ì „ì¥í™˜ê²½ì—ì„œì˜ ê²½ë¡œê³„íšì€ê³¼ ë‹¬ë¦¬ íŠ¹ìˆ˜ì„±ì„ ì§€ë‹Œë‹¤. ê¸°ì¡´ì˜ ê²½ë¡œê³„íšì´ ì¥ì• ë¬¼ì€ ê¼­
í”¼í•´ì•¼ í•œë‹¤ë©´, ì „ì¥í™˜ê²½ì—ì„œëŠ” ì´ë¥¼ ì´ìš©í•´ì•¼í•œë‹¤. ì¥ì• ë¬¼ì„ ì´ìš©í•˜ì—¬ ì ìœ¼ë¡œë¶€í„°
ìˆ¨ê¸°ë„í•˜ê³ , ë•Œë¡œëŠ” ë¹ ë¥¸ ì´ë™ì„ ìœ„í•´ íŒŒê´´í•˜ê¸°ë„ í•œë‹¤.
íŠ¹íˆ, ì§€ìƒì—ì„œì˜ ì§€í˜• í™œìš©ì€ êµ°ì‚¬ì‘ì „ì—ì„œ ì¤‘ìš”í•œ ìš”ì†Œì¤‘ í•˜ë‚˜ì´ë©°, ì§€í˜•ì˜ íŠ¹ì„±ì„
íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ê²ƒì€ ì „ì‹œì—ì„œ ìŠ¹ë¦¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” í™•ë¥ ì„ ë†’ì¸ë‹¤. 
ì§€ìƒì „ì¥ì—ì„œëŠ” ê³µì¤‘ì´ë‚˜ í•´ìƒì˜ ë¹„í•´ ì ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ í˜ë“¤ë©°, ì  ì—­ì‹œ ì•„êµ°ì˜
ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ í˜ë“¤ë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ì§€í˜•ì˜ íŠ¹ì„±ì„ ì˜ í™œìš©í•­ ì€íì™€ ì—„íë¥¼ í•˜ë©°
ì´ë™í•˜ëŠ” ê²ƒì€ í•„ìˆ˜ì ì´ë‹¤.
ë³¸ ì—°êµ¬ëŠ” ì§€ìƒ ì „ì¥í™˜ê²½ì—ì„œì˜ ì§€í˜•ì„ í™œìš©í•œ ê²½ë¡œê³„íšì„ ë‹¤ë£¬ë‹¤

<6>
ì¸ê³µì „ìœ„ì¥ë²•(Artificial Potential Field, APF)ëŠ” ì¥ì• ë¬¼ íšŒí”¼ë¥¼ ìœ„í•´ ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆë‹¤. 
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¥ì• ë¬¼ì„ í™œìš©í•˜ì—¬ ê²½ë¡œê³„íšì„ í˜•ì„±í•˜ëŠ” ì „ëµì  ì¸ê³µì „ìœ„ì¥ë²•(Strategic 
Artificial Potential Field, SAPF)ë¥¼ ì •ì˜í•˜ê³  ì‹¤í—˜ì— í™œìš©í•˜ì˜€ë‹¤. Q ëŸ¬ë‹ ì—­ì‹œ ê²½ë¡œê³„íš ë°
ì¥ì• ë¬¼ íšŒí”¼ì—ì„œ í™˜ê²½ì„ í†µí•œ í•™ìŠµì˜ ê³„ì‚° ë°©ë²•ì„ í™œìš©í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì—°êµ¬ì— í™œìš©ë˜ê³ 
ìˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì§€í˜•ì„ ê³ ë ¤í•œ ì§€ìƒ ì „ì¥í™˜ê²½ì—ì„œ ì€íì™€ ì—„íë¥¼
ê³ ë ¤í•œ ì „ëµì  ì¸ê³µì „ìœ„ì¥ë²•ê¸°ë°˜ì˜ Q ëŸ¬ë‹(Q-SAPF)ì„ í™œìš©í•œ ê²½ë¡œê³„íš ì•Œê³ ë¦¬ì¦˜ì„
ì œì•ˆí•˜ì˜€ë‹¤.
ì œì•ˆëœ Q-SAPF ì•Œê³ ë¦¬ì¦˜ì€ Q ëŸ¬ë‹ê³¼ ë¹„êµí•œë‹¤. ì‹¤í—˜ ê²°ê³¼ì˜ ìœ ì˜ì„±ì„ í™•ì¸í•˜ê¸°
ìœ„í•˜ì—¬ ë¶„ì‚°ë¶„ì„ì„ ì‹¤ì‹œí•˜ì˜€ìœ¼ë©°, ê° ì•Œê³ ë¦¬ì¦˜ ê°„ì˜ ì°¨ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•˜ì—¬, ì‚¬í›„ë¶„ì„ì„
ì‹¤ì‹œí•˜ì˜€ë‹¤. ì—°êµ¬ì—ì„œ ì œì•ˆëœ Q-SAPF ëŠ” Q ëŸ¬ë‹ì— ë¹„í•´, ê²½ë¡œ ê¸¸ì´ëŠ” 1.96-8.77 ì°¨ì´ì˜
ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ê²½ë¡œê³„íšì„ ì•½ 14.57 ì´ˆ-69.29 ì´ˆ ê°€ëŸ‰ ë¹ ë¥¸ ì‹œê°„ì— ë„ì¶œí•˜ì˜€ë‹¤. ì´ ë•Œ, 
ê²½ë¡œ ì ìˆ˜ëŠ” ì•½ 2.73-47.96 í¬ì¸íŠ¸ ë†’ê²Œ ë‚˜íƒ€ë‚¬ë‹¤. íŠ¹íˆ, ì„±ê³µìœ¨ì€ 13.73%-54.89%ê¹Œì§€
ì°¨ì´ê°€ ë‚¬ë‹¤. ë‹¬ì„±ëœ ê²°ê³¼ëŠ” Q-SAPF ê°€ ìš°ìˆ˜í•œ í•™ìŠµëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
ë³¸ ì—°êµ¬ ê²°ê³¼ë¥¼ í†µí•´ ì¥ì• ë¬¼ì„ í™œìš©í•˜ì—¬ ì€íì™€ ì—„íê°€ ê°€ëŠ¥í•œ ê²½ë¡œ ê³„íšì´ ê°€ëŠ¥í•¨ì„
í™•ì¸í•˜ì˜€ë‹¤. í–¥í›„, ì¼ë°˜ì ì¸ ê²½ë¡œê³„íšì´ ì•„ë‹Œ, ì „ì‹œìƒí™©ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œì˜
íŠ¹ìˆ˜ì„±ì„ ì§€ë‹Œ ê²½ë¡œê³„íšì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.

ì¤‘ì‹¬ì–´: ê²½ë¡œê³„íš, ì „ì¥ í™˜ê²½, ì€íì™€ ì—„í, Q ëŸ¬ë‹, ì¸ê³µì „ìœ„ì¥ë²•

<7>
ACKNOWLEDMENTS
ë°•ì‚¬ë¼ëŠ” ê¿ˆì„ ê°€ì§€ê³  ì‹œì‘í•œ ì—´ì •ê°€ë“í–ˆë˜ ê¸°ë‚˜ê¸´ ì—¬ì •ì„ ì´ì œ ë§ˆë¬´ë¦¬í•˜ê³ ì í•©ë‹ˆë‹¤.
ì´ëŸ° ì—¬ì •ì„ ì‘ì›í•´ì£¼ì‹  ë¶„ë“¤ê»˜ ê°ì‚¬ì˜ ì¸ì‚¬ë¥¼ ì „í•˜ê³ ì í•©ë‹ˆë‹¤.
ì €ë¥¼ ì§€ë„í•´ì£¼ì‹  ì„œìœ¤í˜¸ êµìˆ˜ë‹˜ê»˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. í•™ìœ„ë¥¼ ì·¨ë“í•  ìˆ˜ ìˆë„ë¡ ëê¹Œì§€
ë“ ë“ í•˜ê²Œ ì§€ì¼œì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í•™ìœ„ë¥¼ ë°›ëŠ” ì§€ê¸ˆì—ì„œì•¼ ì—°êµ¬ì‹¤ì—ì„œ êµìˆ˜ë‹˜ì´ í•´ì£¼ì‹ 
ë§ì€ ì´ì•¼ê¸°ë“¤ì´ ëŠê»´ì§‘ë‹ˆë‹¤. ì•ìœ¼ë¡œì˜ ë¯¸ë˜ì—ì„œ í° ê¿ˆì„ ê¿€ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ
ë²ˆ ê°ì‚¬ì˜ ì¸ì‚¬ ì „í•©ë‹ˆë‹¤.
ë°”ì˜ì‹  ì¼ì • ì†ì—ì„œë„ ì œ í•™ìœ„ë…¼ë¬¸ ì‹¬ì‚¬ë¥¼ í•´ì£¼ì‹  ë°±ì¤€ê±¸ êµìˆ˜ë‹˜, ì •íƒœìˆ˜ êµìˆ˜ë‹˜,
ê¹€ì°½ìš± êµìˆ˜ë‹˜, ì „í˜„ìš° êµìˆ˜ë‹˜ê»˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.
ì €ì˜ í•™ìœ„ë¥¼ ë§ì´ ê¸°ë‹¤ë ¸ì„ ìš°ë¦¬ ISD ì—°êµ¬ì‹¤ ì‹êµ¬ë“¤ì—ê²Œ ê³ ë§™ë‹¤ëŠ” ë§ ì „í•©ë‹ˆë‹¤.
í•¨ê»˜ì˜€ê¸°ì— í˜ë“¤ì—ˆì§€ë§Œ ì¦ê²ê²Œ ì—°êµ¬í•˜ë©° ìƒí™œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ê²Œì¬ ì†Œì‹ì„
ëˆ„êµ¬ë³´ë‹¤ ê¸°ë»í•´ì¤€ ë™í‘œì˜¤ë¹ ì™€ ë™í™”ì˜¤ë¹ , ë’¤ëŠ¦ê²Œ ë°•ì‚¬í•™ìœ„ë¥¼ ì‹œì‘í•´ì„œ ì„¸ì‹¬íˆ í•™ìœ„ì‹¬ì‚¬ë¥¼
ì±™ê²¨ì¤€ ìš©í¬ ì˜¤ë¹ , ì—°êµ¬ì‹¤ ìƒí™œì—ì„œ ë¹¼ë†“ì„ìˆ˜ ì—†ëŠ” í•˜ë‚˜ë¿ì¸ ë‚´ ë™ê¸°ì¸ ë„í˜„ì˜¤ë¹ ì™€ ì¸¤ë°ë ˆ
ë™ìˆ˜ì˜¤ë¹ , ë‚´ ë•¡ê¹¡ì„ ë‹¤ ë°›ì•„ì¤€ ì§€í¬ì–¸ë‹ˆ, ë‹¤ë“¤ ë„ˆë¬´ ê³ ë§™ìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ í•¨ê»˜ í–ˆë˜
ì—°êµ¬ì‹¤ ì‹êµ¬ë“¤! ì •ë§ ê³ ë§™ìŠµë‹ˆë‹¤!
í•™ìœ„ ë…¼ë¬¸ ì“¸ ì‹œê°„ì„ ë°°ë ¤í•´ì¤€ ìš°ë¦¬ ì‚¬ë¬´ì‹¤ ì‹êµ¬ë“¤ì—ê²Œ ê³ ë§™ë‹¤ëŠ” ë§ ì „í•©ë‹ˆë‹¤.
êµ­ë°©ë¶„ì•¼ì— í•¨ê»˜ í•  ìˆ˜ ìˆê²Œ ê¸¸ì„ ë§Œë“¤ì–´ì¤€ ë°•ì˜ìš± êµìˆ˜ë‹˜, ë¶€ì¡±í•œ ë‚˜ë¥¼ ë¯¿ê³  ë”°ë¼ ì¤€
ë¯¸ì„ íŒ€ì¥, ìš°ë¦¼ê°„ì‚¬, ì •ì›ê³¼ì¥ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ë°°ë ¤í•´ì¤€ ì‹œê°„ ë•ì— ì˜ ë§ˆë¬´ë¦¬í•  ìˆ˜
ìˆì—ˆìŠµë‹ˆë‹¤.

<8>
ì´ ê²°ì‹¤ì„ ì •ë§ì´ê³  ê¸°ë‹¤ë ¸ì„ ì‚¬ë‘í•˜ëŠ” ë¶€ëª¨ë‹˜ê»˜ ì´ ë…¼ë¬¸ì„ ë°”ì¹©ë‹ˆë‹¤. ë§ë¡œ í‘œí˜„í•  ìˆ˜
ì—†ì„ ì •ë„ë¡œ ê°ì‚¬í•©ë‹ˆë‹¤. ê¸°ì•½ì—†ëŠ” ë¨¼ ì—¬ì •ì— ê·¸ê°„ ë…¸ì‹¬ì´ˆì‚¬í–ˆì„ ë§ˆìŒì˜ ê¸°ì¨ê³¼ ìœ„ì•ˆì´
ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì •ë§ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤. ì‚¬ë‘í•©ë‹ˆë‹¤.
ë¬µë¬µíˆ ì§€ì¼œë´ì¤€ ì˜¤ë¹ ì™€ ìƒˆì–¸ë‹ˆì—ê²Œë„ ê°ì‚¬ì˜ ë§ ì „í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³ , ì•„ì§ì€ ì´ê²Œ ì–´ë–¤
ì˜ë¯¸ì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ, ì •ì—°ì•„! ê°€ì€ì•„! ê³ ëª¨ê°€ ë°•ì‚¬ë€ë‹¤. 
ì €ë¥¼ ë¯¿ê³  ì‘ì›í•´ì¤€ ë§ì€ ë¶„ë“¤ì´ ê³„ì‹­ë‹ˆë‹¤. ê·¸ëŒ€ë“¤ì´ ìˆì–´ ì œê°€ ë” ë¹›ë‚˜ëŠ” ì‚¬ëŒì´ ë 
ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ì œ ê·¸ëŸ° ì‘ì›ê³¼ ê°ì‚¬ì˜ ë§ˆìŒì„ ë‹´ì•„, ì œê°€ ì˜†ì— ìˆì–´ ë¹›ì´ ë‚  ìˆ˜ ìˆë„ë¡
ì‘ì›í•˜ê² ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì„ ê¸°ë‹¤ë ¸ì„ ë§ì€ ë¶„ë“¤ì—ê²Œ ì§ì ‘ ì°¾ì•„ê°€ ë§ˆìŒì„ ì „í•˜ê² ìŠµë‹ˆë‹¤.
ëê¹Œì§€ í¬ê¸°í•˜ì§€ ì•Šì•˜ë˜ ì œ ìì‹ ì—ê²Œ ê³ ìƒí–ˆê³  ìˆ˜ê³ í–ˆë‹¤ëŠ” ë§í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. 
ì§€ì„ ì•„! ìˆ˜ê³ í–ˆë‹¤!
2024 ë…„ 7 ì›”,
ê°ì‚¬ì˜ ë§ˆìŒì„ ë‹´ì•„
ì´ ì§€ ì„ 

<9>
TABLE OF CONTENTS
ABSTRACT............................................................................................................... i
êµ­ë¬¸ ì´ˆë¡.................................................................................................................. iii
ACKNOWLEDMENTS ............................................................................................v
TABLE OF CONTENTS........................................................................................ vii
LIST OF TABLES................................................................................................... ix
LIST OF FIGURES ................................................................................................. xi
CHAPTER 1. Introduction.........................................................................................1
CHAPTER 2. Related Work ......................................................................................5
2.1 General path planning .................................................................................5
2.2 Path planning of UAV, USV.......................................................................6
2.3 Artificial Potential Field(APF) Method ......................................................7
2.4 Q-learning .................................................................................................13
CHAPTER 3. Environment representation..............................................................19
3.1 Experiment configuration .........................................................................19
3.2 Result value of experiment .......................................................................22
3.2.1 Definition of a path length .....................................................................22
3.2.2 Definition of a path score.......................................................................23
3.2.3 Definition of experiment time................................................................26
3.2.4 Definition of success rate.......................................................................27

<10>
CHAPTER 4. Q-SAPF Algorithm for Path Planning..............................................28
4.1 Strategic artificial potential field (SAPF) Method....................................28
4.2 Q-learning based on SAFP (Q-SAFP) Method.........................................33
CHAPTER 5. Experimental results .........................................................................37
5.1 Analysis of Map 1 Results........................................................................39
5.2 Analysis of Map 2 Results........................................................................43
5.3 Analysis of Map 3 Results........................................................................47
5.4 Analysis of Map 4 Results........................................................................49
5.5 Analysis of Map 5 Results........................................................................52
5.6 Analysis of Map 6 Results........................................................................56
5.7 Analysis of Map 7 Results........................................................................59
5.8 Analysis of Map 8 Results........................................................................62
5.9 Overall Review of the Analysis Results ...................................................66
CHAPTER 6. Conclusion and Future Directions ....................................................75
REFERENCES.........................................................................................................76

<11>
LIST OF TABLES
Table 1. Path planning pseudocode for APF........................................................... 12
Table 2. Path planning pseudocode for Q-learning................................................. 17
Table 3. Terrain and code shown in the maps......................................................... 19
Table 4. Terrain distribution of the experimental maps.......................................... 21
Table 5. Score of terrains........................................................................................ 24
Table 6. Path planning pseudocode for SAPF ........................................................ 31
Table 7. Path planning pseudocode Q-SAFP.......................................................... 35
Table 8. Experimental parameters .......................................................................... 37
Table 9. Experimental results.................................................................................. 38
Table 10. ANOVA of Map 1 .................................................................................. 40
Table 11. Post hoc analysis results for Map 1......................................................... 41
Table 12. ANOVA of Map 2 .................................................................................. 44
Table 13. Post hoc analysis results for Map 2......................................................... 45
Table 14. ANOVA of Map 3 .................................................................................. 47
Table 15. Post hoc analysis results for Map 3......................................................... 48

<12>
Table 16. ANOVA of Map 4 .................................................................................. 50
Table 17. Post hoc analysis results for Map 4......................................................... 51
Table 18. ANOVA of Map 5 .................................................................................. 53
Table 19. Post hoc analysis results for Map 5......................................................... 54
Table 20. ANOVA of Map 6 .................................................................................. 56
Table 21. Post hoc analysis results for Map 6......................................................... 57
Table 22. ANOVA of Map 7 .................................................................................. 59
Table 23. Post hoc analysis results for Map 7......................................................... 60
Table 24. ANOVA of Map 8 .................................................................................. 63
Table 25. Post hoc analysis results for Map 8......................................................... 64
Table 26. Path length results per map..................................................................... 67
Table 27. Path score(pt) results per map................................................................. 69
Table 28. Experiment time(s) per map.................................................................... 71
Table 29. Success rate(%) per map......................................................................... 72
Table 30. Rank result of Friedman test................................................................... 73

<12>
LIST OF FIGURES
Figure 1. Concept of the APF method .................................................................... 11
Figure 2. Organization of Data set.......................................................................... 20
Figure 3. Movement direction from the current location........................................ 22
Figure 4. Concept of a path length.......................................................................... 23
Figure 5. Method for assigning additional scores................................................... 25
Figure 6. Concept of the SAPF method .................................................................. 29
Figure 7. Comparison of SAPF and APF................................................................ 32
Figure 8. Example of Oscillations........................................................................... 34
Figure 9. Experimental results for Map 1 ............................................................... 42
Figure 10. Experimental results for Map 2 ............................................................. 46
Figure 11. Experimental results for Map 3 ............................................................. 49
Figure 12. Experimental results for Map 4 ............................................................. 52
Figure 13. Experimental results for Map 5 ............................................................. 55

<13>
Figure 14. Experimental results for Map 6 ............................................................. 58
Figure 15. Experimental results for Map 7 ............................................................. 61
Figure 16. Experimental results for Map 8 ............................................................. 65
Figure 17. Experiment time(s) per map .................................................................. 70
Figure 18. Success rate(%) per map........................................................................ 71

<14>
CHAPTER 1. Introduction
A battlespace is the area in which military operations and strategies are devised; 
these include air (air warfare), land (land warfare), sea (naval warfare), and 
information (information warfare). It also encompasses nonphysical battlespaces, 
comprising all the environments and factors needed to achieve mission objectives, 
wage battles, and safeguard troops. A battlefield is a specific location where physical 
battles are fought. Defense tactics such as concealment and cover are utilized to 
evade enemies. Concealment involves utilizing geographical features to avoid enemy 
detection, while cover entails obstructing enemy attacks or reducing the area under 
attack.
The utilization of terrain features is one of the key factors in planning military 
operations, and effectively using the characteristics of the terrain increases the 
number of factors that can lead to victory[1]. On a battlefield, identifying the location 
of an enemy is difficult; hence, it is necessary to traverse the terrain while taking 
precautions and utilizing concealment and cover. While prompt arrival at the 
destination is crucial, military conditions such as ammunition and fuel availability, 
soldier strength, and food supply must also be factored in while moving.
Recently, there has been a surge of research in the field of defense on MannedUnmanned
Teaming (MUM-T) worldwide. MUM-T refers to the operation of

<15>
missions where Manned Systems and Unmanned Systems work together[2]. Due to 
its diverse applicability and potential, MUM-T is expected to be a game-changer in 
future warfare. This is because the autonomous mission execution capabilities are 
continuously improving, and MUM-T can be utilized for difficult missions that are 
directly related to the survival of combat personnel[3]. Effective mission execution 
in MUM-T necessitates path planning for movement. MUM-T can be broadly 
categorized into three types: Ground MUM-T, which involves the use of Unmanned 
Ground Vehicles (UGVs) or unmanned armored vehicles; Aerial MUM-T, which 
employs fighter jets, helicopters, or Unmanned Aerial Vehicles (UAVs); and 
Maritime MUM-T, which utilizes Unmanned Underwater vehicles (UUVs).
Unmanned aerial vehicles (UAVs) are designed to perform advanced tasks such 
as complex terrain exploration, dangerous target reconnaissance, and aerial 
surveillance of enemy-occupied areas in place of humans. In this context, the most 
important research task is path planning [4], as it enables UAVs to navigate through 
enemy territories and successfully execute missions in air defense regions [5]. UAVs 
have been used effectively in a wide range of missions and have significantly reduced 
casualties during warfare[6]. Consequently, research is currently underway to 
investigate the feasibility of using UAVs for military supply transportation in 
mountainous terrain.
Path planning is crucial to navigation mission management systems, and a

<16>
sophisticated and secure approach to path planning is necessary in complex 
operational environments[7]. Autonomous underwater vehicles (AUVs) are 
particularly useful in various areas of battlefields, such as maritime reconnaissance, 
underwater search and investigation, navigation support, and tracking submarines[8]. 
To accomplish missions in a marine environment that is easily detectable by enemies, 
autonomous paths must be carefully planned to avoid collisions in a complex and 
unpredictable underwater environment and conserve energy [9].
Unlike in air or marine environments, planning executable paths for mobile 
vehicles such as robots on uneven terrain with slopes, steps, and rough surfaces is a 
challenging task[10]. Moreover, executing tasks safely and stably in a battlefield 
environment with atypical paths, uncertain conditions, and dense forests without a 
map for navigation poses a unique challenge. For instance, in a battlefield 
environment, obstacles may be utilized as concealment and cover instead of being 
avoided, as in traditional path planning. In this way, path planning in the battlefield 
environment has specificity. Assuming that a tank moves to the enemy base, if it 
moves by road, it can move quickly, but it can also be exposed to the enemy. 
Considering the battlefield environment, a tank can pass by, destroying the building 
if necessary, when moving.
This study proposes a path planning approach that enables concealment and 
cover in ground battlefield environments, taking into account various terrains such

<17>
as forests, rivers, and puddles. The approach suggested in this study utilizes obstacles 
as tools for concealment and cover rather than avoiding them to explore paths.
The remainder of this paper is organized as follows. Section 2 introduces related 
work of the path planning and methodology. Section 3 presents the problem 
description. Section 4 presents explains the concepts of strategic APF, and strategic 
APF-based Q-learning. Section 5 presents the experiment and analysis, and finally, 
Section 6 presents the conclusion and future directions of this study.

<18>
CHAPTER 2. Related Work
2.1 General path planning
In general, path finding involves avoiding obstacles while traveling from a 
starting point to a target point with the aim of minimizing distance and time. Path 
planning, on the other hand, aims to solve problems in various situations where the 
environment, including obstacles, is completely or partially unknown. Path planning 
can be particularly challenging when uncertainty is heightened due to a lack of 
information about the environment.
Path planning has been studied extensively, but the complexity of the problem 
increases in dynamic and complex environments, making it difficult to find optimal 
routes. Consequently, many researchers continuously explore methods to facilitate 
smooth movement of agents in unknown environments. Han, J. (2019) proposed a 
mechanism for path planning in unknown environments based on the importance of 
obstacles and surrounding points[11]. Chen, X., Zhao, M., & Yin, L. (2020) 
suggested a dynamic path planning method for unmanned aerial vehicles using the 
A* algorithm to prepare for sudden threats[12]. Chen, P., Pei, J., Lu, W., & Li, M. 
(2022) introduced a path planning method using the Soft Actor-Critic (SAC), a deep 
reinforcement learning algorithm, for dynamic obstacle avoidance with 
manipulators[13]. Huang, R., Qin, C., Li, J. L., & Lan, X. (2023) conducted research

<19>
using a modified reward approach with Deep Q-Networks (DQN) to achieve 
obstacle-free paths in dynamic unknown environments[14].
2.2 Path planning of UAV, USV
In recent years, UAVs have demonstrated excellent mobility and adaptability 
and are being widely used not only in the military sector for surveillance, search, and 
rescue operations but also in the civilian sector in urban environments[15]. The main 
objective of UAV path planning is to ensure that UAV performance requirements 
are met when designing a safe flight path to a target location [16]. UAV path 
planning takes into account various factors, including mission role, collision 
avoidance, energy limitations, and task-related constraints [17].
Several studies have been conducted on UAV path planning. Bai et al. (2021) 
proposed a path planning algorithm that combines the A* algorithm and the dynamic 
window algorithm (DWA) to satisfy both the security and speed requirements of 
UAVs[18]. Phung & Ha (2021) proposed a spherical vector-based particle swarm 
optimization (SPSO) for UAV path planning, focusing on the safety and validity of 
the path [19]. Han et al. (2022) researched the UAV path planning problem in 
complex indoor environments where a poor flight path may be formed[20].
Unmanned surface vehicles (USVs) are utilized for various military missions, 
such as intelligent surveillance and reconnaissance (ISR), mine countermeasures

<20>
(MCMs), and anti-submarine warfare (ASW)[21]. In the marine environment, 
obstacles and environmental factors such as weather pose significant challenges in 
path planning for USVs. Accordingly, the necessity of effective path planning for 
USVs has been emphasized [22]. To address the path planning problems of USVs in 
complex marine environments, Chen et al. (2021) suggested an improved ACO-APF 
algorithm with an adaptive early warning function. Liu et al. (2019) proposed the ant 
colony algorithm (ACA) and the clustering algorithm (CA) for obstacle avoidance
[23], [24]. Singh et al. (2018) proposed USV path planning that considers variable 
ocean currents and analyzed the effects of headwind and tailwind surface ocean 
currents on path planning [7]. Additionally, Guo et al. (2019) suggested an improved 
genetic algorithm (GA) to deduce an optimal anti-submarine search path model that 
considers the direction and speed of a USV [25]. Bai et al. (2023) investigated USV 
path planning in complex marine environments using a plant growth-based path 
planning algorithm (PGR) [26].
2.3 Artificial Potential Field(APF) Method
The APF method is a popular collision avoidance technique used in the 
development of unmanned vehicles and mobile robots. This effective methodology 
utilizes attractive and repulsive forces to control an artificial potential field. The 
attractive force is generated in the direction of the target point, while the repulsive 
force is generated in the opposite direction of any obstacle. Using this mechanism,

<21>
an agent can avoid obstacles and follow the intended operational direction [27], [28].
The APF algorithm plans a path by creating a virtual potential field within a 
given environment, which includes both attractive and repulsive potential fields. The 
attractive potential field is generated by the target point and acts as a gravitational 
force on the agent, while the obstacle generates the repulsive potential field. By 
calculating the resultant force of these fields, the agent can navigate a path that avoids 
collisions with obstacles.
The potential fields are typically defined mathematically as functions of distance. 
The attractive potential increases as the distance between the agent and its goal 
decreases, implying a relationship where the force becomes stronger as the agent 
approaches its goal. This reflects a direct relation to the closeness rather than an 
inverse relationship with distance itself. Conversely, the repulsive potential is indeed 
inversely proportional to the distance between the agent and obstacles, up to a certain 
threshold distance. Beyond this threshold, the repulsive potential becomes either 
constant or null, indicating that obstacles no longer exert an influence on the agent's 
movement[29].
In the path planning problem, the space is considered to be two-dimensional. 
The gravitational potential field function as defined by the target point is given by 
the following equation [30], [31]:

<22> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
ğ‘ˆğ‘ˆğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) = 1
2 ğ‘˜ğ‘˜ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ âˆ™ ğ›¿ğ›¿2ï¿½ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ï¿½ (1)
where ğ‘˜ğ‘˜ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ is the attraction coefficient, ğ‘‹ğ‘‹ is the 2D spatial coordinate of the 
controlled agent, ğ‘‹ğ‘‹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ is the target point, and ğ›¿ğ›¿ï¿½ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ï¿½ is the distance between 
the controlled agent and the target point. ğ‘ˆğ‘ˆğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) represents the targeted 
gravitational attraction between the controlled agent and the goal, which increases 
with the coefficient of gravitational attraction.
The derivative of the gravitational potential field ğ‘ˆğ‘ˆğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ with respect to the relative 
distance between the controlled agent and the target point is given by
ğ¹ğ¹ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) = âˆ’âˆ‡ğ‘ˆğ‘ˆğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) = âˆ’ 1
2
ğ‘˜ğ‘˜ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘âˆ‡Î´2ï¿½ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ï¿½ = âˆ’ğ¾ğ¾ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡) (2)
where âˆ‡ is the partial derivative operator. The repulsion force potential field 
generated between the controlled agent and the obstacle is given by
ğ‘ˆğ‘ˆğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ(ğ‘‹ğ‘‹) =
(3)
{
1
2ğ¾ğ¾ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ ï¿½ 1
ğ›¿ğ›¿(ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ) âˆ’ 1
ğ›¿ğ›¿0
ï¿½
2
ğ›¿ğ›¿(ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ) â‰¤ ğ›¿ğ›¿0
0 ğ›¿ğ›¿(ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ) > ğ›¿ğ›¿010

<23> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
where ğ¾ğ¾ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ is the repulsion coefficient, ğ‘‹ğ‘‹ is the 2D spatial coordinate of the 
controlling agent, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ is the coordinate of the obstacle, ğ›¿ğ›¿(ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ) is the 
distance between the controlling agent and the obstacle, and ğ›¿ğ›¿0 is the maximum 
repulsive force acting distance of the obstacle. If the distance between the agent and 
the obstacle, ğ›¿ğ›¿(ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ), is larger than the maximum influence distance ğ›¿ğ›¿0, the 
repulsion force potential field for the obstacle of the controlled agent becomes zero 
(ğ‘ˆğ‘ˆğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = 0). Conversely, if ğ›¿ğ›¿0 is small, the repulsion force potential field increases 
as the controlled agent approaches the obstacle [32].
Based on Equation (3), the repulsion force can be calculated using Equation (4) 
as follows:
ğ¹ğ¹ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ(ğ‘‹ğ‘‹) = âˆ’âˆ‡ğ‘ˆğ‘ˆğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ(ğ‘‹ğ‘‹) (4)
Using Equation (4), the total force ğ¹ğ¹(ğ‘‹ğ‘‹ğ‘ğ‘) for position ğ‘ğ‘ is calculated using the 
potential force of the obstacle and target point:
ğ¹ğ¹ï¿½ğ‘‹ğ‘‹ğ‘ğ‘ï¿½ = ï¿½ğ¹ğ¹ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹ğ‘–ğ‘–)
ğ‘›ğ‘›
ğ‘–ğ‘–=1
+ ï¿½ğ¹ğ¹ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿï¿½ğ‘‹ğ‘‹ğ‘—ğ‘—ï¿½
ğ‘›ğ‘›
ğ‘—ğ‘—=1
(5)

<24>
Figure 1. Concept of the APF method
Using the above principle, Guan et al. (2021) designed an APF-based reward 
function suitable for reinforcement learning. The reward increases as the agent 
approaches the target point and decreases as the agent approaches the obstacle[33]. 
Instant or stage rewards are assigned based on the current state, where the distance 
to the target point is divided into specific ranges and a higher positive reward is 
assigned for a closer distance to the target point. A gravity reward function using an 
attractive force assigns a higher positive reward if the agent is within a certain range 
of the target point. On the other hand, a repulsion penalty function using repulsive 
force assigns a higher negative reward if the agent is within a specific range of an 
obstacle.
Table 1 presents the pseudocode for the APF used in this study.

<25> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
Table 1. Path planning pseudocode for APF
Input: start point, goal point, map environment
Output: Final Path, Path Length, Path Score
Set the start point ğ‘†ğ‘†, the goal point ğºğº
Set the attraction coefficient ğ¾ğ¾ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ , the repulsive coefficient ğ¾ğ¾ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ
Set the current position ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘
Set the current state ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ to the start point ğ‘†ğ‘†
While(ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ is not the Goal point ğºğº)
Calculate the attractive force ğ¹ğ¹ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ as a vector point from ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ to ğºğº using 
Eq .(2)
Calculate the repulsive force ğ¹ğ¹ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ as a vector point away from each obstacle 
using Eq. (4)
Calculate the total force ğ¹ğ¹ğ‘ğ‘ using Equation (5)
Update ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ by taking a step in the direction ofğ¹ğ¹ğ‘ğ‘
End While
End
The APF method is utilized in path planning because of its simplicity and 
efficiency in providing smooth and safe plans [34]. By using the resultant forces 
from the target points and obstacles, safe paths can be efficiently calculated. 
However, the APF method can be sensitive to obstacles, which can limit its

<26>
effectiveness. Despite this, it is still widely used due to its easy maneuverability and 
ability to generate smoother trajectories with minimal computing resources [35]. 
Researchers have continued to explore the potential of the APF in path planning. Fan 
et al. (2020) proposed a modified APF algorithm to address path planning for 
avoiding movable, unknown obstacles[36]. Wang et al. (2019) proposed an APF 
algorithm that incorporated driving behavior characteristics and demonstrated its 
effectiveness and feasibility in obstacle avoidance path planning [37]. Shin & Kim 
(2021) proposed and validated a local hybrid PR-APF positioning network path 
planning approach to provide safe navigation for UVs [38].
2.4 Q-learning 
Reinforcement learning is a method for learning the optimal decision-making 
process through repeated interactions between an agent and the environment.
Q-learning is a type of reinforcement learning and is a model-free algorithm. The 
model-free algorithm attempts to find a policy function that maximizes the expected 
sum of future rewards through the action of the agent. The environment is not one of 
the input variables for this algorithm. Therefore, the next state and the next reward 
are passively obtained. In other words, this algorithm has to explore because it does 
not know the environment. Policy functions should be gradually learned through trial 
and error based on such exploration.

<27>
Additionally, Q-learning is based on the Markov decision process (MDP). The 
goal of Q-learning is to determine the optimal policy for an agent to take a specific 
action in a specific situation in a finite MDP. The predicted value of the final reward 
should be maximized, starting from the current state and proceeding through 
successive stages.
Q-learning learns the optimal policy by estimating the Q-function, which 
predicts the expected utility of taking a particular action in a specific state. The 
optimal policy can be derived from the learned Q-function by selecting the action 
with the highest Q-value in each state. One of the advantages of Q-learning is its 
ability to compare the expected values of actions in a model-free manner without any 
prior knowledge of the environment. It can also be used in environments where 
transitions and rewards are probabilistic. In Q-learning, "Q" represents the expected 
reward of the action taken in the current state. [39], [40], [41].
In Q-learning, the goal of the agent is to maximize the reward until the end of 
the episode. Reinforcement learning uses state-value functions and action-value 
functions to maximize rewards. The state-value function is expressed using Equation 
(6).

<28> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
ğ‘‰ğ‘‰ğœ‹ğœ‹(ğ‘ ğ‘ ) = ğ¸ğ¸ğœ‹ğœ‹[ğ‘…ğ‘…ğ‘¡ğ‘¡+1 + ğ›¾ğ›¾ğ‘…ğ‘…ğ‘¡ğ‘¡+2 + ğ›¾ğ›¾2ğ‘…ğ‘…ğ‘¡ğ‘¡+3 + â‹¯ |ğ‘†ğ‘†ğ‘¡ğ‘¡ = ğ‘ ğ‘ ]
= ğ¸ğ¸ğœ‹ğœ‹[ğ‘…ğ‘…ğ‘¡ğ‘¡+1 + ğ›¾ğ›¾ğ¸ğ¸ğœ‹ğœ‹(ğ‘…ğ‘…ğ‘¡ğ‘¡+2 + â‹¯ )|ğ‘†ğ‘†ğ‘¡ğ‘¡ = ğ‘ ğ‘ ]
= ğ¸ğ¸ğœ‹ğœ‹[ğ‘…ğ‘…ğ‘¡ğ‘¡+1 + ğ›¾ğ›¾ğ‘‰ğ‘‰ğœ‹ğœ‹(ğ‘†ğ‘†ğ‘¡ğ‘¡+1)|ğ‘†ğ‘†ğ‘¡ğ‘¡ = ğ‘ ğ‘ ]
(6)
In equation (6), ğ‘…ğ‘…ğ‘¡ğ‘¡ refers to the reward value at time ğ‘¡ğ‘¡. When a policy ğœ‹ğœ‹ is 
applied at time ğ‘¡ğ‘¡, the value of the expected state ğ‘ ğ‘  can be expressed as the sum of 
future rewards, where ğ›¾ğ›¾ in Equation (6) represents the discount factor. The discount 
factor, which determines the importance of present rewards relative to future rewards, 
has a value of [0,1].
When a certain action ğ‘ğ‘ is taken in a specific state ğ‘ ğ‘ , the Q-value is used to 
calculate the value of that action, which is known as the action-value function and is 
expressed using Equation (7). The action-value function predicts the value of the 
sum of rewards until the end of the episode, given that a specific action is taken using 
the discount factor.
ğ‘„ğ‘„ğœ‹ğœ‹(ğ‘ ğ‘ , ğ‘ğ‘) = ğ¸ğ¸ğœ‹ğœ‹[ğ‘…ğ‘…ğ‘¡ğ‘¡+1 + ğ›¾ğ›¾ğ‘…ğ‘…ğ‘¡ğ‘¡+2 + ğ›¾ğ›¾2ğ‘…ğ‘…ğ‘¡ğ‘¡+3 + â‹¯ |ğ‘†ğ‘†ğ‘¡ğ‘¡ = ğ‘ ğ‘ , ğ´ğ´ğ‘¡ğ‘¡ = ğ‘ğ‘]
= ğ¸ğ¸ğœ‹ğœ‹[ğ‘…ğ‘…ğ‘¡ğ‘¡+1 + ğ›¾ğ›¾ğ‘‰ğ‘‰ğœ‹ğœ‹(ğ‘†ğ‘†ğ‘¡ğ‘¡+1, ğ´ğ´ğ‘¡ğ‘¡+1)|ğ‘†ğ‘†ğ‘¡ğ‘¡ = ğ‘ ğ‘ , ğ´ğ´ğ‘¡ğ‘¡ = ğ‘ğ‘] (7)
The Q-value can be expressed according to the Bellman Equation (8): ğ‘ğ‘â€² is an 
action that maximizes the Q-value in the next state ğ‘ ğ‘ â€²
.

<29> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
ğ‘„ğ‘„(ğ‘ ğ‘ , ğ‘ğ‘) = ğ‘Ÿğ‘Ÿ + ğ›¾ğ›¾maxğ‘ğ‘â€²ğ‘„ğ‘„(ğ‘ ğ‘ â€²
, ğ‘ğ‘â€²
) (8)
Thus, at each time ğ‘¡ğ‘¡, the agent takes action ğ‘ğ‘ğ‘¡ğ‘¡ in state ğ‘ ğ‘ ğ‘¡ğ‘¡ and transitions to a new 
state ğ‘ ğ‘ ğ‘¡ğ‘¡+1. At this time, the reward ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ is obtained, and the Q-value is updated. The 
key aspect of the algorithm is its use of the Bellman equation, which enables iterative 
updates of the value function. The algorithm achieves this by taking a weighted 
average of the current value and new information at each iteration.
In summary, the Q-learning formula is given as follows Equation (9):
ğ‘„ğ‘„(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) â† (1 âˆ’ ğ›¼ğ›¼)ğ‘„ğ‘„(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) + ğ›¼ğ›¼[ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ + ğ›¾ğ›¾maxğ‘ğ‘â€²ğ‘„ğ‘„(ğ‘ ğ‘ ğ‘¡ğ‘¡+1, ğ‘ğ‘) âˆ’ ğ‘„ğ‘„(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)] (9)
ğ›¼ğ›¼: learning rate,0 < ğ›¼ğ›¼ â‰¤ 1 
ğ‘ ğ‘ ğ‘¡ğ‘¡: the current state at time ğ‘¡ğ‘¡
ğ‘ğ‘ğ‘¡ğ‘¡: the action at time ğ‘¡ğ‘¡ 
ğ‘„ğ‘„(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡): the current value
maxğ‘ğ‘â€²ğ‘„ğ‘„(ğ‘ ğ‘ ğ‘¡ğ‘¡+1, ğ‘ğ‘): estimate of optimal future value

<30> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
Table 2. Path planning pseudocode for Q-learning
Input: start point, goal point, map environment
Output: Final Path, Path Length, Path Score
Initialize Q-table with all zeros
Initialize the state ğ‘ ğ‘ , the action ğ‘ğ‘
Set the start point ğ‘†ğ‘†, the goal point ğºğº
Set the learning rate ğ›¼ğ›¼, the discount rate ğ›¾ğ›¾, the exploration rate ğœ€ğœ€
Set the max iteration
While(iteration < Max iteration)
Set the current state ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  to the initial state ğ‘ ğ‘ 
While(ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  is not the Goal point ğºğº)
With probability epsilon, Select a random action, ğ‘ğ‘
 Otherwise, select the action with the highest Q-value in the next 
state for ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘ 
 Perform the action ğ‘ğ‘, and Observe the new state, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘_ğ‘ ğ‘  , and 
reward, ğ‘Ÿğ‘Ÿ
 Update Q-value for ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  and a using Equation (9).
 Set ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  to ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘_ğ‘ ğ‘ ,
End While
Decrease epsilon to gradually reduce exploration rate ğœ€ğœ€
End While
End

<31>
With advancements in computer technology, path planning problems can be 
solved using a variety of learning algorithms. These algorithms have the advantage 
of including conditions that have not been solved previously. Reinforcement learning, 
in particular, can be used to explore unknown environments based on rewards and 
penalties. It is suitable for path planning problems in which an agent receives a 
reward if it avoids colliding with obstacles and a penalty if it collides with them. 
Low et al. (2019) proposed an improved Q-learning algorithm for path planning in 
environments with static obstacles of various shapes, sizes, and layouts. The use of 
Q-learning reduces computation time and generates efficient paths [42]. Jiang & Xin 
(2019) proposed the Q-learning algorithm for path planning in a free-space 
environment [43]. Wei et al. (2016) presented a novel discrete-time deterministic Qlearning
algorithm that updates the Q-function for all states and control spaces to 
induce the Q-function to converge to an optimal value, thereby considerably 
accelerating the convergence speed[44]. In addition, Low et al. (2022) addressed 
path planning via distance measurement, Q-function modification to overcome dead 
ends, and the introduction of virtual targets to bypass dead ends via an approach 
known as improved Q-learning (IQL) [45].

<32>
CHAPTER 3. Environment representation
3.1 Experiment configuration
This study proposes a path planning approach that takes into account terrain 
conditions to find an optimal route from a starting point to a target point in a 
battlefield environment while ensuring concealment and cover using terrain features. 
The starting point, target point, and terrain features are represented in a twodimensional
grid on a 50 Ã— 50 map. The terrain and corresponding codes used in the 
maps are shown in Table 3. This terrain includes what can be considered a battlefield 
environment.
Table 3. Terrain and code shown in the maps
Code Terrain Code Terrain
0 Target point 6 Forest
1 Flatland 7 Puddle
2 Hill 8 Sand
3 Destructible building 9 River
4 Destroyed building A Road
5 Indestructible building B Bridge

<31>
In this study, the map created custom for the experiments. In the event of ground 
warfare in Korea, locations near the Korean Demilitarized Zone (DMZ), which are 
of strategic military importance and typically host villages and military bases, were 
randomly selected. The terrain of the randomly designated maps was examined and 
formed into a 50*50 grid.
Figure 2. Organization of Data set
A total of eight maps were used in the experiment, and the terrain distribution of 
each map is presented in Table 4.

<32>
Table 4. Terrain distribution of the experimental maps
|      | 1      | 2      | 3            | 4         | 5              | 6      |
|------|--------|--------|--------------|-----------|----------------|--------|
|      | Flatland | Hill | Destructible | Destroyed | Indestructible | Forest |
| Map 1| 31.24% | 12.80% | 14.60%       | 1.60%     | 14.36%         | 2.16%  |
| Map 2| 37.96% | 7.20%  | 14.20%       | 3.08%     | 12.44%         | 8.04%  |
| Map 3| 46.32% | 10.24% | 24.36%       | 0.00%     | 14.04%         | 0.00%  |
| Map 4| 43.64% | 3.80%  | 15.16%       | 0.00%     | 15.52%         | 2.52%  |
| Map 5| 41.39% | 8.45%  | 15.88%       | 0.94%     | 12.08%         | 8.00%  |
| Map 6| 38.82% | 4.41%  | 24.69%       | 0.00%     | 12.41%         | 5.67%  |
| Map 7| 29.04% | 6.40%  | 16.32%       | 2.48%     | 14.44%         | 17.52% |
| Map 8| 36.00% | 0.88%  | 16.12%       | 1.28%     | 7.88%          | 14.20% |

|      | 7     | 8     | 9     | A      | B      |
|------|-------|-------|-------|--------|--------|
|      | Puddle| Sand  | River | Road   | Bridge |
| Map 1| 0.64% | 0.60% | 5.92% | 14.08% | 2.00%  |
| Map 2| 0.76% | 0.32% | 3.44% | 12.00% | 0.56%  |
| Map 3| 0.00% | 0.00% | 0.00% | 5.04%  | 0.00%  |
| Map 4| 3.32% | 1.56% | 6.20% | 7.36%  | 0.92%  |
| Map 5| 0.00% | 0.00% | 1.59% | 11.27% | 0.41%  |
| Map 6| 7.71% | 0.00% | 0.00% | 6.29%  | 0.00%  |
| Map 7| 0.52% | 1.56% | 0.00% | 11.72% | 0.00%  |
| Map 8| 0.00% | 0.16% | 3.24% | 19.48% | 0.76%  |

The movement of an agent is determined by choosing one of the available actions, 
which include moving left, right, up, down, or diagonally. If there is an obstacle 
blocking a certain direction, the agent can choose to move in any of the other 
available directions.

<33> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
Figure 3. Movement direction from the current location
3.2 Result value of experiment
The results of the experiment aim to compare path length, path score, success rate, and 
experiment time. The methods for deriving each of these values are described below.
3.2.1 Definition of a path length
In this study, the path length is defined as the total distance between the start and 
target points, as shown in Equation (10):
ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘â„ = ï¿½ï¿½(ğ‘¥ğ‘¥ğ‘–ğ‘–+1 âˆ’ ğ‘¥ğ‘¥ğ‘–ğ‘–)2 + (ğ‘¦ğ‘¦ğ‘–ğ‘–+1 âˆ’ ğ‘¦ğ‘¦ğ‘–ğ‘–)2
ğ‘›ğ‘›
ğ‘–ğ‘–=0
(10)
Here, in ğ‘–ğ‘– = 0, 1, â‹¯ , ğ‘›ğ‘›, when ğ‘–ğ‘– = 0, the starting point is ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  = (ğ‘¥ğ‘¥0 , ğ‘¦ğ‘¦0), and 

<34> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
when ğ‘–ğ‘– = ğ‘›ğ‘›, the target point is ğ‘ƒğ‘ƒğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ = (ğ‘¥ğ‘¥ğ‘›ğ‘› , ğ‘¦ğ‘¦ğ‘›ğ‘›). Furthermore, the current state 
point is ğ‘ƒğ‘ƒğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = (ğ‘¥ğ‘¥ğ‘–ğ‘– , ğ‘¦ğ‘¦ğ‘–ğ‘–), and the next state point is expressed as ğ‘ƒğ‘ƒğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› = (ğ‘¥ğ‘¥ğ‘–ğ‘–+1, ğ‘¦ğ‘¦ğ‘–ğ‘–+1). 
Figure 4. Concept of a path length
When an agent is assumed to move once, the path length is 1 when it moves left, 
right, up, or down, and the path length is âˆš2 when it moves in diagonal directions. 
3.2.2 Definition of a path score 
In this study, two types of path scores are defined for comparison: a common 
path score based on terrain characteristics and an additional path score based on 
concealment and cover. 
For the common path score, each terrain is assigned a score based on its 
characteristics, taking into account the possibility of applying concealment and cover. 
Terrains that allowed for concealment and cover were assigned a positive score, 
while those that hindered movement were assigned a negative score. Terrains such

<35> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
as roads and flatlands, which are relatively easy to traverse but do not offer much 
opportunity for concealment and cover, are not assigned a score. Obstacles are also 
not assigned a score, as a path cannot be established through them.
Table 5. Score of terrains
| Code | Terrain                | Score     |
|------|-----------------------|-----------|
| 0    | Target point          | 0         |
| 1    | Flatland              | 0         |
| 2    | Hill                  | 3         |
| 3    | Destructible building | 3         |
| 4    | Destroyed building    | 0         |
| 5    | Indestructible building | Obstacle |
| 6    | Forest                | 3         |
| 7    | Puddle                | -1        |
| 8    | Sand                  | -1        |
| 9    | River                 | Obstacle  |
| A    | Road                  | 0         |
| B    | Bridge                | 0         |
ğ‘†ğ‘†ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = ï¿½ğ‘ ğ‘ ğ‘¥ğ‘¥ğ‘–ğ‘–,ğ‘¦ğ‘¦ğ‘–ğ‘–
ğ‘›ğ‘›
ğ‘–ğ‘–=0
(11)
where ğ‘†ğ‘†ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ is the common path score in which ğ‘ ğ‘ ğ‘¥ğ‘¥ğ‘–ğ‘–,ğ‘¦ğ‘¦ğ‘–ğ‘– is the score of the current 
state point (ğ‘¥ğ‘¥ğ‘–ğ‘–, ğ‘¦ğ‘¦ğ‘–ğ‘–).

<36> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
For the additional path score, a positive score is assigned if concealment and 
cover are possible on the path. Starting from the locations where concealment and 
cover are possible, a score of ğœ”ğœ” is assigned to up to two spaces in the left, right, up, 
and down directions and to one space in the diagonal direction.
Figure 5. Method for assigning additional scores
ğ‘†ğ‘†ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = ï¿½ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ğ‘–ğ‘–,ğ‘¦ğ‘¦ğ‘–ğ‘–
ğ‘›ğ‘›
ğ‘–ğ‘–=0
(12)
where ğ‘†ğ‘†ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ is the additional path score and ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ğ‘–ğ‘–,ğ‘¦ğ‘¦ğ‘–ğ‘– is the additional score of the 
current state point (ğ‘¥ğ‘¥ğ‘–ğ‘–, ğ‘¦ğ‘¦ğ‘–ğ‘–).

<37> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
The total path score, represented as ğ‘†ğ‘†, is the sum of the common path score and 
additional path score, as shown in Equation (13).
ğ‘†ğ‘† = ğ‘†ğ‘†ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ + ğ‘†ğ‘†ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (13)
A high path score indicates that concealment and cover have been effectively 
applied.
By contrast, the score of the target point is set to zero: the value of the path score 
is derived when path exploration is successful, and a score of zero is assigned to the 
target point to ensure that only the concealment and cover scores of the final searched 
path are evaluated.
The unit of measurement for the results is expressed in points. Furthermore, 
these scores are calculated after each algorithm is completed and are used solely for 
the purpose of comparing results. The reward and penalty values for Q-learning can 
be found in Chapter 5.
3.2.3 Definition of experiment time
The experiment time is defined as the duration from the start to the end of the 
experiment. The termination conditions for APF and SAPF are defined as when a path has 
been planned from the start point to the target point. For Q-learning, the termination 
condition is defined as when the applied iterations have been completed.

<38> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
The Experiment time is represented as ğ¸ğ¸ğ¸ğ¸, as show Equation(14).
ğ¸ğ¸ğ¸ğ¸ = ğ‘‡ğ‘‡ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ âˆ’ ğ‘‡ğ‘‡ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  (14)
ğ‘‡ğ‘‡ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ : Start time 
ğ‘‡ğ‘‡ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’: End time 
3.2.4 Definition of success rate 
The success rate is defined as a metric to evaluate how well each algorithm 
locates a path plan during the experiment. It is calculated using the number of 
successful path plans from the start point to the target point out of the total number 
of trials.
The Experiment time is represented as ğ‘†ğ‘†ğ‘†ğ‘†, as show Equation(15).
ğ‘†ğ‘†ğ‘†ğ‘† = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘œğ‘œğ‘œğ‘œ ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†
ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› ğ‘œğ‘œğ‘œğ‘œ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ (15)

<39>
CHAPTER 4. Q-SAPF Algorithm for Path 
Planning
In this paper, we define a Strategic Artificial Potential Field for solving path 
planning in battlefield environments, and employ a methodology based on Qlearning.
4.1 Strategic artificial potential field (SAPF) Method
This study proposes a SAPF method for path planning for concealment and cover, 
which uses only a gravity potential field. In conventional APF, obstacles are avoided 
through the repulsive force that acts on them. However, on a battlefield, it may be 
necessary to use obstacles for concealment and coverage while moving toward a 
target point.
The SAPF method uses an attractive force for concealment and cover instead of 
the repulsive force that acts on obstacles, as shown in Equation (5) for a conventional 
APF. In the SAPF, the attractive force is defined separately for the target point and 
the obstacle.

<40> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
Figure 6. Concept of the SAPF method
Using Equations (1) and (2), the attractive force of a target point is defined as 
follows:
ğ¹ğ¹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) = âˆ’âˆ‡ğ‘ˆğ‘ˆğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ‘‹ğ‘‹) = âˆ’ğ‘˜ğ‘˜ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ï¿½ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ï¿½ (16)
where ğ‘˜ğ‘˜ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ is the attraction coefficient for a target point, ğ‘‹ğ‘‹ is the 2D spatial 
coordinate of the controlled agent, ğ‘‹ğ‘‹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ is the target point, and ğ‘ˆğ‘ˆğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) is the 
targeted gravitational attraction between the controlled agent and goal.
The attractive potential field for an obstacle is defined as follows:
ğ¹ğ¹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) = âˆ’âˆ‡ğ‘ˆğ‘ˆğ‘œğ‘œğ‘œğ‘œğ‘ ğ‘ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ‘‹ğ‘‹) = âˆ’ğ‘˜ğ‘˜ğ‘œğ‘œğ‘œğ‘œğ‘ ğ‘ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ‘‹ğ‘‹, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ) (17)

<41> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
where ğ‘˜ğ‘˜ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ is the attraction coefficient for an obstacle, ğ‘‹ğ‘‹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ is the 2D 
spatial coordinate of the obstacle, and ğ‘ˆğ‘ˆğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘‹ğ‘‹) is the targeted gravitational 
attraction between the controlled agent and goal. However, to ensure that the force 
toward a target point has a greater impact, ğ‘˜ğ‘˜ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ < ğ‘˜ğ‘˜ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘.
Using the above equation, the total force ğ¹ğ¹(ğ‘‹ğ‘‹ğ‘ğ‘) at position ğ‘ğ‘ is calculated using 
the potential forces of the obstacle and target point:
ï¿½ğ‘‹ğ‘‹ğ‘ğ‘ï¿½ = ï¿½ğ¹ğ¹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ‘‹ğ‘‹ğ‘–ğ‘–)
ğ‘›ğ‘›
ğ‘–ğ‘–=1
+ ï¿½ğ¹ğ¹ğ‘œğ‘œğ‘œğ‘œğ‘ ğ‘ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ‘‹ğ‘‹ğ‘—ğ‘—)
ğ‘›ğ‘›
ğ‘—ğ‘—=1
(18)

<42> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
Table 6. Path planning pseudocode for SAPF
Input: Start point, goal point, map environment
Output: Final Path, Path Length, Path Score
---
    Set the start point ğ‘†ğ‘†, the goal point ğºğº
    Set the attraction coefficient for target point ğ¾ğ¾ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
Set the attraction coefficient for obstacle pointğ¾ğ¾ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
    Set the current position ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘
    Set the current state ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ to the start point ğ‘†ğ‘†
While(ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ is not the Goal point ğºğº)
    Calculate the attractive force for Target point ğ¹ğ¹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ as a vector 
point from ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ to ğºğº using Eq. (13)
    Calculate the attraction force for obstacle point ğ¹ğ¹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ as a vector 
point away from each obstacle using Eq.(14)
    Calculate the total force ğ¹ğ¹ğ‘ğ‘ using Eq. (15)
    Update ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ by taking a step in the direction of ğ¹ğ¹ğ‘ğ‘
End While
End
---

<43>
SAPF formed a force different from APF owing to the attractive force generated 
from the obstacle. As a result, it heads to the target point along a different path from 
the APF. Figure 6 shows the movement lines of APF and SAPF according to 
obstacles. The arrow in the picture represents the total force received when the agent 
reached that location.
Figure 7. Comparison of SAPF and APF

<44>
4.2 Q-learning based on SAFP (Q-SAFP) Method
This study proposes Q-learning using SAPF.
Oscillations can occur in path planning when using APF due to the force of 
obstacles, resulting in unstable movements [28]. The SASF proposed in this study 
also has oscillations. An example of oscillations generated in APF and SAPF is 
shown in Figure 7.
To overcome the issues of oscillations in path planning, this study proposed a 
hybrid algorithm that incorporates Q-learning. The algorithm first explores a path 
using the SAPF algorithm, then configures a Q-table based on the explored path, and 
finally performs Q-learning to learn the optimal policy.

<45>
(a) Oscillation
(b) Oscillations of APF
(c) Oscillations of SAPF

Figure 8. Example of Oscillations

<46> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
Table 7. Path planning pseudocode Q-SAFP
Input: start point, goal point, map environment
Output: Final Path, Path Length, Path Score
---
    Initialize Q-table with all zeros
    Initialize the state ğ‘ ğ‘ , the action ğ‘ğ‘
    Set the start point ğ‘†ğ‘†, the goal point ğºğº
    Set the learning rate ğ›¼ğ›¼, the discount rate ğ›¾ğ›¾, the exploration rate ğœ€ğœ€
    Set the max iteration
    Set the attraction coefficient for target point ğ¾ğ¾ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
    Set the attraction coefficient for obstacle point ğ¾ğ¾ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
    Set the current position ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘
    Set the current state ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ to the start point ğ‘†ğ‘†
While(ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ is not the Goal point ğºğº)
    Calculate the attractive force for Target point ğ¹ğ¹ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ as a vector point from ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ to ğºğº using Eq. (13)
    Calculate the attraction force for obstacle point ğ¹ğ¹ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ as a vector point away from each obstacle using Eq.(14)
    Calculate the total force ğ¹ğ¹ğ‘ğ‘ using Eq. (15)
    Update ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ğ‘ by taking a step in the direction of ğ¹ğ¹ğ‘ğ‘
End While
End

<47> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
    While(iteration < Max iteration)
        Set the current state ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  to the initial state ğ‘ ğ‘ 
    While(ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  is not the Goal point ğºğº)
            With probability epsilon, Select a random action, ğ‘ğ‘
        Otherwise, select the action with the highest Q-value in the next state for ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘ 
        Perform the action ğ‘ğ‘, and Observe the new state, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘_ğ‘ ğ‘  , and reward,ğ‘Ÿğ‘Ÿ
        Update Q-value for ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  and a using Eq. (9).
        Set ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶_ğ‘ ğ‘  to ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘_ğ‘ ğ‘ ,
            End While
        Decrease epsilon to gradually reduce exploration rate ğœ€ğœ€
        End While
        End

<48> ìˆ˜ì‹ ë‹¤ìˆ˜ êµ¬ê°„ ê¸€ì ê¹¨ì§ ë§ìŒ
CHAPTER 5. Experimental results
In this section, the proposed Q-SAPF is experimentally compared against APF, 
Sapf, and Q-learning. The experiment was conducted on a 50 Ã— 50 grid map with 
various terrain types. The algorithms for path length, path score, experiment time, 
and success rate were derived and compared.
The parameters used for the experiment are listed in Table 8.
Table 8. Experimental parameters
Parameter
APF ğ¾ğ¾ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = 10,ğ¾ğ¾ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = 5
SAPF ğ¾ğ¾ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = 10,ğ¾ğ¾ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = 2
Q ğ›¼ğ›¼ = 0.3, ğ›¾ğ›¾ = 0.9, ğœ€ğœ€ = 0.9, max iteration = 4000
Q-SAPF ğ¾ğ¾ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = 10, ğ¾ğ¾ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ_ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = 2, ğ›¼ğ›¼ = 0.3, ğ›¾ğ›¾ = 0.9, ğœ€ğœ€ = 0.9, max iteration = 4000
In Q-learning and Q-SAPF, the reward is +1 when the movement is successful 
and -1 when it fails, and a reward value of +10 is given when the goal is reached. 
The path score was set to ğœ”ğœ” = 5 when deducing the experimental results. Through 
repeated experiments using four different experimental methods, the averages and 
standard deviations for eight different maps were derived and are listed in Table 9.

<49>
Table 9. Experimental results
### Map 1

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 113.16(6.79) | 141.30(13.96)  | 80.70(5.82)    | 78.31(4.55)     |
| Path Score(pt)    | 280.20(73.21)| 369.00(83.14)  | 215.33(39.83)  | 233.03(23.29)   |
| Exp. Time (s)     | 29.50(8.39)  | 95.39(41.96)   | 40.86(13.33)   | 16.56(9.56)     |
| Success rate (%)  | 30.71(9.84)  | 7.00(3.25)     | 72.14(6.29)    | 99.65(0.06)     |

### Map 2

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 112.15(11.25)| 147.37(12.53)  | 81.52(4.41)    | 72.75(3.81)     |
| Path Score(pt)    | 338.13(64.90)| 340.30(48.36)  | 190.67(43.43)  | 228.50(22.96)   |
| Exp. Time (s)     | 27.91(9.44)  | 266.99(85.63)  | 86.19(20.91)   | 16.50(6.43)     |
| Success rate (%)  | 66.16(14.55) | 2.23(0.78)     | 44.76(12.97)   | 99.65(0.06)     |

### Map 3

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 117.22(13.58)| 154.04(10.89)  | 80.43(3.05)    | 76.25(2.79)     |
| Path Score(pt)    | 407.17(54.16)| 468.87(73.65)  | 225.07(35.96)  | 239.77(26.70)   |
| Exp. Time (s)     | 26.17(8.85)  | 401.66(162.16) | 68.21(20.16)   | 15.35(2.46)     |
| Success rate (%)  | 58.23(13.31) | 2.60(1.37)     | 68.09(8.31)    | 99.61(0.06)     |

### Map 4

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 106.29(14.61)| 139.34(17.08)  | 65.60(3.71)    | 63.64(1.41)     |
| Path Score(pt)    | 286.53(40.61)| 347.00(75.50)  | 205.90(25.31)  | 219.20(26.31)   |
| Exp. Time (s)     | 11.35(3.31)  | 74.12(48.53)   | 46.07(12.49)   | 18.05(9.69)     |
| Success rate (%)  | 30.76(8.76)  | 5.16(6.26)     | 78.65(5.81)    | 99.74(0.05)     |

### Map 5

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 103.53(7.68) | 152.10(13.20)  | 78.36(2.93)    | 71.95(2.90)     |
| Path Score(pt)    | 284.47(33.47)| 447.87(75.86)  | 246.70(54.53)  | 249.43(23.10)   |
| Exp. Time (s)     | 70.66(23.82) | 145.10(71.14)  | 41.77(8.59)    | 10.34(1.57)     |
| Success rate (%)  | 19.90(7.54)  | 3.92(1.65)     | 72.88(5.34)    | 99.64(0.08)     |

### Map 6

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 127.90(15.10)| 117.19(13.14)  | 79.07(4.98)    | 71.70(3.60)     |
| Path Score(pt)    | 349.17(65.73)| 354.07(64.47)  | 217.63(27.67)  | 209.00(19.58)   |
| Exp. Time (s)     | 16.43(4.50)  | 12.78(1.73)    | 32.88(10.80)   | 7.21(1.24)      |
| Success rate (%)  | 74.26(9.58)  | 79.79(7.15)    | 75.26(7.48)    | 99.57(0.09)     |

### Map 7

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 132.44(17.37)| 91.95(14.34)   | 72.98(3.14)    | 64.72(2.66)     |
| Path Score(pt)    | 443.97(58.13)| 312.03(58.13)  | 198.87(22.22)  | 246.83(12.15)   |
| Exp. Time (s)     | 24.22(7.97)  | 2.90(0.71)     | 23.30(5.88)    | 8.73(2.32)      |
| Success rate (%)  | 27.94(13.42) | 75.78(8.46)    | 86.62(3.79)    | 99.65(0.06)     |

<50>
### Map 8

| í•­ëª©              | APF           | SAPF           | Q              | Q-SAPF          |
|-------------------|--------------|----------------|----------------|-----------------|
| Path length       | 142.85(17.32)| 122.63(12.71)  | 69.49(11.00)   | 80.95(1.37)     |
| Path Score(pt)    | 321.63(89.33)| 252.53(36.39)  | 154.30(24.04)  | 240.70(26.70)   |
| Exp. Time (s)     | 173.98(90.86)| 24.10(8.71)    | 47.18(8.81)    | 17.44(8.13)     |
| Success rate (%)  | 1.80(1.34)   | 32.88(10.79)   | 64.94(5.84)    | 99.67(0.07)     |

Analysis of variance (ANOVA) was conducted to compare the averages of the 
experimental methods on each map. In this case, the null hypothesis was that the 
average path length, path score, experiment time, and success rate for the four 
experimental methods are all equal. To verify the differences among the respective 
experimental methods, post hoc analysis was conducted using the Tukey method. 
Appendix 1 show result of the experimental methods on each map. 
5.1 Analysis of Map 1 Results 
Map 1 consists of hills, destructible buildings, and roads at 12â€“15% of the map. 
There are numerous indestructible buildings designated as obstacles located around 
the starting point. The ANOVA results for Map 1 are summarized in Table 10. 
Analysis of the results reveals statistically significant differences among the methods 
in terms of path length, path score, experiment time, and success rate, with all 
significance levels lower than 0.05. The Tukey HSD results for Map 1 are 
summarized in Table 11.

<51>
Table 10. ANOVA of Map 1
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 107,359.610    | 3   | 35,786.537   | 244.862  | <.001  |
|                       | Within Groups  | 16,953.407     | 116 | 146.150      |          |        |
|                       | Total          | 124,313.018    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 424,351.758    | 3   | 141,450.586  | 53.397   | <.001  |
|                       | Within Groups  | 307,287.033    | 116 | 2,649.026    |          |        |
|                       | Total          | 731,638.792    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 483,825.597    | 3   | 161,275.199  | 76.124   | <.001  |
|                       | Within Groups  | 245,756.015    | 116 | 2,118.586    |          |        |
|                       | Total          | 729,581.612    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 159,214.142    | 3   | 53,071.381   | 1393.065 | <.001  |
|                       | Within Groups  | 4,419.233      | 116 | 38.097       |          |        |
|                       | Total          | 163,633.375    | 119 |              |          |        |

<52>
Table 11. Post hoc analysis results for Map 1
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | 20.22067*             | <.001  | 12.0841                             | 28.3572     |
|                     | APF       | Q         | 73.36233*             | <.001  | 65.2258                             | 81.4989     |
|                     | APF       | Q-SAPF    | 61.89733*             | <.001  | 53.7608                             | 70.0339     |
|                     | SAPF      | Q         | 53.14167*             | <.001  | 45.0051                             | 61.2782     |
|                     | SAPF      | Q-SAPF    | 41.67667*             | <.001  | 33.5401                             | 49.8132     |
|                     | Q         | Q-SAPF    | -11.46500*            | .002   | -19.6015                            | -3.3285     |
| Path Score (pt)     | APF       | SAPF      | 69.10000*             | <.001  | 34.4596                             | 103.7404    |
|                     | APF       | Q         | 167.33333*            | <.001  | 132.6929                            | 201.9737    |
|                     | APF       | Q-SAPF    | 80.93333*             | <.001  | 46.2929                             | 115.5737    |
|                     | SAPF      | Q         | 98.23333*             | <.001  | 63.5929                             | 132.8737    |
|                     | SAPF      | Q-SAPF    | 11.83333              | .810   | -22.8071                            | 46.4737     |
|                     | Q         | Q-SAPF    | -86.40000*            | <.001  | -121.0404                           | -51.7596    |
| Experiment Time (s) | APF       | SAPF      | 149.87867*            | <.001  | 118.9000                            | 180.8573    |
|                     | APF       | Q         | 126.80300*            | <.001  | 95.8243                             | 157.7817    |
|                     | APF       | Q-SAPF    | 156.54400*            | <.001  | 125.5653                            | 187.5227    |
|                     | SAPF      | Q         | -23.07567             | .217   | -54.0543                            | 7.9030      |
|                     | SAPF      | Q-SAPF    | 6.66533               | .943   | -24.3133                            | 37.6440     |
|                     | Q         | Q-SAPF    | 29.74100              | .065   | -1.2377                             | 60.7197     |
| Success Rate (%)    | APF       | SAPF      | -31.07733*            | <.001  | -35.2315                            | -26.9232    |
|                     | APF       | Q         | -63.14533*            | <.001  | -67.2995                            | -58.9912    |
|                     | APF       | Q-SAPF    | -97.87367*            | <.001  | -102.0278                           | -93.7195    |
|                     | SAPF      | Q         | -32.06800*            | <.001  | -36.2222                            | -27.9138    |
|                     | SAPF      | Q-SAPF    | -66.79633*            | <.001  | -70.9505                            | -62.6422    |
|                     | Q         | Q-SAPF    | -34.72833*            | <.001  | -38.8825                            | -30.5742    |
The path lengths of Q and Q-SAPF on Map 1 are equivalent; in contrast, QSAPF
is shorter than APF by approximately 61.90, and Q-SAPF and SAPF differ in 
length by approximately 41.68. Although there was no significant difference 
between the SAPF and Q-SAPF in terms of path score, the path score of the Q-SAPF

<53>
was 80 points lower than that of the APF and 86.4 points higher than that of the QPF. 
The Tukey HSD analysis revealed that APF and SAPF were derived approximately 
149.88 seconds after Q, whereas Q-SAPF was derived approximately 156.54 
seconds after Q. The post hoc analysis of the success rate revealed significant 
average differences among all the methodologies. The success rates of Q-SAPF were 
97.87, 66.80, and 34.73% greater than those of APF, SAPF, and Q, respectively. 
Figure 8 shows the experimental results for Map 1, which represent the shortest path 
length obtained by each experiment
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 9. Experimental results for Map 1

<54>
5.2 Analysis of Map 2 Results
Like Map 1, Map 2 also consists of hills, destructible buildings, and roads on 7â€“
12% of the map. A river flows through the center of Map 2, and the path should be 
planned to cross the river to reach the target point.
The ANOVA results for Map 2 are summarized in Table 12. Analysis of these 
data revealed that there were statistically significant differences among the methods 
in terms of path length, path score, experiment time, and success rate, with all 
significance levels lower than 0.05. The Tukey HSD results for Map 2 are 
summarized in Table 13. The Tukey HSD results for path length indicate that there 
are differences between the path lengths obtained by APF, SAPF, Q-learning, and 
Q-SAPF. The path length of Q-SAPF is shorter than the lengths of APF, SAPF, and 
Q by 39.39, 74.62, and 8.77, respectively. The post hoc analysis of the path scores 
indicated that there was no significant difference between APF and SAPF; however, 
the path score of Q-SAPF exceeded that of Q by 37.83, and Q-SAPF had a shorter 
path length and a higher path score than did Q-SAPF.

<55>
Table 12. ANOVA of Map 2
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 102,841.463    | 3   | 34,280.488   | 431.799  | <.001  |
|                       | Within Groups  | 9,209.224      | 116 | 79.390       |          |        |
|                       | Total          | 112,050.687    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 525,684.867    | 3   | 175,228.289  | 78.199   | <.001  |
|                       | Within Groups  | 259,931.933    | 116 | 2,240.793    |          |        |
|                       | Total          | 785,616.800    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 1,207,344.102  | 3   | 402,448.034  | 203.764  | <.001  |
|                       | Within Groups  | 229,108.573    | 116 | 1,975.074    |          |        |
|                       | Total          | 1,436,452.675  | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 149,826.107    | 3   | 49,942.036   | 525.042  | <.001  |
|                       | Within Groups  | 11,033.936     | 116 | 95.120       |          |        |
|                       | Total          | 160,860.043    | 119 |              |          |        |

<55>
Table 13. Post hoc analysis results for Map 2
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -35.22633*            | <.001  | -41.2232                            | -29.2295    |
|                     | APF       | Q         | 30.62333*             | <.001  | 24.6265                             | 36.6202     |
|                     | APF       | Q-SAPF    | 39.39533*             | <.001  | 33.3985                             | 45.3922     |
|                     | SAPF      | Q         | 65.84967*             | <.001  | 59.8528                             | 71.8465     |
|                     | SAPF      | Q-SAPF    | 74.62167*             | <.001  | 68.6248                             | 80.6185     |
|                     | Q         | Q-SAPF    | 8.77200*              | .001   | 2.7752                              | 14.7688     |
| Path Score (pt)     | APF       | SAPF      | -2.16667              | .998   | -34.0263                            | 29.6929     |
|                     | APF       | Q         | 147.46667*            | <.001  | 115.6071                            | 179.3263    |
|                     | APF       | Q-SAPF    | 109.63333*            | <.001  | 77.7737                             | 141.4929    |
|                     | SAPF      | Q         | 149.63333*            | <.001  | 117.7737                            | 181.4929    |
|                     | SAPF      | Q-SAPF    | 111.80000*            | <.001  | 79.9404                             | 143.6596    |
|                     | Q         | Q-SAPF    | -37.83333*            | .013   | -69.6929                            | -5.9737     |
| Experiment Time (s) | APF       | SAPF      | -239.07900*           | <.001  | -268.9900                           | -209.1680   |
|                     | APF       | Q         | -58.27367*            | <.001  | -88.1847                            | -28.3626    |
|                     | APF       | Q-SAPF    | 11.41500              | .753   | -18.4960                            | 41.3260     |
|                     | SAPF      | Q         | 180.80533*            | <.001  | 150.8943                            | 210.7164    |
|                     | SAPF      | Q-SAPF    | 250.49400*            | <.001  | 220.5830                            | 280.4050    |
|                     | Q         | Q-SAPF    | 69.68867*             | <.001  | 39.7776                             | 99.5997     |
| Success Rate (%)    | APF       | SAPF      | 63.92500*             | <.001  | 57.3609                             | 70.4891     |
|                     | APF       | Q         | 21.40400*             | <.001  | 14.8399                             | 27.9681     |
|                     | APF       | Q-SAPF    | -33.48900*            | <.001  | -40.0531                            | -26.9249    |
|                     | SAPF      | Q         | -42.52100*            | <.001  | -49.0851                            | -35.9569    |
|                     | SAPF      | Q-SAPF    | -97.41400*            | <.001  | -103.9781                           | -90.8499    |
|                     | Q         | Q-SAPF    | -54.89300*            | <.001  | -61.4571                            | -48.3289    |
The experimental results indicate that Q-SAPF is approximately 11.4, 250.49, 
and 69.68 seconds faster than APF, SAPF, and Q, respectively. Furthermore, the 
success rates of Q-SAPF are 33.48, 97.41, and 54.89% greater than those of APF, 
SAPF, and Q, respectively. Based on the experimental results of Map 2, Q-SAPF

<56>
exhibited a high success rate and a short path length. Figure 9 shows the experimental 
results for Map 2, which represent the shortest path length obtained by each 
experiment.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 10. Experimental results for Map 2

<57>
5.3 Analysis of Map 3 Results
Map 3 comprises hills, destructible buildings, indestructible buildings, and roads, 
with flatlands in the center. The only obstacle in Map 3 is the indestructible building, 
which accounts for approximately 14% of the map. The ANOVA results for Map 3 
are summarized in Table 14. Analysis of the results revealed that there were 
statistically significant differences among the methods in terms of path length, path 
score, experiment time, and success rate, with all significance levels lower than 0.05.
Table 14. ANOVA of Map 3
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 119,061.235    | 3   | 39,687.078   | 495.710  | <.001  |
|                       | Within Groups  | 9,287.094      | 116 | 80.061       |          |        |
|                       | Total          | 128,348.329    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 1,328,485.500  | 3   | 442,828.500  | 170.917  | <.001  |
|                       | Within Groups  | 300,544.867    | 116 | 2,590.904    |          |        |
|                       | Total          | 1,629,030.367  | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 3,045,759.296  | 3   | 1,015,253.099| 151.614  | <.001  |
|                       | Within Groups  | 776,770.673    | 116 | 6,696.299    |          |        |
|                       | Total          | 3,822,529.969  | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 146,987.043    | 3   | 48,995.681   | 790.490  | <.001  |
|                       | Within Groups  | 7,189.842      | 116 | 61.981       |          |        |
|                       | Total          | 154,176.885    | 119 |              |          |        |

<58>
The post hoc analysis results for Map 3 are summarized in Table 15. On this 
map, there were no differences between Q-SAPF and Q in terms of path length, path 
score, or experiment time. However, the success rate of Q-SAPF is 31.52% greater 
than that of Q-SAPF. Furthermore, the path length of Q-SAPF is 40.97 shorter than 
that of APF and 77.7 shorter than that of SAPF. 
Table 15. Post hoc analysis results for Map 3
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -36.81367*            | <.001  | -42.8358                            | -30.7915    |
|                     | APF       | Q         | 36.79567*             | <.001  | 30.7735                             | 42.8178     |
|                     | APF       | Q-SAPF    | 40.97433*             | <.001  | 34.9522                             | 46.9965     |
|                     | SAPF      | Q         | 73.60933*             | <.001  | 67.5872                             | 79.6315     |
|                     | SAPF      | Q-SAPF    | 77.78800*             | <.001  | 71.7659                             | 83.8101     |
|                     | Q         | Q-SAPF    | 4.17867               | .274   | -1.8435                             | 10.2008     |
| Path Score (pt)     | APF       | SAPF      | -61.70000*            | <.001  | -95.9583                            | -27.4417    |
|                     | APF       | Q         | 182.10000*            | <.001  | 147.8417                            | 216.3583    |
|                     | APF       | Q-SAPF    | 167.40000*            | <.001  | 133.1417                            | 201.6583    |
|                     | SAPF      | Q         | 243.80000*            | <.001  | 209.5417                            | 278.0583    |
|                     | SAPF      | Q-SAPF    | 229.10000*            | <.001  | 194.8417                            | 263.3583    |
|                     | Q         | Q-SAPF    | -14.70000             | .679   | -48.9583                            | 19.5583     |
| Experiment Time (s) | APF       | SAPF      | -375.49500*           | <.001  | -430.5703                           | -320.4197   |
|                     | APF       | Q         | -42.04500             | .198   | -97.1203                            | 13.0303     |
|                     | APF       | Q-SAPF    | 10.81667              | .956   | -44.2586                            | 65.8920     |
|                     | SAPF      | Q         | 333.45000*            | <.001  | 278.3747                            | 388.5253    |
|                     | SAPF      | Q-SAPF    | 386.31167*            | <.001  | 331.2364                            | 441.3870    |
|                     | Q         | Q-SAPF    | 52.86167              | .065   | -2.2136                             | 107.9370    |
| Success Rate (%)    | APF       | SAPF      | 55.63200*             | <.001  | 50.3333                             | 60.9307     |
|                     | APF       | Q         | -9.85500*             | <.001  | -15.1537                            | -4.5563     |
|                     | APF       | Q-SAPF    | -41.38067*            | <.001  | -46.6794                            | -36.0820    |
|                     | SAPF      | Q         | -65.48700*            | <.001  | -70.7857                            | -60.1883    |
|                     | SAPF      | Q-SAPF    | -97.01267*            | <.001  | -102.3114                           | -91.7140    |
|                     | Q         | Q-SAPF    | -31.52567*            | <.001  | -36.8244                            | -26.2270    |

<59>
Figure 10 represents the shortest path length of each experiment on Map 3.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 11. Experimental results for Map 3
5.4 Analysis of Map 4 Results 
Map 4 does not contain destructible buildings, but other terrain types are evenly 
distributed. In Map 4, terrains that can provide concealment and cover are mostly 
located on the left-hand side with respect to the center. The ANOVA results for Map

<60>
4 are summarized in Table 16, from which it can be concluded that there is a 
statistically significant difference between the methods in terms of path length, path 
score, experiment time, and success rate, with all significance levels lower than 0.05.
Table 16. ANOVA of Map 4
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 118,046.159    | 3   | 39,348.720   | 302.251  | <.001  |
|                       | Within Groups  | 15,101.510     | 116 | 130.185      |          |        |
|                       | Total          | 133,147.669    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 383,330.025    | 3   | 127,776.675  | 58.864   | <.001  |
|                       | Within Groups  | 251,800.967    | 116 | 2,170.698    |          |        |
|                       | Total          | 635,130.992    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 74,294.900     | 3   | 24,764.967   | 37.859   | <.001  |
|                       | Within Groups  | 75,880.050     | 116 | 654.138      |          |        |
|                       | Total          | 150,174.950    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 168,749.963    | 3   | 56,249.988   | 1,503.845| <.001  |
|                       | Within Groups  | 4,338.878      | 116 | 37.404       |          |        |
|                       | Total          | 173,088.841    | 119 |              |          |        |
The post hoc analysis results for Map 4 are presented in Table 17. On this map, 
there were no differences between Q-SAPF and Q in terms of path length, path score, 
or experiment time. However, the success rate of Q-SAPF is 21.1% greater than that

<61>
of Q, and the path length of Q-SAPF is 42.64 shorter than that of APF and 75.70 
shorter than that of SAPF.
Table 17. Post hoc analysis results for Map 4
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -33.05367*            | <.001  | -40.7330                            | -25.3744    |
|                     | APF       | Q         | 40.68800*             | <.001  | 33.0087                             | 48.3673     |
|                     | APF       | Q-SAPF    | 42.64867*             | <.001  | 34.9694                             | 50.3280     |
|                     | SAPF      | Q         | 73.74167*             | <.001  | 66.0624                             | 81.4210     |
|                     | SAPF      | Q-SAPF    | 75.70233*             | <.001  | 68.0230                             | 83.3816     |
|                     | Q         | Q-SAPF    | 1.96067               | .910   | -5.7186                             | 9.6400      |
| Path Score (pt)     | APF       | SAPF      | -60.46667*            | <.001  | -91.8240                            | -29.1093    |
|                     | APF       | Q         | 80.63333*             | <.001  | 49.2760                             | 111.9907    |
|                     | APF       | Q-SAPF    | 67.33333*             | <.001  | 35.9760                             | 98.6907     |
|                     | SAPF      | Q         | 141.10000*            | <.001  | 109.7427                            | 172.4573    |
|                     | SAPF      | Q-SAPF    | 127.80000*            | <.001  | 96.4427                             | 159.1573    |
|                     | Q         | Q-SAPF    | -13.30000             | .687   | -44.6573                            | 18.0573     |
| Experiment Time (s) | APF       | SAPF      | -62.76933*            | <.001  | -79.9830                            | -45.5556    |
|                     | APF       | Q         | -34.71367*            | <.001  | -51.9274                            | -17.5000    |
|                     | APF       | Q-SAPF    | -6.69733              | .741   | -23.9110                            | 10.5164     |
|                     | SAPF      | Q         | 28.05567*             | <.001  | 10.8420                             | 45.2694     |
|                     | SAPF      | Q-SAPF    | 56.07200*             | <.001  | 38.8583                             | 73.2857     |
|                     | Q         | Q-SAPF    | 28.01633*             | <.001  | 10.8026                             | 45.2300     |
| Success Rate (%)    | APF       | SAPF      | 25.60333*             | <.001  | 21.4871                             | 29.7196     |
|                     | APF       | Q         | -47.88800*            | <.001  | -52.0042                            | -43.7718    |
|                     | APF       | Q-SAPF    | -68.98300*            | <.001  | -73.0992                            | -64.8668    |
|                     | SAPF      | Q         | -73.49133*            | <.001  | -77.6076                            | -69.3751    |
|                     | SAPF      | Q-SAPF    | -94.58633*            | <.001  | -98.7026                            | -90.4701    |
|                     | Q         | Q-SAPF    | -21.09500*            | <.001  | -25.2112                            | -16.9788    |

<62>
Figure 11 represents the shortest path length of each experiment on Map 4.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 12. Experimental results for Map 4
5.5 Analysis of Map 5 Results
Map 5 contains destructible buildings, indestructible buildings, and roads and 
accounts for 11â€“16% of the map. A forest where concealment and cover are possible 
is located on the right-hand side of the map. There were statistically significant

<63>
differences in the success rate among the methods, with all significance levels lower 
than 0.05. The results of the ANOVA for Map 5 are summarized in Table 18. Table 
19 summarizes the Tukey HSD results for Map 5.
Table 18. ANOVA of Map 5
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 119,199.162    | 3   | 39,733.054   | 635.663  | <.001  |
|                       | Within Groups  | 7,250.746      | 116 | 62.506       |          |        |
|                       | Total          | 126,449.908    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 819,033.767    | 3   | 273,011.256  | 105.188  | <.001  |
|                       | Within Groups  | 301,074.600    | 116 | 2,595.471    |          |        |
|                       | Total          | 1,120,108.367  | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 298,814.549    | 3   | 99,604.850   | 69.836   | <.001  |
|                       | Within Groups  | 165,447.010    | 116 | 1,426.267    |          |        |
|                       | Total          | 464,261.560    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 180,396.980    | 3   | 60,132.327   | 2,731.984| <.001  |
|                       | Within Groups  | 2,553.218      | 116 | 22.010       |          |        |
|                       | Total          | 182,950.197    | 119 |              |          |        |

<64>
Table 19. Post hoc analysis results for Map 5
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | -48.56900*            | <.001  | -53.8901                            | -43.2479    |
|                     | APF       | Q         | 25.16933*             | <.001  | 19.8482                             | 30.4904     |
|                     | APF       | Q-SAPF    | 31.58500*             | <.001  | 26.2639                             | 36.9061     |
|                     | SAPF      | Q         | 73.73833*             | <.001  | 68.4172                             | 79.0594     |
|                     | SAPF      | Q-SAPF    | 80.15400*             | <.001  | 74.8329                             | 85.4751     |
|                     | Q         | Q-SAPF    | 6.41567*              | .011   | 1.0946                              | 11.7368     |
| Path Score (pt)     | APF       | SAPF      | -163.40000*           | <.001  | -197.6884                           | -129.1116   |
|                     | APF       | Q         | 37.76667*             | .025   | 3.4782                              | 72.0551     |
|                     | APF       | Q-SAPF    | 35.03333*             | .043   | .7449                               | 69.3218     |
|                     | SAPF      | Q         | 201.16667*            | <.001  | 166.8782                            | 235.4551    |
|                     | SAPF      | Q-SAPF    | 198.43333*            | <.001  | 164.1449                            | 232.7218    |
|                     | Q         | Q-SAPF    | -2.73333              | .997   | -37.0218                            | 31.5551     |
| Experiment Time (s) | APF       | Sapf      | -74.44167*            | <.001  | -99.8596                            | -49.0237    |
|                     | APF       | Q         | 28.89100*             | .019   | 3.4731                              | 54.3089     |
|                     | APF       | Q-SAPF    | 60.32233*             | <.001  | 34.9044                             | 85.7403     |
|                     | SAPF      | Q         | 103.33267*            | <.001  | 77.9147                             | 128.7506    |
|                     | SAPF      | Q-SAPF    | 134.76400*            | <.001  | 109.3461                            | 160.1819    |
|                     | Q         | Q-SAPF    | 31.43133*             | .009   | 6.0134                              | 56.8493     |
| Success Rate (%)    | APF       | SAPF      | 15.98667*             | <.001  | 12.8291                             | 19.1442     |
|                     | APF       | Q         | -52.97200*            | <.001  | -56.1296                            | -49.8144    |
|                     | APF       | Q-SAPF    | -79.73367*            | <.001  | -82.8912                            | -76.5761    |
|                     | SAPF      | Q         | -68.95867*            | <.001  | -72.1162                            | -65.8011    |
|                     | SAPF      | Q-SAPF    | -95.72033*            | <.001  | -98.8779                            | -92.5628    |
|                     | Q         | Q-SAPF    | -26.76167*            | <.001  | -29.9192                            | -23.6041    |
The results indicate that there are differences in path length among APF, SAPF, Qlearning,
and Q-SAPF. The path lengths of Q-SAPF are shorter than those of APF, 
SAPF, and Q by 31.58, 80.15, and 6.41, respectively. The post hoc analysis results 
of the path score indicate that Q-SAPF does not significantly differ from Q-SAPF.

<65>
Q-SAPF achieved an experiment time of approximately 60.32, 134.76, and 31.43 
seconds faster than APF, Sapf, and Q, respectively. Furthermore, the success rates 
of Q-SAPF are 79.73, 95.72, and 26.76% greater than those of APF, SAPF, and Q, 
respectively. Figure 12 shows the experimental results for Map 5. The results 
represent the shortest path length of each experiment.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 13. Experimental results for Map 5

<66>
5.6 Analysis of Map 6 Results 
Map 6 has a significant proportion of destructible buildings, occupying 
approximately 25% of the map, and large roads are located on the left and top-left 
regions. The ANOVA results for Map 6 are summarized in Table 20. Statistically 
significant differences in path length, path score, experiment time, and success rate 
were observed among the methods, with all significance levels lower than 0.05. The 
Tukey HSD results for Map 6 are summarized in Table 21.
Table 20. ANOVA of Map 6
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 69,266.605     | 3   | 23,088.868   | 210.619  | <.001  |
|                       | Within Groups  | 12,716.383     | 116 | 109.624      |          |        |
|                       | Total          | 81,982.988     | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 575,284.867    | 3   | 191,761.622  | 79.682   | <.001  |
|                       | Within Groups  | 279,163.000    | 116 | 2,406.578    |          |        |
|                       | Total          | 854,447.867    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 10,977.154     | 3   | 3,659.051    | 103.463  | <.001  |
|                       | Within Groups  | 4,102.427      | 116 | 35.366       |          |        |
|                       | Total          | 15,079.581     | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 12,561.673     | 3   | 4,187.224    | 84.273   | <.001  |
|                       | Within Groups  | 5,763.633      | 116 | 49.686       |          |        |
|                       | Total          | 18,325.306     | 119 |              |          |        |

<67>
Table 21. Post hoc analysis results for Map 6
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | 10.71433*             | <.001  | 3.6675                              | 17.7611     |
|                     | APF       | Q         | 48.83767*             | <.001  | 41.7909                             | 55.8845     |
|                     | APF       | Q-SAPF    | 56.20300*             | <.001  | 49.1562                             | 63.2498     |
|                     | SAPF      | Q         | 38.12333*             | <.001  | 31.0765                             | 45.1701     |
|                     | SAPF      | Q-SAPF    | 45.48867*             | <.001  | 38.4419                             | 52.5355     |
|                     | Q         | Q-SAPF    | 7.36533*              | .037   | .3185                               | 14.4121     |
| Path Score (pt)     | APF       | SAPF      | -4.90000              | .980   | -37.9171                            | 28.1171     |
|                     | APF       | Q         | 131.53333*            | <.001  | 98.5162                             | 164.5505    |
|                     | APF       | Q-SAPF    | 140.16667*            | <.001  | 107.1495                            | 173.1838    |
|                     | SAPF      | Q         | 136.43333*            | <.001  | 103.4162                            | 169.4505    |
|                     | SAPF      | Q-SAPF    | 145.06667*            | <.001  | 112.0495                            | 178.0838    |
|                     | Q         | Q-SAPF    | 8.63333               | .904   | -24.3838                            | 41.6505     |
| Experiment Time (s) | APF       | SAPF      | 3.64800               | .088   | -.3545                              | 7.6505      |
|                     | APF       | Q         | -16.45233*            | <.001  | -20.4548                            | -12.4498    |
|                     | APF       | Q-SAPF    | 9.22600*              | <.001  | 5.2235                              | 13.2285     |
|                     | SAPF      | Q         | -20.10033*            | <.001  | -24.1028                            | -16.0978    |
|                     | SAPF      | Q-SAPF    | 5.57800*              | .002   | 1.5755                              | 9.5805      |
|                     | Q         | Q-SAPF    | 25.67833*             | <.001  | 21.6758                             | 29.6808     |
| Success Rate (%)    | APF       | SAPF      | -5.52833*             | .015   | -10.2725                            | -.7842      |
|                     | APF       | Q         | -1.00200              | .946   | -5.7462                             | 3.7422      |
|                     | APF       | Q-SAPF    | -25.31033*            | <.001  | -30.0545                            | -20.5662    |
|                     | SAPF      | Q         | 4.52633               | .067   | -.2178                              | 9.2705      |
|                     | SAPF      | Q-SAPF    | -19.78200*            | <.001  | -24.5262                            | -15.0378    |
|                     | Q         | Q-SAPF    | -24.30833*            | <.001  | -29.0525                            | -19.5642    |
The results indicate that there are differences in path length among APF, SAPF, 
Q-learning, and Q-SAPF. The path lengths of Q-SAPF are 56.20, 45.48, and 7.36 
shorter than those of APF, SAPF, and Q, respectively. Although there is no 
difference between Q and Q-SAPF in terms of path score, the duration of the Q-

<68>
SAPF experiment is approximately 25.67 seconds faster, and its success rate is 24.50% 
greater than that of the Q-SAPF. Figure 13 represents the shortest path length result 
for each experiment on Map 6.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 14. Experimental results for Map 6

<69>
5.7 Analysis of Map 7 Results 
Map 7 contains a forest that covers 17.5% of the area, and the target point can be 
reached only by passing through it. The ANOVA results for Map 7 indicate that 
there are statistically significant differences among the methods in terms of path 
length, path score, experiment time, and success rate, with all significance levels 
lower than 0.05. The ANOVA results for Map 7 are summarized in Table 22.
Table 22. ANOVA of Map 7
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 81,979.484     | 3   | 27,326.495   | 208.565  | <.001  |
|                       | Within Groups  | 15,198.485     | 116 | 131.021      |          |        |
|                       | Total          | 97,177.969     | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 1,017,753.758  | 3   | 339,251.253  | 183.398  | <.001  |
|                       | Within Groups  | 214,577.567    | 116 | 1,849.807    |          |        |
|                       | Total          | 1,232,331.325  | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 10,181.692     | 3   | 3,393.897    | 130.609  | <.001  |
|                       | Within Groups  | 3,014.273      | 116 | 25.985       |          |        |
|                       | Total          | 13,195.965     | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 87,987.877     | 3   | 29,329.292   | 441.011  | <.001  |
|                       | Within Groups  | 7,714.549      | 116 | 66.505       |          |        |
|                       | Total          | 95,702.427     | 119 |              |          |        |

<70>
The post hoc analysis results in Table 23 indicate that there are significant 
differences in path length among APF, SAPF, Q-learning, and Q-SAPF. 
Table 23. Post hoc analysis results for Map 7
| í•­ëª©                | (I) group | (J) group | Mean Difference (I-J) | Sig.   | 95% Confidence Interval Lower Bound | Upper Bound |
|---------------------|-----------|-----------|-----------------------|--------|-------------------------------------|-------------|
| Path Length         | APF       | SAPF      | 40.48600*             | <.001  | 32.7821                             | 48.1899     |
|                     | APF       | Q         | 59.45700*             | <.001  | 51.7531                             | 67.1609     |
|                     | APF       | Q-SAPF    | 67.72233*             | <.001  | 60.0184                             | 75.4262     |
|                     | SAPF      | Q         | 18.97100*             | <.001  | 11.2671                             | 26.6749     |
|                     | SAPF      | Q-SAPF    | 27.23633*             | <.001  | 19.5324                             | 34.9402     |
|                     | Q         | Q-SAPF    | 8.26533*              | .030   | .5614                               | 15.9692     |
| Path Score (pt)     | APF       | SAPF      | 131.93333*            | <.001  | 102.9864                            | 160.8803    |
|                     | APF       | Q         | 245.10000*            | <.001  | 216.1531                            | 274.0469    |
|                     | APF       | Q-SAPF    | 197.13333*            | <.001  | 168.1864                            | 226.0803    |
|                     | SAPF      | Q         | 113.16667*            | <.001  | 84.2197                             | 142.1136    |
|                     | SAPF      | Q-SAPF    | 65.20000*             | <.001  | 36.2531                             | 94.1469     |
|                     | Q         | Q-SAPF    | -47.96667*            | <.001  | -76.9136                            | -19.0197    |
| Experiment Time (s) | APF       | SAPF      | 21.32100*             | <.001  | 17.8901                             | 24.7519     |
|                     | APF       | Q         | .92700                | .895   | -2.5039                             | 4.3579      |
|                     | APF       | Q-SAPF    | 15.49367*             | <.001  | 12.0628                             | 18.9245     |
|                     | SAPF      | Q         | -20.39400*            | <.001  | -23.8249                            | -16.9631    |
|                     | SAPF      | Q-SAPF    | -5.82733*             | <.001  | -9.2582                             | -2.3965     |
|                     | Q         | Q-SAPF    | 14.56667*             | <.001  | 11.1358                             | 17.9975     |
| Success Rate (%)    | APF       | SAPF      | -47.83967*            | <.001  | -53.3283                            | -42.3510    |
|                     | APF       | Q         | -58.68100*            | <.001  | -64.1697                            | -53.1923    |
|                     | APF       | Q-SAPF    | -71.71100*            | <.001  | -77.1997                            | -66.2223    |
|                     | SAPF      | Q         | -10.84133*            | <.001  | -16.3300                            | -5.3527     |
|                     | SAPF      | Q-SAPF    | -23.87133*            | <.001  | -29.3600                            | -18.3827    |
|                     | Q         | Q-SAPF    | -13.03000*            | <.001  | -18.5187                            | -7.5413     |

<71>
The path lengths of Q-SAPF are 67.72, 27.23, and 8.26 shorter than those of 
APF, SAPF, and Q, respectively. The path score of Q-SAPF is 47.97 higher than that 
of Q; thus, Q-SAPF achieved a shorter path length and a higher path score than did 
Q. Furthermore, the success rates of Q-SAPF were 71.71, 23.87, and 13.03% greater 
than those of APF, SAPF, and Q, respectively. 
Figure 14 represents the shortest path length for each experiment on Map 7.
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 15. Experimental results for Map 7

<72>
5.8 Analysis of Map 8 Results
Map 8 consists of a forest that accounts for 14.2% of the area and a road that 
accounts for 19.5% of the area. The road extends through the forest. The ANOVA 
results for Map 8 are summarized in Table 24. There are statistically significant 
differences among the methods in terms of path length, path score, experiment time, 
and success rate, with all significance levels lower than 0.05. The Tukey HSD results 
indicate that there are differences in terms of path length among APF, SAPF, Qlearning,
and Q-SAPF. The path lengths of Q-SAPF are 61.89 and 41.67 shorter than 
those of APF and SAPF, respectively. Although it is 11.46 times longer than that of 
Q, its path score exceeds that of Q by 86.4 points. The success rates of Q-SAPF are 
97.87, 66.79, and 34.72% greater than those of APF, SAPF, and Q, respectively. 
Figure 15 represents the shortest path length for each experiment on Map 8.

<73>
Table 24. ANOVA of Map 8
|                       |                | Sum of Squares | df  | Mean Square   | F        | Sig.   |
|-----------------------|----------------|----------------|-----|--------------|----------|--------|
| Path Length           | Between Groups | 107,359.610    | 3   | 35,786.537   | 244.862  | <.001  |
|                       | Within Groups  | 16,953.407     | 116 | 146.150      |          |        |
|                       | Total          | 124,313.018    | 119 |              |          |        |
| Path Score (pt)       | Between Groups | 424,351.758    | 3   | 141,450.586  | 53.397   | <.001  |
|                       | Within Groups  | 307,287.033    | 116 | 2,649.026    |          |        |
|                       | Total          | 731,638.792    | 119 |              |          |        |
| Experiment Time (s)   | Between Groups | 483,825.597    | 3   | 161,275.199  | 76.124   | <.001  |
|                       | Within Groups  | 245,756.015    | 116 | 2,118.586    |          |        |
|                       | Total          | 729,581.612    | 119 |              |          |        |
| Success Rate (%)      | Between Groups | 159,214.142    | 3   | 53,071.381   | 1,393.065| <.001  |
|                       | Within Groups  | 4,419.233      | 116 | 38.097       |          |        |
|                       | Total          | 163,633.375    | 119 |              |          |        |

<74>
Table 25. Post hoc analysis results for Map 8
| í•­ëª© | (I) group | (J) group | Mean Difference (I-J) | Sig. | 95% Confidence Interval Lower Bound | Upper Bound |
|-------|-----------|-----------|----------------------|------|-------------------------------------|-------------|
| Path Length | APF | SAPF | 20.22067* | <.001 | 12.0841 | 28.3572 |
|             | APF | Q    | 73.36233* | <.001 | 65.2258 | 81.4989 |
|             | APF | Q-SAPF | 61.89733* | <.001 | 53.7608 | 70.0339 |
|             | SAPF | Q    | 53.14167* | <.001 | 45.0051 | 61.2782 |
|             | SAPF | Q-SAPF | 41.67667* | <.001 | 33.5401 | 49.8132 |
|             | Q    | Q-SAPF | -11.46500* | .002 | -19.6015 | -3.3285 |
| Path Score (pt) | APF | SAPF | 69.10000* | <.001 | 34.4596 | 103.7404 |
|             | APF | Q    | 167.33333* | <.001 | 132.6929 | 201.9737 |
|             | APF | Q-SAPF | 80.93333* | <.001 | 46.2929 | 115.5737 |
|             | SAPF | Q    | 98.23333* | <.001 | 63.5929 | 132.8737 |
|             | SAPF | Q-SAPF | 11.83333 | .810 | -22.8071 | 46.4737 |
|             | Q    | Q-SAPF | -86.40000* | <.001 | -121.0404 | -51.7596 |
| Experiment Time (s) | APF | SAPF | 149.87867* | <.001 | 118.9000 | 180.8573 |
|             | APF | Q    | 126.80300* | <.001 | 95.8243 | 157.7817 |
|             | APF | Q-SAPF | 156.54400* | <.001 | 125.5653 | 187.5227 |
|             | SAPF | Q    | -23.07567 | .217 | -54.0543 | 7.9030 |
|             | SAPF | Q-SAPF | 6.66533 | .943 | -24.3133 | 37.6440 |
|             | Q    | Q-SAPF | 29.74100 | .065 | -1.2377 | 60.7197 |
| Success Rate (%) | APF | SAPF | -31.07733* | <.001 | -35.2315 | -26.9232 |
|             | APF | Q    | -63.14533* | <.001 | -67.2995 | -58.9912 |
|             | APF | Q-SAPF | -97.87367* | <.001 | -102.0278 | -93.7195 |
|             | SAPF | Q    | -32.06800* | <.001 | -36.2222 | -27.9138 |
|             | SAPF | Q-SAPF | -66.79633* | <.001 | -70.9505 | -62.6422 |
|             | Q    | Q-SAPF | -34.72833* | <.001 | -38.8825 | -30.5742 |

<75>
(a) APF (b) SAPF
(c) Q (d) Q-SAPF
Figure 16. Experimental results for Map 8

<76>
5.9 Overall Review of the Analysis Results
When comparing Q-learning with APF and SAPF, Q-learning was shown to 
deduce shorter path lengths. This is due to the occurrence of oscillations in APF and 
SAPF, resulting in longer path lengths than those of Q-learning and Q-SAPF. In 
comparison, between Q-learning and Q-SAPF, the path length of Q-SAPF was found 
to be relatively lower overall. Figure 16 displays these results.
Figure 16. Path length results per map

<77>
Table 26. Path length results per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 113.16(6.79)  | 141.30(13.96) | 80.70(5.82)  | 78.31(4.55)   |
| Map 2 | 112.15(11.25) | 147.37(12.53) | 81.52(4.41)  | 72.75(3.81)   |
| Map 3 | 117.22(13.58) | 154.04(10.89) | 80.43(3.05)  | 76.25(2.79)   |
| Map 4 | 106.29(14.61) | 139.34(17.08) | 65.60(3.71)  | 63.64(1.41)   |
| Map 5 | 103.53(7.68)  | 152.10(13.20) | 78.36(2.93)  | 71.95(2.90)   |
| Map 6 | 127.90(15.10) | 117.19(13.14) | 79.07(4.98)  | 71.70(3.60)   |
| Map 7 | 132.44(17.37) | 91.95(14.34)  | 72.98(3.14)  | 64.72(2.66)   |
| Map 8 | 142.85(17.32) | 122.63(12.71) | 69.49(11.00) | 80.95(1.37)   |
Differences were confirmed through ANOVA and post hoc tests for each map. 
Q-SAPF showed a difference ranging from 31.58 to 67.72 compared to APF, 
depending on the map. In the case of APF, it appeared that differences arose due to 
the force pushing obstacles away while searching for a path. Compared to SAPF, 
differences ranged from 27.23 to 80.15 depending on the map. For SAPF, strong 
oscillations between obstacles resulted in a longer path length. However, when 
comparing Q and path length, the difference in results for maps 3 and 4 (refer to 
tables 15 and 17) was not significant. This finding suggests that there might be no 
difference in path lengths between Q and Q-SAPF, depending on the type of map 
and the locations of the start point and target points.

<78>
Compared to the APF and SAPF, the use of Q-learning resulted in lower path 
scores. Like in the previous results, oscillations occurring in the APF and SAPF 
resulted in higher path scores than did those in the Q-learning and Q-SAPF methods. 
When comparing Q-learning and Q-SAPF, the overall results of Q-SAPF were 
greater than those of Q-learning. These results are shown in Figure 17.
Figure 17. Path score(pt) results per map

<79>
Table 27. Path score(pt) results per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 280.20(73.21) | 369.00(83.14) | 215.33(39.83)| 233.03(23.29) |
| Map 2 | 338.13(64.90) | 340.30(48.36) | 190.67(43.43)| 228.50(22.96) |
| Map 3 | 407.17(54.16) | 468.87(73.65) | 225.07(35.96)| 239.77(26.70) |
| Map 4 | 286.53(40.61) | 347.00(75.50) | 205.90(25.31)| 219.20(26.31) |
| Map 5 | 284.47(33.47) | 447.87(75.86) | 246.70(54.53)| 249.43(23.10) |
| Map 6 | 349.17(65.73) | 354.07(64.47) | 217.63(27.67)| 209.00(19.58) |
| Map 7 | 443.97(58.13) | 312.03(58.13) | 198.87(22.22)| 246.83(12.15) |
| Map 8 | 321.63(89.33) | 252.53(36.39) | 154.30(24.04)| 240.70(26.70) |

Path scores for Q-SAPF were found to be lower, ranging from 35.04 points to 
197.14 points compared to APF, and 65.02 points to 229.1 points lower than SAPF. 
These differences were confirmed through ANOVA and post hoc tests. APF and 
SAPF exhibited higher points due to oscillations around obstacles that hindered 
quick escape. This suggests that using Q-SAPF could enable quicker escapes without 
lingering around obstacles. However, careful examination is needed for the path 
scores between Q and Q-SAPF. In the cases of maps 3, 4, 5, and 6, the differences 
in points were not significant (refer to figures 15, 17, 19, and 21). This observation 
is attributed to the dispersed distribution of obstacles across the maps. If the target 
point changes, differences might be observed.

<80>
According to Figure 16, the experiment time, with the exception of SAPF, 
appeared to be similar across each map for the different methodologies. Q-SAPF 
seems to derive paths faster than APF, SAPF, and Q. In particular, it shows about a 
20-second difference compared to Q.
Figure 17. Experiment time(s) per map
APF and SAPF showed significant differences in experiment time depending on 
the map. This appears to be due to the varying forces arising from the distribution of 
obstacles on the map, which also resulted in longer experiment times. Q and Q-SAPF, 
on the other hand, incurred penalties due to obstacles, facilitating faster pathfinding.

<81>
Table 28. Experiment time(s) per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 29.50(8.39)   | 95.39(41.96)  | 40.86(13.33) | 16.56(9.56)   |
| Map 2 | 27.91(9.44)   | 266.99(85.63) | 86.19(20.91) | 16.50(6.43)   |
| Map 3 | 26.17(8.85)   | 401.66(162.16)| 68.21(20.16) | 15.35(2.46)   |
| Map 4 | 11.35(3.31)   | 74.12(48.53)  | 46.07(12.49) | 18.05(9.69)   |
| Map 5 | 70.66(23.82)  | 145.10(71.14) | 41.77(8.59)  | 10.34(1.57)   |
| Map 6 | 16.43(4.50)   | 12.78(1.73)   | 32.88(10.80) | 7.21(1.24)    |
| Map 7 | 24.22(7.97)   | 2.90(0.71)    | 23.30(5.88)  | 8.73(2.32)    |
| Map 8 | 173.98(90.86) | 24.10(8.71)   | 47.18(8.81)  | 17.44(8.13)   |
It is worth noting that the success rate of Q-SAFP was consistently high, ranging 
from 99.5% to 99.7%. On the other hand, the success rate of Q varied significantly 
depending on the terrain, ranging from 44.76 to 86.62%, and did not exceed 90% in 
any of the experiments. 
Figure 18. Success rate(%) per map

<82>
Table 29. Success rate(%) per map
| Map   | APF           | SAPF          | Q            | Q-SAPF        |
|-------|---------------|---------------|--------------|---------------|
| Map 1 | 30.71(9.84)   | 7.00(3.25)    | 72.14(6.29)  | 99.65(0.06)   |
| Map 2 | 66.16(14.55)  | 2.23(0.78)    | 44.76(12.97) | 99.65(0.06)   |
| Map 3 | 58.23(13.31)  | 2.60(1.37)    | 68.09(8.31)  | 99.61(0.06)   |
| Map 4 | 30.76(8.76)   | 5.16(6.26)    | 78.65(5.81)  | 99.74(0.05)   |
| Map 5 | 19.90(7.54)   | 3.92(1.65)    | 72.88(5.34)  | 99.64(0.08)   |
| Map 6 | 74.26(9.58)   | 79.79(7.15)   | 75.26(7.48)  | 99.57(0.09)   |
| Map 7 | 27.94(13.42)  | 75.78(8.46)   | 86.62(3.79)  | 99.65(0.06)   |
| Map 8 | 1.80(1.34)    | 32.88(10.79)  | 64.94(5.84)  | 99.67(0.07)   |
Q-SAPF has shown superior performance compared to other methodologies. 
APF and SAPF exhibited varying differences in success rates depending on the map. 
For instance, APF struggled on map 8 due to the presence of large horizontal 
obstacles that impeded movement. In contrast, SAPF, unlike APF, was affected by 
various forces due to scattered obstacles, influencing its pathfinding capabilities. 
When compared to Q, differences in success rates were observed across all maps. Q 
likely encountered difficulties in pathfinding when encountering obstacles, incurring 
penalties. However, Q-SAPF facilitated movement close to obstacles, thereby 
reducing penalties and achieving higher success rates.

<84>
The experimental results indicate that the proposed Q-SAPF method 
outperforms other methods in terms of path length, path score, experiment time, and 
success rate during learning. Therefore, the proposed method is highly effective for 
path planning in various terrain environments where concealment and cover are 
possible. 
In this study, the Friedman test was performed. As a result, the rank was 
derived. The rankings are all statistically significant. Table 26 shows the results. 
Q-SAPF is ranked at the top overall. 
Table 30. Rank result of Friedman test
| Map   | Metric            | APF  | SAPF | Q    | Q-SAPF | Chi-Square | Asymp. Sig. |
|-------|-------------------|------|------|------|--------|------------|-------------|
| Map 1 | Path length       | 3.03 | 3.97 | 1.67 | 1.33   | 80.840     | <.001       |
|       | Path Score        | 2.63 | 3.67 | 1.67 | 2.03   | 41.240     | <.001       |
|       | Exp. Time (s)     | 2.17 | 3.90 | 2.80 | 1.13   | 72.520     | <.001       |
|       | Success rate (%)  | 3.00 | 4.00 | 2.00 | 1.00   | 90.000     | <.001       |
| Map 2 | Path length       | 3.03 | 3.97 | 1.93 | 1.07   | 86.600     | <.001       |
|       | Path Score        | 3.47 | 3.50 | 1.23 | 1.80   | 72.520     | <.001       |
|       | Exp. Time (s)     | 1.93 | 4.00 | 2.97 | 1.10   | 85.480     | <.001       |
|       | Success rate (%)  | 2.17 | 4.00 | 2.83 | 1.00   | 85.000     | <.001       |
| Map 3 | Path length       | 3.03 | 3.97 | 1.83 | 1.17   | 83.840     | <.001       |
|       | Path Score        | 3.27 | 3.73 | 1.42 | 1.58   | 74.458     | <.001       |
|       | Exp. Time (s)     | 1.90 | 4.00 | 3.00 | 1.10   | 86.760     | <.001       |
|       | Success rate (%)  | 2.73 | 4.00 | 2.27 | 1.00   | 82.960     | <.001       |

<85>
| Map   | Metric            | APF  | SAPF | Q    | Q-SAPF | Chi-Square | Asymp. Sig. |
|-------|-------------------|------|------|------|--------|------------|-------------|
| Map 4 | Path length       | 3.10 | 3.90 | 1.70 | 1.30   | 79.200     | <.001       |
|       | Path Score        | 3.07 | 3.73 | 1.43 | 1.77   | 63.320     | <.001       |
|       | Exp. Time (s)     | 1.33 | 3.53 | 3.37 | 1.77   | 66.920     | <.001       |
|       | Success rate (%)  | 3.03 | 3.97 | 2.00 | 1.00   | 88.840     | <.001       |
| Map 5 | Path length       | 3.00 | 4.00 | 1.93 | 1.07   | 87.760     | <.001       |
|       | Path Score        | 2.47 | 4.00 | 1.80 | 1.73   | 60.322     | <.001       |
|       | Exp. Time (s)     | 2.93 | 3.93 | 2.13 | 1.00   | 83.280     | <.001       |
|       | Success rate (%)  | 3.00 | 4.00 | 2.00 | 1.00   | 90.000     | <.001       |
| Map 6 | Path length       | 3.70 | 3.30 | 1.93 | 1.07   | 80.200     | <.001       |
|       | Path Score        | 3.43 | 3.57 | 1.77 | 1.23   | 75.221     | <.001       |
|       | Exp. Time (s)     | 2.80 | 2.20 | 4.00 | 1.00   | 84.240     | <.001       |
|       | Success rate (%)  | 3.22 | 2.65 | 3.13 | 1.00   | 57.562     | <.001       |
| Map 7 | Path length       | 3.93 | 3.03 | 2.00 | 1.03   | 85.320     | <.001       |
|       | Path Score        | 3.97 | 3.00 | 1.07 | 1.97   | 85.320     | <.001       |
|       | Exp. Time (s)     | 3.50 | 1.00 | 3.47 | 2.03   | 79.240     | <.001       |
|       | Success rate (%)  | 4.00 | 2.87 | 2.13 | 1.00   | 85.840     | <.001       |
| Map 8 | Path length       | 3.87 | 3.13 | 1.23 | 1.77   | 79.400     | <.001       |
|       | Path Score        | 3.60 | 2.80 | 1.07 | 2.53   | 60.400     | <.001       |
|       | Exp. Time (s)     | 4.00 | 1.80 | 2.93 | 1.27   | 80.080     | <.001       |
|       | Success rate (%)  | 4.00 | 3.00 | 2.00 | 1.00   | 90.000     | <.001       |

<86>
CHAPTER 6. Conclusion and Future 
Directions
This study proposed a path planning approach for effective concealment and 
cover in wartime situations using Q-learning and SAPF. The experimental results 
confirmed that the proposed method minimized the final path length and algorithm 
execution time more than did the general Q-learning method. The path score for 
verifying concealment and cover was also relatively higher. Furthermore, the 
proposed method demonstrated outstanding results on maps with varying 
characteristics. Therefore, the proposed method can be effectively used for military 
supply transportation or troop advancement in wartime situations.
The algorithm proposed in this study analyzed different paths in a grid 
environment. In future studies, the proposed algorithm can be more effectively 
applied to path planning in a linear environment. Additionally, path planning in this 
study considered all mobile vehicles that can be operated on the ground. For more 
realistic path planning, additional studies must be conducted in which the 
characteristics of mobile vehicles operable on the ground, such as tanks, armored 
vehicles, and reconnaissance vehicles, are taken into account.

<87>
REFERENCES
[1] W. Dawid and K. Pokonieczny, â€œMethodology of using terrain passability maps for 
planning the movement of troops and navigation of unmanned ground vehicles,â€ 
Sensors, vol. 21, no. 14, 2021, doi: 10.3390/s21144682.
[2] F. J. Guadalupe, â€œManned-unmanned teaming,â€ Aviation Digest, vol. 2, no. 3, 2014.
[3] Son In-geun, Lee Tae-gyun, and Lim Jae-seong, â€œTechnological trends of manned 
and unmanned combined combat system (MUM-T) for implementation of Mosaic 
warfare,,â€ The Journal of The Korean Institute of Communication Sciences, vol. 40, 
no. 4, pp. 22â€“30, 2023, [Online]. Available: 
http://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11342843
[4] R. Zhao, Y. Wang, G. Xiao, C. Liu, P. Hu, and H. Li, â€œA method of path planning 
for unmanned aerial vehicle based on the hybrid of selfish herd optimizer and particle 
swarm optimizer,â€ Applied Intelligence, vol. 52, no. 14, 2022, doi: 10.1007/s10489-
021-02353-y.
[5] D. Zhuoning, Z. Rulin, C. Zongji, and Z. Rui, â€œStudy on UAV Path Planning 
Approach Based on Fuzzy Virtual Force,â€ Chinese Journal of Aeronautics, vol. 23, 
no. 3, 2010, doi: 10.1016/S1000-9361(09)60225-9.
[6] N. TuÅ›nio and W. WrÃ³blewski, â€œThe efficiency of drones usage for safety and rescue 
operations in an open area: A case from Poland,â€ Sustainability (Switzerland), vol. 
14, no. 1, 2022, doi: 10.3390/su14010327.
[7] Y. Singh, S. Sharma, R. Sutton, D. Hatton, and A. Khan, â€œA constrained A* approach 
towards optimal path planning for an unmanned surface vehicle in a maritime 
environment containing dynamic obstacles and ocean currents,â€ Ocean Engineering, 
vol. 169, 2018, doi: 10.1016/j.oceaneng.2018.09.016.
[8] R. L. Wernli, â€œRecent US Navy underwater vehicle projects,â€ Space and Naval 77
Warfare Systems Center Report, 2002.
[9] Y. Zheng, â€œMultimachine Collaborative Path Planning Method Based on Aâˆ—
Mechanism Connection Depth Neural Network Model,â€ IEEE Access, vol. 10, 2022, 
doi: 10.1109/ACCESS.2022.3168719.
[10] B. Zhang, G. Li, Q. Zheng, X. Bai, Y. Ding, and A. Khan, â€œPath Planning for 
Wheeled Mobile Robot in Partially Known Uneven Terrain,â€ Sensors, vol. 22, no. 
14, 2022, doi: 10.3390/s22145217.
[11] J. Han, â€œA surrounding point set approach for path planning in unknown 
environments,â€ Comput Ind Eng, vol. 133, pp. 121â€“130, 2019.
[12] X. Chen, M. Zhao, and L. Yin, â€œDynamic path planning of the UAV avoiding static 
and moving obstacles,â€ J Intell Robot Syst, vol. 99, pp. 909â€“931, 2020.
[13] P. Chen, J. Pei, W. Lu, and M. Li, â€œA deep reinforcement learning based method for 
real-time path planning and dynamic obstacle avoidance,â€ Neurocomputing, vol. 497, 
pp. 64â€“75, 2022.
[14] R. Huang, C. Qin, J. L. Li, and X. Lan, â€œPath planning of mobile robot in unknown 
dynamic continuous environment using rewardâ€modified deep Qâ€network,â€ Optim 
Control Appl Methods, vol. 44, no. 3, pp. 1570â€“1587, 2023.
[15] J. Chen, Y. Zhang, L. Wu, T. You, and X. Ning, â€œAn Adaptive Clustering-Based 
Algorithm for Automatic Path Planning of Heterogeneous UAVs,â€ IEEE 
Transactions on Intelligent Transportation Systems, vol. 23, no. 9, 2022, doi: 
10.1109/TITS.2021.3131473.
[16] Y. Zhao, Z. Zheng, and Y. Liu, â€œSurvey on computational-intelligence-based UAV 
path planning,â€ Knowl Based Syst, vol. 158, 2018, doi: 
10.1016/j.knosys.2018.05.033.78
[17] J. Chen, X. Zhang, B. Xin, and H. Fang, â€œCoordination Between Unmanned Aerial 
and Ground Vehicles: A Taxonomy and Optimization Perspective,â€ IEEE Trans 
Cybern, vol. 46, no. 4, 2016, doi: 10.1109/TCYB.2015.2418337.
[18] X. Bai, H. Jiang, J. Cui, K. Lu, P. Chen, and M. Zhang, â€œUAV Path Planning Based 
on Improved A âˆ— and DWA Algorithms,â€ International Journal of Aerospace 
Engineering, vol. 2021, 2021, doi: 10.1155/2021/4511252.
[19] M. D. Phung and Q. P. Ha, â€œSafety-enhanced UAV path planning with spherical 
vector-based particle swarm optimization,â€ Appl Soft Comput, vol. 107, 2021, doi: 
10.1016/j.asoc.2021.107376.
[20] B. Han et al., â€œGrid-optimized UAV indoor path planning algorithms in a complex 
environment,â€ International Journal of Applied Earth Observation and 
Geoinformation, vol. 111. 2022. doi: 10.1016/j.jag.2022.102857.
[21] I. Bae and J. Hong, â€œSurvey on the Developments of Unmanned Marine Vehicles: 
Intelligence and Cooperation,â€ Sensors, vol. 23, no. 10. 2023. doi: 
10.3390/s23104643.
[22] F. Wang, Y. Bai, and L. Zhao, â€œPhysical Consistent Path Planning for Unmanned 
Surface Vehicles under Complex Marine Environment,â€ J Mar Sci Eng, vol. 11, no. 
6, 2023, doi: 10.3390/jmse11061164.
[23] Y. Chen, G. Bai, Y. Zhan, X. Hu, and J. Liu, â€œPath Planning and Obstacle Avoiding 
of the USV Based on Improved ACO-APF Hybrid Algorithm with Adaptive EarlyWarning,â€
IEEE Access, vol. 9, 2021, doi: 10.1109/ACCESS.2021.3062375.
[24] X. Liu, Y. Li, J. Zhang, J. Zheng, and C. Yang, â€œSelf-Adaptive Dynamic Obstacle 
Avoidance and Path Planning for USV under Complex Maritime Environment,â€ 
IEEE Access, vol. 7, 2019, doi: 10.1109/ACCESS.2019.2935964.
[25] H. Guo, Z. Mao, W. Ding, and P. Liu, â€œOptimal search path planning for unmanned 79
surface vehicle based on an improved genetic algorithm,â€ Computers and Electrical 
Engineering, vol. 79, 2019, doi: 10.1016/j.compeleceng.2019.106467.
[26] X. Bai, B. Li, X. Xu, and Y. Xiao, â€œUSV path planning algorithm based on plant 
growth,â€ Ocean Engineering, vol. 273, 2023, doi: 10.1016/j.oceaneng.2023.113965.
[27] J. Sun, G. Liu, G. Tian, and J. Zhang, â€œSmart obstacle avoidance using a danger index 
for a dynamic environment,â€ Applied Sciences (Switzerland), vol. 9, no. 8, 2019, doi: 
10.3390/app9081589.
[28] Y. Koren and J. Borenstein, â€œPotential field methods and their inherent limitations 
for mobile robot navigation,â€ in Proceedings - IEEE International Conference on 
Robotics and Automation, 1991. doi: 10.1109/robot.1991.131810.
[29] O. Khatib, â€œReal-time obstacle avoidance for manipulators and mobile robots,â€ in 
Proceedings - IEEE International Conference on Robotics and Automation, 1985. 
doi: 10.1109/ROBOT.1985.1087247.
[30] Q. Yao et al., â€œPath Planning Method with Improved Artificial Potential Field - A 
Reinforcement Learning Perspective,â€ IEEE Access, vol. 8, 2020, doi: 
10.1109/ACCESS.2020.3011211.
[31] Z. Wu, J. Dai, B. Jiang, and H. R. Karimi, â€œRobot path planning based on artificial 
potential field with deterministic annealing,â€ ISA Trans, vol. 138, 2023, doi: 
10.1016/j.isatra.2023.02.018.
[32] T. Luan, Z. Tan, B. You, M. Sun, and H. Yao, â€œPath planning of unmanned surface 
vehicle based on artificial potential field approach considering virtual target points,â€ 
Transactions of the Institute of Measurement and Control, vol. 46, no. 6, 2024, doi: 
10.1177/01423312231190208.
[33] M. Guan, F. X. Yang, J. C. Jiao, and X. P. Chen, â€œResearch on path planning of 
mobile robot based on improved Deep Q Network,â€ in Journal of Physics: 80
Conference Series, 2021. doi: 10.1088/1742-6596/1820/1/012024.
[34] U. Orozco-Rosas, O. Montiel, and R. SepÃºlveda, â€œMobile robot path planning using 
membrane evolutionary artificial potential field,â€ Applied Soft Computing Journal, 
vol. 77, 2019, doi: 10.1016/j.asoc.2019.01.036.
[35] H. Y. Zhang, W. M. Lin, and A. X. Chen, â€œPath planning for the mobile robot: A 
review,â€ Symmetry (Basel), vol. 10, no. 10, 2018, doi: 10.3390/sym10100450.
[36] X. Fan, Y. Guo, H. Liu, B. Wei, and W. Lyu, â€œImproved Artificial Potential Field 
Method Applied for AUV Path Planning,â€ Math Probl Eng, vol. 2020, 2020, doi: 
10.1155/2020/6523158.
[37] P. Wang, S. Gao, L. Li, B. Sun, and S. Cheng, â€œObstacle avoidance path planning 
design for autonomous driving vehicles based on an improved artificial potential field 
algorithm,â€ Energies (Basel), vol. 12, no. 12, 2019, doi: 10.3390/en12122342.
[38] Y. Shin and E. Kim, â€œHybrid path planning using positioning risk and artificial 
potential fields,â€ Aerosp Sci Technol, vol. 112, 2021, doi: 10.1016/j.ast.2021.106640.
[39] C. J. C. H. Watkins and P. Dayan, â€œQ-learning,â€ Mach Learn, vol. 8, no. 3, pp. 279â€“
292, 1992, doi: 10.1007/BF00992698.
[40] J. Clifton and E. Laber, â€œQ-Learning: Theory and applications,â€ Annual Review of 
Statistics and Its Application, vol. 7. 2020. doi: 10.1146/annurev-statistics-031219-
041220.
[41] B. Jang, M. Kim, G. Harerimana, and J. W. Kim, â€œQ-Learning Algorithms: A 
Comprehensive Classification and Applications,â€ IEEE Access, vol. 7, 2019, doi: 
10.1109/ACCESS.2019.2941229.
[42] E. S. Low, P. Ong, and K. C. Cheah, â€œSolving the optimal path planning of a mobile 
robot using improved Q-learning,â€ Rob Auton Syst, vol. 115, 2019, doi: 
10.1016/j.robot.2019.02.013.81
[43] J. Jiang and J. Xin, â€œPath planning of a mobile robot in a free-space environment 
using Q-learning,â€ Progress in Artificial Intelligence, vol. 8, no. 1, 2019, doi: 
10.1007/s13748-018-00168-6.
[44] Q. Wei, F. L. Lewis, Q. Sun, P. Yan, and R. Song, â€œDiscrete-time deterministic Qlearning:
A novel convergence analysis,â€ IEEE Trans Cybern, vol. 47, no. 5, 2017, 
doi: 10.1109/TCYB.2016.2542923.
[45] E. S. Low, P. Ong, C. Y. Low, and R. Omar, â€œModified Q-learning with distance 
metric and virtual target on path planning of mobile robot,â€ Expert Syst Appl, vol. 
199, 2022, doi: 10.1016/j.eswa.2022.117191.